{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Exam 7 2016 Overall Summary\"\nauthor: \"Cliff Lau\"\ndate: \"`r format(Sys.time(), '%B %d, %Y')`\"\noutput: \n  html_document:\n    toc: true\n    toc_float:\n      collapsed: false\n      smooth_scroll: false\n    toc_depth: 4\n---\n\n## Section A\n\n### A1 Least Square\n\nLoss Development Using Credibility - Brosius\n\nLeast square methods\n\n* Need to know how to calculate $b$\n\nTheoretical Bayesian method and also estimating the Bayesian credibility with 2 methods for getting $Z$\n\n* Can do $Z = \\dfrac{b}{c}$ or the VHM EVPV formula\n* Under certain relationships between $X$ and $Y$ supports different methods\n* Some formulas to remember if we do the theoretical Bayes\n\nDiscussion on the caseload effect where the development and ultimate is not independent\n\n### A2 Benktander\n\nCredible Claims Reserve: The Benktander Method - T. Mack (2000)\n\nBenktander Method\n\n### A3 Credible Claims Reserve\n\nCredible Claims Reserve: Benktander, Neuhaus and Mack - W. Hurlimann\n\nMethod that is based on the incremental loss ratio triangles\n\nWeight between the analogous development and BF method\n\n$R_i^c = Z_iR_i^{ind} + (1-Z_i)R_i^{coll}$\n\n| $Z_i$ | Method |\n| --- | --- | --- |\n| 1     | Chainladder; Individual LR|\n| 0     | BF; Collective LR|\n| $p_i$ | Benktander |\n| $p_i \\times ELR$ | Neuhaus |\n| $Z = \\dfrac{p_i}{p_i + \\sqrt{p_i}}$ | Optimal |\n\n### A4 High Deductible\n\nA Model for Reserving Workers Compensation High Deductibles - J. Siewert\n\nCovers 6 methods for dealing with deductibles each with their own pros and cons\n\n1) Loss Ratio Method:  \nApply occurence and aggregate charges\n\n2) Implied Development:  \nCalculate as the difference between unlimited and limited\n\n3) Direct Development:  \n$XSLDF_t^{L} = \\dfrac{Ult_{XS}}{S_t^{XS}} = \\dfrac{Ult \\cdot \\chi}{\\frac{Ult}{LDF_t} - \\frac{Ult\\cdot(1-\\chi)}{LDF_t^L}}$\n    \n4) Credibility Weight Method:  \nWeight between direct development and expected loss ratio method\n\n5) Development Method\n\n    * Severity needs to be trended\n    * Claim counts are developed separately ground up\n    * Severity LDF formulas, know them well to manipulate and know what formula requires what\n    \n        * $LDF_t^L = LDF_t \\dfrac{R^L}{R_t^L}$\n    \n        * $XSLDF_t^L = LDF_t \\dfrac{(1-R^L)}{(1-R_t^L)}$\n    \n        * $LDF_t = R^L_t \\cdot LDF^L_t + (1 - R^L_t) \\cdot XSLDF^L_t$\n    \n        * $\\dfrac{ILDF^L_t}{ILDF_t} = \\Delta R^L_t = \\dfrac{R^L_{t+1}}{R^L_t}$\n\n        * $\\dfrac{IXSLDF^L_t}{IXSLDF_t} = \\Delta (1 - R^L_t) = \\dfrac{1 - R^L_{t+1}}{1 - R^L_t}$\n\n6) Distribution Method\n\n### A5 Developement by Layer\n\nClaims Development by Layer - R. Sahasrabuddhe\n\nKnow the process for setting up [base](#a5-1) triangle and then [convert](#a5-2) the base LDFs to any layer\n\nMain formulas to memorize:\n\n* For loss capping:  \n$LEV(X; \\Phi \\sim Exp(\\theta)) = \\theta \\: \\left[ 1 - \\operatorname{exp}\\left\\{-\\left(\\dfrac{x}{\\theta}\\right)\\right\\} \\right]$\n\n* For conversion:  \n$\\begin{align}\n  F_{ij}^X = F_{nj}^B \\times \\frac{LEV(X;\\Phi_{i\\infty})\\div LEV(B;\\Phi_{n\\infty})}{LEV(X;\\Phi_{ij}) \\div LEV(B;\\Phi_{nj})}\n \\end{align}$\n\nWe can do this for an XS layer as well\n\nIf we donâ€™t have the severity distribution by age, we can work with the severity at ultimate\n\n* $\\begin{align}\n  F_{ij}^X = F_{nj}^B \\times \\frac{LEV(X;\\Phi_{i\\infty})\\div LEV(B;\\Phi_{ {\\color{red}i} \\infty})}{R_j{(X,B)}}\n \\end{align}$\n\n### A6 Variability in CL Method\n\nMeasuring the Variability of Chain Ladder Reserve Estimate - T. Mack (1994)\n\nFocus on cummulative losses\n\nChain ladder **assumptions**:\n\n1) $\\operatorname{E}\\left [c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ] = c_{i,k} \\: f_k$\n    \n2) $\\left \\{c_{i,1} \\cdots c_{i,I} \\right \\} \\: {\\perp\\!\\!\\!\\perp} \\: \\left \\{c_{j,1} \\cdots c_{j,I} \\right \\}$ for $i \\neq\\ j$\n    \n3) $\\operatorname{Var}\\left (c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ) = \\alpha_k^2 \\: c_{i,k}$\n\nLDF weight and **variance** assumptions:\n\n$\\begin{align}\n  \\hat{f_k} = \\frac{\\sum_j \\overbrace{\\frac{c_{j,k+1}}{c_{j,k}}}^{LDF_j} \\: w_{j,k}}{\\sum_j w_{j,k}}\n  \\end{align}$\n\n| Weight $w_{j,k}$ | Description | Variance | Residual |\n| ---------------- | ----------- | -------- | -------- |\n| 1                | Simple Average | $\\alpha_k^2 \\times \\mathbf{c_{j,k}^2}$ | $\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{j,k}^2}}}$ |\n| $c_{j,k}$        | Weighted Average | $\\alpha_k^2 \\times \\mathbf{c_{j,k}}$ | $\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{j,k}}}}$ |\n| $c_{j,k}^2$      | Least Square | $\\alpha_k^2 \\times \\mathbf{1}$ | $\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{1}}}$ |\n\n**Mean squared error**\n\n$\\begin{align}\n  MSE(\\hat{c}_{i,I}) = \\hat{c}_{i,I}^2 \\Bigg \\{ \\sum_{k = I + 1 - i}^{I-1} \\frac{\\hat{\\alpha}_k^2}{\\hat{f_k}^2} \\Bigg ( \\frac{1}{\\hat{c}_{i,k}} + \\underbrace{\\frac{1}{\\sum_{j=1}^{I-k}c_{j,k}}}_{\\text{Column x latest}}\\Bigg ) \\Bigg \\}\n\\end{align}$\n\n$\\begin{align}\n  \\hat{\\alpha}_k^2 = \\frac{1}{I - k - 1} \\sum_{j=1}^{I-k} c_{j,k} \\Big ( \\underbrace{\\frac{c_{j,k+1}}{c_{j,k}}-\\hat{f_k}}_{\\text{AY LDFs - Selected}} \\Big )^2\n\\end{align}$\n\nFor the last one $\\alpha^2_{I-1}$:\n\n* If the 3rd last one is lower, take that\n* Else, you take the 2nd last times the ratio of the 2nd last to 3rd last\n\n**Confidence Interval**\n\nNormal: $\\hat{R}_i \\pm Z_\\alpha \\: s.e.(\\hat{R}_i)$\n\nLogNormal: $R_i \\operatorname{exp}\\left \\{ -\\dfrac{\\sigma_i^2}{2} \\pm \\: Z_\\alpha \\sigma_i \\right \\}$\n\n* $\\sigma_i^2 = \\operatorname{ln} \\left [ 1 + \\left ( \\dfrac{s.e.(\\hat{R}_i)}{\\hat{R}_i} \\right)^2 \\right]$\n* $\\mu_i = \\operatorname{ln}(\\hat{R}_i) - \\dfrac{\\sigma_i^2}{2}$\n\n**Assumptions test**\n\n* Intercepts plot\n* Residuals plot\n* [CY Test](#a6-1)\n* [Correlation on adjacent LDFs](#a6-2)\n\n### A7 ATA Assumptions Tests\n\nTesting the Assumptions of Age-to-Age Factors - G. Venter\n\nStandards used in this paper\n\n* The $n$ for this paper excludes the first column\n\n* Coefficient $> 2 \\sigma$ is significant\n\n[Error measures](#a7-1) used are Adjusted SSE, AIC and BIC\n\nTestable implications from assumptions\n\n1) Statistical significance of $f(d)$\n2) Is there a better estimate for $q$ than $f \\times c$\n\n| $\\mathbf{q(w,d)}$ | Parameters | Comments |\n| ------ | ---------- | --- |\n| $f(d) c(w,d) + g(d)$ | $2m - 2$ | e.g. Least Squares |\n| Chainladder | $m - 1$ | | \n| $f(d)h(w)$ | $2m-2$ | e.g. BF |\n| $f(d)h$ | $m-1$ | e.g. Cape Cod |\n\n| Method | $\\mathbf{\\operatorname{Var}(q)}$ | $\\mathbf{f(d)}$: Col Parameters | $\\mathbf{h(w)}$: Row Parameters |\n| ------------------ | ------------------ | ------------------ | ------------------ |\n| BF (Constant Var, Least Square) | $a(d); p=q=0$ | $f(d) = \\dfrac{\\sum_w h^2 \\frac{q}{h}}{\\sum_w h^2}$ | $h(w) = \\dfrac{\\sum_d f^2 \\frac{q}{f}}{\\sum_d f^2}$ |\n| Cape Cod | $a(d); p=q=0$ | $f(d) = \\dfrac{\\sum_w h^2 \\frac{q}{h}}{\\sum_w h^2}$ | $h = \\dfrac{\\sum_\\Delta f^2 \\frac{q}{f}}{\\sum_\\Delta f^2}$ |\n| BF (Var $\\propto$ $fh$)| $a(d) \\cdot f \\cdot h; p=q=1$ | $f^2(d) = \\dfrac{\\sum_w h (\\frac{q}{h})^2}{\\sum_w h}$ | $h^2(w) = \\dfrac{\\sum_d f (\\frac{q}{f})^2}{\\sum_d f}$ |\n\n* Here we have to do the calculation recursively starting with either the row or column parameters\n\n3) Check residuals against $c(w,d)$\n4) Stability of $f(d)$ down the column\n5) No [correlation](#a7-2) among columns\n6) No particularly high or low diagonals\n\n### A8 Curve-Fitting\n\nLDF Curve-Fitting and Stochastic Reserving: A Maximum Likelihood Approach - D. Clark\n\n$x =$ average age of AY\n\n% paid to date growth function: $G(x \\mid \\omega, \\theta)$\n\nLoglogistic:  \n$G(x \\mid \\omega, \\theta) = \\dfrac{x^{\\omega}}{x^{\\omega} + \\theta^{\\omega}}$\n\nWeibull:  \n$G(x \\mid \\omega, \\theta) = 1- \\operatorname{exp}\\left\\{ { - \\left( \\dfrac{x}{ \\theta } \\right)^{\\omega}} \\right \\}$\n\nExpected Ultimate Loss\n\n* Method 1: Cape Cod  \n$Premium_{AY} \\times ELR$\n\n* Method 2: LDF  \n$ULT_{AY}$\n\nEstimate Future Emergence\n\n* Method 1: Cape Cod  \n$[G(y \\mid \\omega, \\theta) - G(x \\mid \\omega, \\theta)] \\times [Premium_{AY} \\times ELR]$\n\n* Method 2: LDF  \n$[G(y \\mid \\omega, \\theta) - G(x \\mid \\omega, \\theta)] \\times ULT_{AY}$\n\nTruncation\n\n* Method 1: Cape Cod  \nTruncated @ age $x_t$  \nReserve = On-level Premium $\\times ELR \\times [(G(x_t) - G(x)]$\n\n* Method 2: LDF  \nTruncated @ age $x_t$  \n$G'(x) = \\dfrac{G(x)}{G(x_t)}$\n\nVariance of reserve\n\n* Process Variance of $R$:  \n$\\sigma^2 \\sum_i \\mu_i$\n\n* Parameter Variance of $R$:  \n$\\operatorname{Var}(\\operatorname{E}[R]) = (\\partial R)'\\Sigma (\\partial R)$\n\n$\\dfrac{Variance}{Mean} = \\sigma^2 = \\dfrac{1}{n-p}\\sum\\limits_{i \\in \\Delta}^n\\dfrac{(c_i - \\mu_i)^2}{\\mu_i}$\n\n* $n =$ # of data points in triangle\n\n* $p =$ # of parameters\n\n    * Cape Cod $p=3$ ($\\omega, \\theta, ELR$)\n    * LDF $p=2 +$ # of AYs ($\\omega, \\theta,$ row parameters)\n\n* $c_i =$ actual incremental loss emergence\n\n* $\\mu_i =$ expected incremental loss emergence\n\n### A9 Risk Margin\n\nA Framework for Assessing Risk Margins - K. Marshall et al\n\nNeed framework because quantitative is not enough, need both quantitative and qualitative measures to examine the uncertainty\n\n* Quantitative analysis requires lots of data\n\n* Only captures historical risk\n\n* Does not capture risk that did not have an episode (of systemic risk) in the experience period\n\n**Independent**\n\nParameter and process variance model with stochastic model\n\n**Internal Systemic**\n\nKnow the [3 components](#a9-2)\n\nScore against best practice and calibrate to CoV\n\n* Know how to score given actual examples as in past exam\n* Hindsight analysis\n\n**Extnernal Systemic**\n\nKnow the different [risk categories](#a9-3)\n\n* And what lines they impact most\n\nUse benchmark similar to internal but select CoV directly\n\n**Correlation**\n\nRisk sources are independent of each other\n\nIndependent: assume independence across lines, weight by liabilities\n\nInternal: base on correlation matrix $\\Sigma$, again weighted by liabilities\n\nExternal: correlation between each valuation group and risk categories $\\Rightarrow$ then roll up to the risk categories and assume they are independent of each other\n\n**Risk Margin**\n\n$\\text{Risk Margin} = \\underbrace{\\mu}_{\\text{Expected Loss}} \\times \\underbrace{\\phi }_{\\text{CoV}} \\times \\underbrace{Z_{\\alpha}}_{0.67\\text{ for the }75^{th}\\text{ percentile}}$\n\nAdditional analysis\n\n* Sensitivity, scenario testing\n\n* [Internal benchmarking](#a9-1), important to know the relationships and consistency, been heavily tested in the past\n\n* External benchmarking\n\n* Hindsight and mechanical hindsight\n\nRegularity of review\n\n### A10 Bootstrap\n\nBootstrap Modeling: Beyond the Basics - Shapland Leong\n\nWorks with incremental triangles\n\n$m_{wd} = \\operatorname{exp} \\left [\\alpha_w + \\sum_j \\beta_j \\right]$\n\n$\\operatorname{Var}[q(w,d)] = \\phi m_{wd}^z$ with $z=1$ for ODP\n\n* 3 methods for [dispersion](#a10-1) factor\n\nFirst use either [GLM](#a10-4) or [simplified GLM](#a10-5) to get the mean and variance for each cell of the triangle and then do the [bootstrap](#a10-3) procedure\n\nPractical Issues\n\n* Negative incremental values (fit and simulate)\n    * 3 potential methods for adjustment when doing model fit\n    * Simulate with $Gamma(-m_{wd}, -\\phi m_{wd}) + 2m_{wd}$\n* Non-zero sum of residuals\n* N-year wtd average\n* Missing values\n* Outliers\n* Heteroskedasticity: [Hetero adjustment](#a10-2) \n* Partial latest year exposure or partial diagonal\n* Expsoure adjustment\n* Parametric bootstrap\n\nDiagnostics\n\n* Residual graph, normality test, outlier, parameter adj\n* Review results: s.e. and CoV should make sense\n\nOther\n\n* Multiple models: 2 methods for simulation\n* Model outputs\n* Correlations: location mapping or re-sort\n\n### A11 Expert Opinions\n\nObtaining Predictive Distributions for Reserves Which Incorporate Expert Opinions - R. Verrall\n\nWorks with incremental data\n\nModel is defined with losses following ODP and stochastic row parameters that follows gamma as prior\n\nBayesian BF calculation and assumptions\n\n* $\\operatorname{E}[c_{ij}] = Z_{ij} \\overbrace{[(\\lambda_j - 1) \\underbrace{D_{ij-1}}_{\\text{Actual up to }j-1}]}^{\\text{CL Results}} + (1- Z_{ij}) \\overbrace{[(\\lambda_j - 1) \\underbrace{M_i p_{j-1}}_{\\text{Expected up to }j-1}]}^{\\text{BF Results}}$\n\n* $Z_{ij} = \\dfrac{p_{j-1}}{\\beta_i \\varphi + p_{j-1}}$\n\n* $c_{ij} \\sim ODP(x_i \\cdot y_j, \\varphi)$\n\n* $x_i \\sim \\Gamma(\\alpha_i, \\beta_i)$\n\n* $\\operatorname{E}[x_i] = \\dfrac{\\alpha_i}{\\beta_i} = M_i$\n    \n* $\\operatorname{Var}[x_i] = \\dfrac{\\alpha_i}{\\beta_i^2}$\n\nStochastic column parameters calculation\n\n* $\\operatorname{E}[c_{ij}] = (\\gamma_i - 1) \\sum \\limits_{m=1}^{i-1} c_{m,j}$\n\n* [Calculate](#a11-1) $\\gamma$ pictorially\n\nUsing vauge or strong priors for model specification\n\n### A12 Reinsurance Reserving\n\nReinsurance Loss Reserving - Patrik\n\nProblems of reinsurance reserving:\n\n1. Longer report lag of claims\n2. Persistent Upward Development of Most Claim Reserves\n3. Reporting Pattern Differ Greatly\n4. Industry Statistics Not Useful\n5. Reports Received Lack Important Information\n6. Data Coding and IT System Problems\n7. Reserve to Surplus Ratio is Higher for Reinsurer\n\n6 Components of reinsurance reserve\n\n1. Case reserve reported by cedent\n2. Additional case reserve from reinsurer\n3. IBNER\n4. Pure IBNR\n5. Discount for future investment income\n6. Risk Load\n\nReinsurance reservering procedure:\n\n* Partition $\\Rightarrow$ Development Patterns $\\Rightarrow$ Estimate $\\Rightarrow$ Monitor and AvE\n\n* Partition priority\n\n* Considerations for short vs medium vs long tail lines\n\n* AvE\n\nStanard Buhlmann\n\n* Must on-level the historical premium and adjust to be pure premium\n\n    * Remove commissions and brokerage and internal expenses (but might not worth the effort)\n\n    * Remove any suspected rate level differences\n\n* $ELR = \\dfrac{\\sum C_k}{\\sum E_k p_k}$\n\n    * $C_k$: reported to date; $E_k$: Adj Prem; $p_k$: expected % reported to date\n\n* Estimate ELR using actual incurred loss\n\n* Sensitive to the accurary of the onlevel EP\n    \nCredibility IBNR Estimates\n\n* Weight the CL method with SB or BF\n\n* $R_k = Z \\cdot R_{CL} + (1-Z) \\cdot R_{SB}$\n\n* $Z_k = p_k \\cdot CF$, $CF \\in [0,1]$\n\n* $CF$ is the credibility factor, Benktander = 1, BF = 0\\\n\n### A13 Premium Asset\n\nEstimating the Premium Asset on Retrospectively Rate Policies - M. Teng and M. Perkins\n\nRetro formula\n\n$\\underbrace{P}_{\\text{Premium}} = [\\underbrace{BP}_{\\text{Basic Premium}} + (\\underbrace{CL}_{\\text{Capped Losses}} \\cdot \\underbrace{LCF}_{\\text{Loss Conversion Factor}})] \\cdot \\underbrace{TM}_{\\text{Tax Multiplier}}$\n\nUse PDLD to get the premium development based on the loss developement\n\nKnow what each term means like basic premium factor or charge\n\nFormula approach for PDLD\n\n$\\begin{align}\n  PDLD_1 =\\underbrace{\\left(\\frac{BP}{SP} \\right)}_{\\text{Basic Prem Factor}} \\frac{TM}{ELR \\cdot \\%Loss_1} + \\underbrace{\\left( \\frac{CL_1}{L_1}\\right)}_{\\text{Loss Capping Ratio}} \\cdot LCF \\cdot TM\n\\end{align}$\n\n* To adjust for being too responsive: $\\begin{align}\n  P_1 = \\underbrace{\\left( \\frac{BP}{SP} \\right) \\frac{TM}{ELR \\cdot \\%Loss_1}}_{\\text{Not }\\propto\\text{ Loss}_1} \\times \\operatorname{E}[L_1] + \\underbrace{\\left( \\frac{CL_1}{L_1} \\right) \\cdot LCF \\cdot TM}_{\\propto \\text{ Loss}_1} \\times L_1\n\\end{align}$\n\n* Subsequent PDLD: $\\begin{array}{cccl}\n  PDLD_n &= &\\dfrac{CL_n - CL_{n-1}}{L_n - L_{n-1}}&LCF \\cdot TM \\:\\:\\:\\:\\text{For }n>1\\\\\n  &= &\\dfrac{\\Delta CL}{\\Delta L}&LCF \\cdot TM\\\\\n\\end{array}$\n\nEmpirical approach for PDLD\n\n* Assume premium lags\n* Ratio selection\n* Cumulate PDLD based on a weighted average with expected % future report for each future periods\n* Know the practical application\n    * First adjustment period might cover more than one policy period\n\nKnow the Teng Perkins improvements and assumptions\n\n### A14 Bayesian MCMC\n\nStochastic Loss Reserving Using Bayesian MCMC Models - G. Meyer\n\nKnow how they selected the underlying data set to avoid insurer with change in operations or ones that best suit the model\n\nThe 3 main test used through out:\n\n1. [KS test](#a14-1)\n2. [p-p plot](#a14-2)\n3. [Freq vs percentile](#a14-3)\n\n![alt text](Section A/figures/Exam 7 Notes - 9.png)\n\nLCL:  \n$e^{\\mu_{wd}} = e^{\\alpha_w}e^{\\beta_d}$\n\n* Same variance parameter ($\\sigma_d$) for each column of *cumulative* loss\n\nCCL:  \n$\\mu_{wd} = \\alpha_w + \\beta_p + \\rho \\cdot \\left[ \\operatorname{ln}\\left(C_{w-1, d}\\right) - \\mu_{w-1,d} \\right]$\n\nCSR:  \n$\\mu_{wd} = \\alpha_w + \\left[ \\beta_d \\cdot (1-\\gamma)^{w-1}\\right]$\n\n* $\\gamma >0$ reflects increase in payment speed as $(1-\\gamma)^{w-1} < 1$\n* $\\gamma$ has less impact further out in the tail as there are less payments happening out there\n\nCIT:\n\n1) Uncorrelated log mean of each cell with CY trend  \n$\\mu_{wd} = \\alpha_w + \\beta_d + \\tau \\cdot(w+d-1)$\n\n2) Draw $Z_{wd} \\sim Lognormal(\\mu_{wd},\\sigma_d)$\n\n    * $\\sigma_1 > \\sigma_2 > \\cdots > \\sigma_{10}$\n    \n    * Smaller less volatile claims should be settled early\n\n3) $\\tilde{I}_{wd} \\sim Normal(Z_{wd},\\delta)$\n\n4) Add correlation between AYs for rows after the first  \n$\\tilde{I}_{wd} \\sim Normal(Z_{wd} + \\rho \\cdot (\\tilde{I}_{w-1,d} - Z_{w-1,d})\\cdot e^{\\tau},\\delta)$\n\nLIT: same as CIT but with $\\rho = 0$\n\n[Skewed](#a14-4) distributions\n\n## Section B\n\n### B1 Valuation Methods\n\nB1 P&C Insurance Company Valuation - R. Goldfarb\n\nThe key methods are DDM, FCFE, and AE\n\n* $V_0 = \\dfrac{\\mathrm{E}[Div_1]}{k - g}$\n* $FCFE = NI + (Non \\:Cash\\:Charges) - \\Delta Working \\:Capital - \\Delta Capital + \\Delta Debt$\n* $\\begin{align} V_0 = BV_0 + \\sum_{t=1} \\frac{(ROE_t - k)BV_{t-1}}{(1+k)^t}\\end{align}$\n\nYou can also look the P:Earning or P:Book multipes which have formulas that's based on the DDM and AE method\n\n* Watch out for some nuance on forward and trailing P:Earning\n\nAnother important bit is to determine the growth rate, if it's not given in a questions then you have to calculate $g$ over a couple years and select\n\n## Section C\n\n## Formula Appendix\n\n<a name=\"a6-1\"></a> **Calendar Year Test**\n\nStep 1) Rank the LDFs in each column\n\nStep 2) Label them S and L and the median is discarded\n\nStep 3) For each diagonal with at least 2 elements, $z = \\operatorname{min}(\\text{# of S}, \\text{# of L})$\n\nStep 4) Calculate $\\operatorname{E}[z_n]$ and $\\operatorname{Var}(z_n)$\n\n* $\\operatorname{E}[z_n] = \\dfrac{n}{2} - c_n$\n\n    * $n =$ # of elements in each diagonal **excluding** the throw away value\n    \n    * $c_n = {n - 1 \\choose m}\\frac{n}{2^n}$\n    \n    * $m = \\operatorname{floor}\\left[ \\dfrac{n-1}{2} \\right]$\n\n* $\\operatorname{Var}(z_n) = \\dfrac{n(n-1)}{4} - c_n (n-1) + \\operatorname{E}[Z_n] - \\operatorname{E}[z_n]^2$\n\n* $z \\sim$ Normal\n\nStep 5) See if $Z$ is in the CI based on the the above\n\n* $Z = \\sum_{diagonal} z$\n\n* Since $Z \\sim$ Normal, can sum the mean and variance by assuming independence\n\n* Test 95% CI: $\\operatorname{E}[Z] \\pm 2 \\times \\sigma$\n\n***\n\n<a name=\"a6-2\"></a> **Correlation of Adjacent LDFs**\n\nUse a relatively low threshold of 50%\n\nStep 1) Calculate Spearman's correlation for each pair of adjacent LDFs\n\n* $S = \\sum \\limits_{\\in rows} \\Big \\{ [Rank \\: Col \\: i \\: LDF] - [Rank \\: Col \\: j \\: LDF] \\Big \\}^2$\n\n* Rank from low to high (i.e. lowest is 1)\n\n* $T_k = 1 - \\dfrac{S}{n(n^2-1)/6}$\n\n    * $n =$ # of rows\n    \n    * For a 10 x 10 triangle, $k$ is at most 7 because there's only 9 LDFs so 8 pairs. And down to 7 because we don't use the pair with only 1 row\n    \nStep 2) Calculate $T$ for the whole triangle\n\n* $T = \\dfrac{\\sum T_k (n_k - 1)}{\\sum (n_k-1)} = \\dfrac{\\sum_k (I - k -1)T_k}{\\sum_k I - k -1}$\n\n    * $I =$ size of triangle\n    \n    * $k$ starts at 2\n    \n    * Formula gives more weight to $T_k$ with more data\n    \n    * Weight goes down to 1 for the last one, the bottom is just the sum of all the weights\n    \nStep 3) Compare $T$ with CI based on distribution\n\n* $\\operatorname{E}[T] = 0$\n\n* $\\operatorname{Var}[T] = \\dfrac{1}{(I-2)(I-3)/2}$\n\n* $\\operatorname{E}[T] \\pm Z \\sqrt{\\operatorname{Var}(T)}$\n\n* Use $Z = 0.67$ for range of [25%, 75%]\n\n* Do not reject the $H_0$ of uncorrelated LDFs if the $T$ is in the CI\n\n***\n\n<a name=\"a7-1\"></a> **Error Measures**\n\nAdjusted SSE = $\\dfrac{SSE}{(n-p^2)}$\n\n$AIC \\approx SSE \\times e^{2p/n}$\n\n$BIC \\approx SSE \\times n^{p/n}$\n\n* $n =$ # of predicted data points **EXCLUDING 1st column**\n\n* $p =$ # of parameters\n\n* $SSE = \\sum (A - E)^2$\n\n    * Here you exclude the first column when calculating the difference\n    \n***\n\n<a name=\"a7-2\"></a> **Pearson Correlation**\n\nCorrelation $r = \\dfrac{\\sum \\tilde{x} \\tilde{y}}{\\sqrt{\\sum \\tilde{x}^2\\sum \\tilde{y}^2}}$\n\n* $\\tilde{x} = x - \\bar{x}$\n\n* $\\tilde{y} = y - \\bar{y}$\n\nFor the adjacent LDFs, we need to subtract 1 out first\n\nTest statistics: $T = r \\sqrt{ \\dfrac{n-2}{1-r^2} }$\n\n* $T \\sim t_{n-2}$\n\n* Look up the t-value from table for 90%\n\n* If the absolute value of $T <$ table value $\\Rightarrow$ Not correlated\n\n***\n\n<a name=\"a10-1\"></a> **Dispersion Factor**\n\nPearson residual:  \n$\\begin{array}{cccl}\n  r_{wd} & = & \\dfrac{A - E}{\\sqrt{\\operatorname{Var}(A)}} & \\\\\n  & = & \\dfrac{q(w,d) - m_{wd}}{\\sqrt{\\phi m_{wd}}} & \\text{Mean & Var Assumptions Above}\\\\\n  & \\propto & \\dfrac{q(w,d) - m_{wd}}{\\sqrt{m_{wd}}} & \\text{Since }\\phi \\text{ is constant for all}\\\\\n\\end{array}$\n\nStandard:  \n$\\phi = \\dfrac{\\sum r_{wd}^2}{n-p}$\n\n* $n =$ # of data points (including first column)\n\n* $p =$ # of parameters\n\n    * $2m-1$: one for each row, one for each column minus first column\n\nEngland & Verrall:  \n$\\phi^{EV} = \\dfrac{\\sum \\left(r^{EV}_{wd}\\right)^2}{n-p}$\n\n* $\\begin{array}{llllc}\n    r_{wd}^{EV} & = & r_{wd} &\\times &f \\\\\n    & = & r_{wd} &\\times &\\sqrt{\\dfrac{n}{n-p}}\\\\\n  \\end{array}$\n\nStandarized Residuals:  \n$\\phi^H = \\dfrac{\\sum \\left(r_{wd}^H \\right)^2}{n}$\n\n* $\\begin{array}{lllc}\n    r_{wd}^H &= r_{wd} &\\times &f_{wd}^H \\\\\n    &= r_{wd} &\\times &\\sqrt{\\dfrac{1}{1-H_{ii}}} \\\\\n  \\end{array}$\n\n* $H_{ii}$ is the diagonal of the Hat Matrix Adjustment Factor: $H = X (X^TWX)^{-1}X^TW$\n\n    * The diagonal is labelled by going down the column of the triangle from left to right\n    \n    * The denominator might actually be $n-2$ but for the purpose of exam just use $n$ as stated in the paper\n\n***\n\n<a name=\"a10-2\"></a> **Hetero-Adjustment**\n\nGroup the residuals then calculate the $\\sigma$ of the residuals in each group and scale up\n\nHetero-adjustment factor: $h^i$ = the largest $\\sigma$ $\\div$ each group's $\\sigma$\n\n$r_{wd}^{i,H} = r_{wd} \\times f_{wd}^H \\times h^i$\n\n* Residual $\\times$ Hat Matrix Factor $\\times$ Hetero Factor\n\nNeed to divide the sampled residual by $h^i$ to reflect the variability of group $i$\n\n* $q^{i*}(w,d) = m_{wd} + \\dfrac{r^{i*}}{h^i}\\sqrt{m_{wd}}$\n\n***\n\n<a name=\"a10-3\"></a> **Bootstrap Procedure**\n\nOnce we have the mean and residuals in each cell, repeat below:\n\n1) Create a sampled $triangle^*$ from the residuals and the means\n\n    * Sample from residuals since data needs to be $iid$ for bootstrap\n    \n    * Use pearson residuals\n    \n    * Simulated loss: $q^*(w,d) = m_{wd} + r_p \\sqrt{m_{wd}^z}$\n    \n        * Simulate by sampling residuals with replacement\n    \n    * Estimate $\\phi$ for step 4: $\\phi = \\dfrac{\\sum r^2}{n-p}$\n        \n2) Determine parameters from $triangle^*$: $(\\alpha_i, \\beta_j)$\n\n    * Use either GLM or Simplified GLM to project ultimate loss\n\n3) Calculate mean and variance: $(m_{wd}, \\phi m_{wd})$\n\n    * For GLM use $m_{wd} = \\operatorname{exp} \\left [\\alpha_w + \\sum_j \\beta_j \\right]$\n    \n    * For Simplified GLm, back out the $c^*(w,d)$ by $\\dfrac{Ult_w}{CDF_d}$ then get the $m_{wd}$\n    \n    * For variance $\\phi m_{wd}^z$\n\n4) Add process variance: draw losses from $Gamma(m_{wd}^*,\\phi m_{wd}^*)$\n\n    * Randomly draw from the distribution for each cell\n\n5) Calculate simulated unpaid: sum of bottom half of triangle\n\n***\n\n<a name=\"a10-4\"></a> **GLM**\n\nFor a $3\\times 3$ triangle:\n\nLog Actual incremental losses\n\n$Y = \\begin{bmatrix}\n    ln[q(1,1)] \\\\\n    ln[q(2,1)] \\\\\n    ln[q(3,1)] \\\\\n    ln[q(1,2)] \\\\\n    ln[q(2,2)] \\\\\n    ln[q(1,3)] \\\\\n\\end{bmatrix}$\n\nSolution Matrix $W$\n\n$\\begin{array}{ccccc}\n  W & = & X &\\times & A \\\\\n  & & \\alpha_1 \\:\\:\\: \\alpha_2 \\:\\:\\: \\alpha_3 \\:\\:\\: \\beta_2 \\:\\:\\: \\beta_3 & &\\\\\n  \\begin{bmatrix}\n    ln[m_{11}] \\\\\n    ln[m_{21}] \\\\\n    ln[m_{31}] \\\\\n    ln[m_{12}] \\\\\n    ln[m_{22}] \\\\\n    ln[m_{13}] \\\\\n  \\end{bmatrix}\n    & =\n      &\n      \\begin{bmatrix}\n        1 & - & - & - & - \\\\\n        - & 1 & - & - & - \\\\\n        - & - & 1 & - & - \\\\\n        1 & - & - & 1 & - \\\\\n        - & 1 & - & 1 & - \\\\\n        1 & - & - & 1 & 1 \\\\\n      \\end{bmatrix}\n        & \\times\n          &\n          \\begin{bmatrix}\n            \\alpha_1 \\\\\n            \\alpha_2 \\\\\n            \\alpha_3 \\\\\n            \\beta_2 \\\\\n            \\beta_3 \\\\\n          \\end{bmatrix}\n\\end{array}$\n\n* $W =$ Solution Matrix\n\n* $X =$ Design Matrix\n\n    * Defines the parameters used to estimate the losses\n\n* $A =$ parameters\n\n    * Solve for $A$ by minimizing the difference^2^ (SSE) between $W$ and $Y$\n    \n    * Use recursive Newton-Raphson method to find the best fit parameters\n    \n***\n\n<a name=\"a10-5\"></a> **Simplified GLM**\n\nGLM model = Chainladder w/ volume-weighted averages when:\n\n* Variance $\\propto$ Mean\n* $\\varepsilon \\sim$ Poisson; Poisson error\n* Parameter for each row and column (x 1^st^ column)\n\n***\n\n<a name=\"a14-1\"></a> **KS Test**\n\n$H_0$: Distribution of $p_i$ is uniform\n\n$D = \\operatorname{max} \\limits_i \\mid p_i - e_i \\mid$\n\n* Maximum difference between the predicted and expected percentiles\n\nReject $H_0$ if $D > \\dfrac{136}{\\sqrt{n}}\\%$ @5% confidence level\n\n* e.g. for $n = 50$: 19.2%; $n=200$: 9.6%\n\n***\n\n<a name=\"a14-2\"></a> **p-p plot**\n\n![alt text](Section A/figures/Exam 7 Notes - 10.png)\n\n* Model is too light tailed: Shallow slope near corner and steep in the middle\n* Model is too heavy tailed: Steep slope near corner and shallow in the middle\n* Model is biased upwards: Bow down\n\n***\n\n<a name=\"a14-3\"></a> **Freq Percentile Plot**\n\n![alt text](Section A/figures/Exam 7 Notes - 12.png)\n\n* Blue line is based on the uniform $e_i$ set at $\\frac{n}{10}$ for the 10 deciles\n\n***\n\n<a name=\"a14-4\"></a> **Skewed Normal**\n\nSkewed Normal:  \n$X = \\mu + (\\omega \\cdot Z) \\cdot \\delta + (\\omega \\cdot \\varepsilon) \\cdot \\sqrt{1 - \\delta^2}$\n\n* $\\varepsilon \\sim Normal(0,1)$\n\n* $Z \\sim Truncated \\: Normal_{[0,\\infty]} (0,1)$\n\n* $\\delta$ is the weight between $\\varepsilon$ and $Z$\n\n* $\\omega$ is the standard deviation\n\nMixed Lognormal-Normal:  \n$X \\sim Normal(Z,\\delta)$\n\n* $Z \\sim Lognormal(\\mu,\\sigma)$\n\n* Mixed $ln - n$ Distribution\n\n***\n\n<a name=\"a5-1\"></a> **Base Triangle**\n\nStep 1:  \nSetup the trend triangle: Trend = AY Trend $\\times$ CY Trend\n\n* The 1.000 typically starts at the top left corner but it doesn't have to\n\nStep 2:  \nDetermine *unlimited* mean for each cell in triangle (Avg sev paid to date)\n\ni) Based on mean for the latest AY (bottom row)\n\n    * $\\operatorname{E}[C_{nj}] = \\theta_j$\n\n    * $C_{nj} \\sim \\Phi_{nj} = Exp(\\theta_j)$\n    \nii) Detrend back up from the bottom row to fill the whole square\n\n    * Usually just need four points per the LDF conversion formula\n\nStep 3:  \nCalculate LEV for each cell in triangle $L$ and the last row for $B$\n\n* $LEV(X; \\Phi \\sim Exp(\\theta)) = \\theta \\: \\left[ 1 - \\operatorname{exp}\\left\\{-\\left(\\dfrac{x}{\\theta}\\right)\\right\\} \\right]$\n\n* Use the \"square\" of mean $\\theta$ calculated from Step 2\n\nStep 4:  \nCalculate the adjusted triangle\n\n* Convert triangle of actual losses by dividing it's LEV at layer $L$ then times it's LEV at layer $B$\n\n* $C_{ij}' = \\underbrace{C_{ij}^L}_{\\text{Cum paid @ L}} \\times \\underbrace{\\dfrac{LEV(B;\\Phi_{nj})}{LEV(L;\\Phi_{ij})}}_{\\text{ILF w/ on-level to }n}$\n\nStep 5:  \nSelect base layer LDFs\n\n***\n\n<a name=\"a5-2\"></a> **Convert LDFs**\n\n$\\begin{align}\n  F_{ij}^X = F_{nj}^B \\times \\frac{LEV(X;\\Phi_{i\\infty})\\div LEV(B;\\Phi_{n\\infty})}{LEV(X;\\Phi_{ij}) \\div LEV(B;\\Phi_{nj})}\n \\end{align}$\n\n![alt text](Section A/figures/Exam 7 Notes - 13.png)\n\n***\n\n<a name=\"a11-1\"></a> **$\\gamma$ Calculation**\n\n$\\gamma_1 = 1.000$ always\n\nFirst calculate LDFs and calculate % unpaid and a-priori for each AY\n\n* $U_i$ = a-priori ultimate based on LDF for AY $i$\n\n* $q_i$ = % unpaid for AY $i$\n\n![alt text](Section A/figures/Exam 7 Notes - 3.png)\n\nNote that in step 4 above if you don't need the individual cells you can just take the $U_3 q_3$\n\n***\n\n<a name=\"a9-1\"></a> **Internal Benchmarking**: Compare CoVs within between OCL and PL and also with other valuation groups\n\n*Independent risk*: \n\n* Large liability will have smaller CoV due to law of large numbers\n    \n* Short tail line will have a small CoV as well due to less volatility\n    \nTherefore:    \n    \n* Outstanding Claims Liability: $\\phi_{short \\: tail} < \\phi_{long \\: tail} < \\phi_{long \\: tail, \\: small \\: book}$\n\n* Premium Liability - Long Tail: OCL > PL $\\Rightarrow$ $\\phi_{OCL} < \\phi_{PL}$\n\n    * Because there are many AYs of claims in the reserves\n    \n* Premium Liability - Short Tail: OCL < PL $\\Rightarrow$ $\\phi_{OCL} > \\phi_{PL}$\n\n    * Because OCL is small\n    \n    * LoB with significant event risk will have different risk profiles for the PL and OCL\n    \n*Internal Systemic*: \n\n* If the methods to estimate liabilities is similar across valuation groups, we would expect the CoV to be similar for classes that have similar claim payment patterns\n\n*External Systemic*:\n\n* Main sources are higher for long tail lines; Event risk is higher for property; Liability for HO can also be significant\n\n***\n\n<a name=\"a9-2\"></a> 3 components of **internal systemic** risk:\n\n1) Specification Error:  \nFrom not perfectly modeling the insurance process because it's too complicated or just don't have the data\n\n2) Parameter Selection Error:  \nTrend is particularly difficult to measure\n\n3) Data Error:  \nLack of data, lack of knowledge of the underlying pricing, u/w, and claim management process, inadequate knowledge of portfolio\n\n***\n\n<a name=\"a9-3\"></a>\n\n**Risk categories** for external\n\n* Economic and Social Risks:  \nInflation, social trends \n\n* Legislative, Political Risks, Claims Inflation Risks:  \nChange in law, frequency of settlement vs suits to completion, loss trend (Long tail lines)\n\n* Claim Management Process Change Risk:  \nChange in process of managing claims e.g. case reserve practice\n\n* Expense Risk:  \nCost of managing a run-off book\n\n* Event Risk:  \nnatural or man-made CAT (Premium liabilities for property)\n\n* Latent Claim Risk:  \nClaim from source not currently considered to be covered\n\n* Recovery Risk:  \nRecoveries from reinsurers or non-reinsurers",
    "created" : 1462231691920.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1285005941",
    "id" : "36D3409E",
    "lastKnownWriteTime" : 1462238391,
    "last_content_update" : 1462238391368,
    "path" : "~/Documents/Exam 7 2016 Notes/Overall_Summary.Rmd",
    "project_path" : "Overall_Summary.Rmd",
    "properties" : {
        "docOutlineVisible" : "1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}