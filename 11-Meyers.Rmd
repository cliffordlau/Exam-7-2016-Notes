# Stochastic Loss Reserving Using Bayesian MCMC Models - G. Meyer

Know how to read all the [test](#meyers-test): KS-test, $p-p$ plot and Freq vs Count plot

Know the form of all the methods in the [model](#model-summary-meyers)

## Introduction & Synopsis

Applying different estimate methods to 200 triangles and comparing the actual outcomes to the predicted distribution to see if the models accurately estimates the distribution of outcomes

Models examined:

* Estimate from Mack and ODP do not have enough variability

* Incurred losses with variable row parameters and AY correlation $\rho$ is sufficient

* Paid losses requires variable row parameters and change in settlement rate parameter $\gamma$

**Three reasons model doesn't predict well**:

1) Insurance loss environment has experienced changes that are not observable at the valuation date

    i.e. There would be different "black swan" events that invalidate any attempt to model loss reserves

2) There could be other models that better fit the existing data

3) Data used to calibrate the model is missing crucial information needed to make a reliable prediction

    e.g. Changes in the way the underlying business is conducted, like changes in claim processes of changes in the direct/ceded/assumed reinsurance composition of the claims triangle

If we can find better model and/or better data we can rule out 1)

* If we review many models and none of them validate it gives 1) credence but does not confirm

## Data Set

200 triangles from Sch P 1997 and reviewed 10 years later

* Paid and incurred

* 50 triangles from 4 LoB (Commercial Auto, Personal Auto, WC, Other Liab)

**Potential pitfalls** in sample triangle selections

1) Insurer with significant changes to their books over the exposure period would violate the assumptions of the model and should be excluded

    * **Solution**: Use consisteny of NEP and Net:Gross Premium to establish stability in book
    
    * Use CoV to establish consistency
    
    * Pick triangles with CoV below a threshold

2) Need to avoid selecting datasets that best suit the model e.g. removing "outliers" from the data

    * **Solution**: Choose the company in an automated and well defined manner

Losses are considered fully developed at 10 years so in practice the paid and incurred ultimate is slightly different

## Testing Procedure {#meyers-test}

We are testing the **process** of each method and not the results of any one distribution generated from the method

* Since there's only 1 actual outcome for each triangle we test

* Focus on the process (the different methods) that gives us the distribution

* Compare the predicted percentiles (from the methods) against the expected percentile

**Testing Procedure**

* Given $N$ triangles and their actual outcome in 10 years

* Generate $N$ sets of distribution (from the $N$ triangles using one of the methods) and determine the predicted percentile $p_i$ based on the predicted distribution

    * i.e. see where the actual outcome lands on our predicted distribution

* The distribution of the $N$ predicted percentiles $p_i$ should follow a uniform distribution if the model is accurate, so we rank them to form $\{p_i\}$

$$\{p_i\} = \{p_1,...,p_n\}$$

* The expected percentiles $\{e_i\}$ should run from $\frac{1}{n+1}$ to $\frac{n}{n+1}$
    
$$\{e_i\} = \left\{ \dfrac{1}{n+1}, \dfrac{2}{n+1},...,\dfrac{n}{n+1} \right\}$$

### Kolmogorov-Smirnov Test {#meyers-ks-test}

For the *KS* test we'll compare $\{ p_i \}$ with $\{ f_i \}$

$$\{f_i\} = \left\{ \dfrac{1}{n}, \dfrac{2}{n},...,\dfrac{n}{n} \right\}$$

$H_0$: Distribution of $p_i$ is uniform

**Test statistics** for maximum difference between the predicted and expected percentiles

\begin{equation}
  D = \max \limits_i \mid p_i - e_i \mid
  (\#eq:ks-test-meyers-diff)
\end{equation}

**Reject** $H_0$ @5% confidence level if:

\begin{equation}
  D > \dfrac{136}{\sqrt{n}}\%
  (\#eq:ks-test-meyers-criteria)
\end{equation}

* i.e. For $n = 50$ $\Rightarrow$ 19.2%; $n=200$ $\Rightarrow$ 9.6%

**Examples**

Table: (\#tab:ks-test-example) Kolmogorov-Smirnov test example

| $f_i$ | $p_i$ | $abs\{ p_i - f_i \}$ |
|:---------:|:---------:|:------------------:|
| (1) | (2) | (3) |

1. Col (1): $\{f_i\} = \left\{ \dfrac{1}{n},...,\dfrac{n}{n}\right\}$

2. Col (2): $\{p_i\}$ = $p_i$ from each realization of the triangles sorted in ascending order

3. Col(3) = Absoluate difference between the first two columns

4. $D$ is the maximum value from column (3)

5. Compare $D$ with $\dfrac{136}{\sqrt{N}}$

    If $D$ is less than the critical value we do not reject the $H_0$ that $\{ p_i \}$ is uniform

```{remark}
Technically based on Klugman you test against both:

$$\{f^+_i\} = \left\{ \dfrac{1}{n}, \dfrac{2}{n},...,\dfrac{n}{n} \right\}$$

and 

$$\{f^-_i\} = \left\{ \dfrac{0}{n}, \dfrac{2}{1},...,\dfrac{n-1}{n} \right\}$$
```

Alternatively, we can use *Anderson-Darling* test that focuses on the tail

* But it failed all the models therefore we do not use it as it does not help in model comparison

### *p-p* Plot

We plot the $p-p$ plot with $e_i$ vs $p_i$ to diagnosis

* Dark blueline is what is expected from uniform distribution

* Light blueline is the critical value for a given $n$ from the [*KS* test](#meyers-ks-test) above

```{r meyers-p-p-plot, echo = FALSE, out.width='100%', fig.show='hold', fig.cap='p-p plot'}
knitr::include_graphics('figures/Exam-7-Notes-10.png')
```

* Model is too light tailed: Shallow slope near corner and steep in the middle

* Model is too heavy tailed: Steep slope near corner and shallow in the middle

* Model is biased upwards: Bow down

### Percentile Histogram

We plot a (flipped) histogram

* y-axis being the predicted percentile

* x-axis is the frequency

* Bins with width 0.1 (so 10 bins)

* Verticle blue line represent the expected freqency based on uniform $\{e_i\}$

    * Expected frequency = $\dfrac{\text{# of points}}{\text{# of bins}}$= $\dfrac{n}{10}$

```{r meyers-freq-perc-plot, echo = FALSE, out.width='100%', fig.show='hold', fig.cap='Percentile Histogram'}
knitr::include_graphics('figures/Exam-7-Notes-12.png')
```

## Models Comparison Summary {#model-summary-meyers}

7 models:

* [Mack](#meyers-mack): See [Mack-1994](#cl-ass)

* [England & Verall ODP](#meyers-odp): See [Shapland](#odp-glm-para), but doesn't have the [residual adjustments](#odp-res)

* [Leveled Chain-Ladder](#meyers-lcl) (LCL): Add variability to the row parameter

* [Correlated Chain-Ladder](#meyers-ccl) (CCL): Add AY correlation $\rho$

* [Leveled Incremental Trend](#meyers-lit) (LIT): Use skewed distribution and CY trend $\tau$

* [Correlated Incremental Trend](#meyers-cit) (CIT): LIT with added AY correlation $rho$

* [Changing Settlement Rate](#meyers-csr) (CSR): LCL with speed up claims closure $\gamma$

```{r meyers-models-overview, echo = FALSE, out.width='100%', fig.show='hold', fig.cap='Overview of models'}
knitr::include_graphics('figures/Exam-7-Notes-9.png')
```

* Mack is the only one that does not have a base form of $\mu_{wd} = \alpha_w + \beta_d$

* ODP is the England & Verall Bootstrap

### Mack Model {#meyers-mack}

See procedure from Mack-1994 on the [log-normal CI](#mack-CI-methods)

Variance is the product of mean in the cell and a constant that varies by column $\operatorname{Var}\left (c_{i,k+1} \mid c_{i,1} \cdots c_{i,k}\right ) = \alpha_k^2 \: c_{i,k}$

**Incurred**

Light on both tail $\Rightarrow$ Does not have enough variability in it's predicted distribution

**Paid**

Similar to ODP, biased high on personal auto and light left tail on WC

### ODP Bootstrap {#meyers-odp}

EV ODP forecasts log incremental losses $\Rightarrow$ Only suitable for paid losses

* Can handle occasional negative losses as long as the $\sum$ column is positive

Same procedure as Shapland Leong paper

Overall shows biased high

## Bayesian Models (Cumulative)

**Inputs**

Prior distribution is needed for each parameters

* **Wide** priors (diffuse)
* **Narrow** priors: Use expert knowledge in selecting mean and variance of the parameters

Parameters:

* $\alpha_w$: row parameters
* $\beta_d$: column parameters
* $\sigma_d$: variance parameters (mostly constant across columns)
* $\tau$: trend
* $\gamma$: change in closure rate

Data: Paid or incurred Triangle

**Output**

The posterior distribution of the parameters is expressed as simulated outputs (not closed form distribution)

### Leveled Chain Ladder {#meyers-lcl}

LCL with incurred data

**Multiplicative** model (based on additive exponential)

Each cell is $e^{\mu_{wd}} = e^{\alpha_w}e^{\beta_d}$

* Log mean of each cell is $\mu_{wd} = \alpha_w + \beta_d$

* $\beta_{10} = 0$ so we have 100% at 10 years

* $\beta_d < 0$ most of the time, represents % paid to date

Uses **cumulative** data $C_{wd}$

Same variance parameter ($\sigma_d$) for each column of *cumulative* loss

$\hookrightarrow$ Highest variability @ early ages $\sigma_1 > \sigma_2 > \cdots > \sigma_{10}$

* Variance varies by column only (not by AYs)

$\alpha_w$ is a random variable

* Not value on the diagonal (incurred to date)
* Model select an $\alpha_w$ for each instance of the simulation based on wide priors
* Main feature of this model for adding variability

Can compare the variability (s.d.) with Mack by plotting the log(s.d.) of the 2 models

Model still does not capture the tail appropriately

### Correlated Chain-Ladder {#meyers-ccl}

Build upon the Leveled Chain-Ladder by adding $\rho$ to create correlation of losses in one AY and the **previous** AY

Uses **cumulative** data $C_{wd}$

$\mu_{wd} = \alpha_w + \beta_p + \rho \cdot \left[ \operatorname{ln}\left(C_{w-1, d}\right) - \mu_{w-1,d} \right]$

* Higher losses in one row $\rightarrow$ higher expected losses in the following row

* Prior is still wide priors

* The correlation here is what drives the additional variability

**Incurred**

Results and k-s test show that this model is sufficient

**Paid**

Worst than ODP and Mack, biased high for all lines

### Changing Settlement Rate {#meyers-csr}

Based on LCL with $\gamma$ that allows for speed up in claim payments

Uses **cumulative** losses

Logmean for each cell:  
$\mu_{wd} = \alpha_w + \left[ \beta_d \cdot (1-\gamma)^{w-1}\right]$

$\gamma >0$ reflects increase in payment speed as $(1-\gamma)^{w-1} < 1$

$\gamma$ has less impact further out in the tail as there are less payments happening out there

Model fits one $\gamma$ for the whole triangle

**Results**

Overall fits well, slightly biased high on Personal Auto but is a big improvement over the other models

## Skewed Distribution

Use incremental data as trends act on incremental loss, which has the following properties:

* Skewed right
* Occasionally negative

### Skewed Normal Distribution

Blend of normal and truncated normal

$X = \mu + (\omega \cdot Z) \cdot \delta + (\omega \cdot \varepsilon) \cdot \sqrt{1 - \delta^2}$

* $\varepsilon \sim Normal(0,1)$

* $Z \sim Truncated \: Normal_{[0,\infty]} (0,1)$

* $\delta$ is the weight between $\varepsilon$ and $Z$

* $\omega$ is the standard deviation

Skewness = 0.995; Not used by the Meyers as it is not skewed enough

### Mixed Lognormal-Normal

$X \sim Normal(Z,\delta)$

* $Z \sim Lognormal(\mu,\sigma)$

* Mixed $ln - n$ Distribution

This can create distribution more skew than the skewed normal and can also have negative values

## Bayesian Models (Incremental)

Model applies trend and therefore uses incremental data

Correlated Incremental Trend Model

Changing Settlement Rate Model

### Correlated Incremental Trend {#meyers-cit}

Single CY trend parameter $\tau$

* Mixed lognormal-normal distribution

Include correlation between AY similar to CCL method

Steps for the method:

1) Uncorrelated log mean of each cell with CY trend  
$\mu_{wd} = \alpha_w + \beta_d + \tau \cdot(w+d-1)$

2) Draw $Z_{wd} \sim Lognormal(\mu_{wd},\sigma_d)$

    * $\sigma_1 > \sigma_2 > \cdots > \sigma_{10}$
    
    * Smaller less volatile claims should be settled early

3) $\tilde{I}_{wd} \sim Normal(Z_{wd},\delta)$

4) Add correlation between AYs for rows after the first  
$\tilde{I}_{wd} \sim Normal(Z_{wd} + \rho \cdot (\tilde{I}_{w-1,d} - Z_{w-1,d})\cdot e^{\tau},\delta)$

**Parameters restrictions**

$\tau$:

* Prior $\sim Normal(0,3.2%)$
* Without restriction it was forecasting very negative trend which is offset by higher $\alpha$ and $\beta$

$\sigma_d$:

* Prior $\sigma_1 \sim Uniform(0,0.5)$
* Prior $\sigma_d \sim Uniform(\sigma^2_{d-1},\sigma^2_{d-1} +0.1)$
* Limit the speed $\sigma_d$ can increase, very high $\sigma_d$ can lead to unreasonably high simulate results

**Results**

Losses not much smaller than CCL while we would like it to be much smaller as CCL was biased high

$\rho$ is lower than from CCL

Strong negative correlation between trend $\tau$ and level parameters $\alpha_w + \beta_d$

* With small data set it is hard for the model to distinguish the AY level + development vs trend

Model showed no improvement over Mack or ODP

### Leveled Incremental Trend {#meyers-lit}

Same as CIT but with $\rho = 0$

Results similar to CIT with lower standard deviation


## Process, Parameter, and Model Risk

$\underbrace{\text{Variance}}_{\operatorname{Var}(X)} = \underbrace{\operatorname{E}[\text{Process Variance}]}_{\operatorname{E}_{\theta}[\operatorname{Var}[X|\theta]]}+\underbrace{\operatorname{Var}[\text{Hypothetical Mean}]}_{\operatorname{Var}_{\theta}[\operatorname{E}[X|\theta]]}$

Typically the parameter risk is much larger than process risk

Model risk is the risk of not selecting the right model

* For known unknown, weight average of multiple models
* If the weight vary a lot in the posterior distribution than this could be an indication of model risk
    * This turns into more or less parameter risk
    
Should focus on the total risk

## Conclusion

Goal of the paper was to test the predictive accuracy of various models, both mean and distribution of outcomes

* Not on the reserve estimate for individual insurers

### Results Summary

**Incurred Data**

Mack understates variability as it assumes AYs are independent

CCL introduces AY correlation and does relatively well

**Paid Data**

Mack and ODP were biased high as well as CCL

There were change in environment that is not captured

* Calendar year trend: LIT and CIT still biased high
* CSR: significantly less bias than LIT and CIT (except for PA still failed)

Mack and ODP did better than CCL, LIT and CIT

### Final Comments

Results were for specific annual statement year 1997

* Possible the speed up was specific to the period $\Rightarrow$ CSR could potentially useless for another year

Could use more narrow prior to incorporate knowledge of insurer's business operation and obtain superior results

## Appendix B: Intro to Bayesian MCMC Models

```{block, type='rmdtip'}
Just additional background information, not part of exam syllabus
```

```{definition, markov-chain-meyers}
Markov Chain

Random process where the transition to the next state depends only on its current state and not on prior states

A Markov chain $X_t$ for $t=1,2,...$ is a sequence of vectors satisfying the property that

$$\Pr(X_{t+1} = x \mid X_1 = x_1, X_2 = x_2,...,X_t = x_t) = \Pr(X_{t+1} \mid X_t = x_t)$$

```

Key properties of Markov chain ofr Bayesian MCMC

* Ergodic class of Markov chain, for which vectors $\{X_t\}$ approaches a limiting distribution

    As $T$ increases, the distribution of $\{X_t\}$ for all $t>T$ approaches a unique limiting distribution
    
* Markov chains used in Bayesian MCMC (e.g. Metropolis Hastings algorithm) are members of Ergodic class

Let $x$ be a vector of observations and let $y$ be a vector of parameters in a model

* In Bayesian MCMC, the Markov chain is defined in terms of the prior distribution $p(y)$ and the conditional distribution $f(x \mid y)$

* The limiting distribtuion is the posterior distribution $f(y \mid x)$

* If we let the chain run *long enough*, the chain will randomly visit all states with frequency that is proportional to their posterior probabilities

The operative phrase above is **long enough**, which means in practice:

1. Develop algorithm for obtaining a chain that is *long enough* as quickly as possible

2. Develop criteria for being *long enough*

### How Bayesian MCMC works in practice

1. User specifies $p(y)$ and $f(x \mid y)$

2. User selects a starting vector $x_1$

    Using computer simulation, runs the Markov chain through a sufficiently large number, $t_1$, of iterations
    
    This first phase of the simulation is called the "adaptive" phase
    
    * The algorithm is automatically modified to increase its efficiency
    
    * See below on the [Metropolis-Hasting alogrithm](#meyers-mh-algo)
    
3. User runs an additional $t_2$ iterations

    This is the "burn-in" phase
    
    $t_2$ is selected to be high enough so that a sample taken from subsequent $t_3$ periods represents the posterior distribution
    
4. User then runs an additional $t_3$ iterations and then takes a sample, $\{ x_t \}$ from the $(t_2 + 1)$^th^ step to the $(t_2 + t_3)$^th^ step to represent the posterior distribution $f(y \mid x)$

5. From the sample, we can constructs various statistics of interest that are relevant to the problem addressed by the analysis

### Metropolis-Hastings Algorithm {#meyers-mh-algo}

Most common algorithms for generating Bayesian Markov chains are variants of the Metropolis-Hastings algorithm

*Given*: $p(y)$ and $f(x \mid y)$

The algorithm introduces a 3^rd^ distribution $J(y_t \mid y_{t-1})$:

* "Proposal" or "jumping" distribution

Given a parameter vector $y_{t-1}$ the algorithm generates a Markov chain by the following steps:

1. Select a candiate value $y^*$, at random from the proposal distribution $J(y_t \mid y_{t-1})$

2. Computer the ratio

$$R \equiv R_1 \times R_2 = \dfrac{f(x \mid y^*) \cdot p(y^*)}{f(x \mid y_{t-1}) \cdot p(y_{t-1})} \times \dfrac{J(y_{t-1} \mid y^*)}{J(y^* \mid y_{t-1})}$$

3. Select $U$ at random from $U(0,1)$ distribution

4. If $U < R$ then set $y_t = y^*$, else $y_t = y_{t-1}$

```{remark}


* $R_1$ represents the ratio of the posterior probability of the proposal $y^*$ to the posterior probability of $y_{t-1}$

* The higher the value of $R_1$, the more likely will be accepted into the chain

* Regardless of how the proposal density distribution is chosen, the distribution of $y_t$ can be regarded as a sample from the posterior distribution, after a suitable burn-in period
```

**Example**

* We can look at *trace plots* to look at the value of the parameter as the chain progresses

* More on how it might break in the paper...

* The key is to be able to scale the proposal distribution to minimize autocorrelation

    * This is difficult with many parameters

**Minimizing autocorrelation** in Metropolis-Hasting

* A good statistics to look at is the acceptance rate of $y^*$

* 50% is near optimal for a one parameter model and the rate decreases to about 25% as we increase the number of parameters in the model

* There are methods to automatically adjust the proposal density function in Metropolis-Hastins

* All these have been mechanized in software like JAGS and STAN

* The phase of generating the Markov chain where the proposal density function is optimized is called the "adaptive" state (as discussed above)

As models become more complex, adaptive MCMC may not be good enough to eliminate the autocorrelation

* Theory on Markov chain convergence will still hold but there is no guarantee on how fast it will converge

* If there is significance autocorrelation after the best scaling effort, the next best practice is to increase $t_3$ until there are sufficient number of ups and downs in the trace plot and then take a sample of the $(t_1 + t_2 +1)$^th^ to $(t_1 + t_2 + t_3)$^th^ iterations

    * This process is known as "thinning"

### Usecase Example for Actuaries

Look at how the posterior distribution of $\mu$ might be of interest

Consider a fitted lognormal distribution for a set of claims to determine the cost of an XS layer

Given $\mu$ and $\sigma$ of the lognormal we can determine the cost of an XS layer (see Klugman, Panjer, and Willmot and the actuar package)

As the posterior distribution of $\mu$ reflects the parameter risk in the model, it is also possible to reflect the parameter risk in the expected cost of a layer by calculating the expected cost of the layer for each $\mu$ in the simulated posterior distribution

It is possible to simulate an actual outcome of a loss $X$ in a layer given $\mu$ in the posterior distribution

The distribution $X$ calculated this way reflects both the parameter risk and process risk in the model

### Bayesian interence Using Gibbs Sampling (BUGS)

WinBUGS, JAGS are examples of software using Gibbs sampling

Sample work process:

1. Read data into R and call JAGS script with R package "runjags"

2. Fetch the sample of the posterior back into R to calculate statistics of interest

JAGS perform the Metropolis-Hasting algorithm we described above

It also has a number of convergence diagnostics

## Past Exam Questions

n/a

### Question Highlights

n/a