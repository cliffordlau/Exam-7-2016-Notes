# ERA 3.2 Modeling Parameter Uncertainty - Venter, Gluck

Parameter risk is more important as the process risk is smaller

Different ways to model the severity trend

* Model the trend indepedently or adjust the general inflation then model the superimposed inflation
* Model with AR(1)

Estimation risk in selecting the parameters and the downstream impact

* Use MLE working with negative log likelihood
* Slope of the negative log likelihood determines our confidence in the selection

Use HQIC to adjust the loglikelihood with a penalty for # of parameters

* Also use a pool of distribution of reasonable parameters

Form of the mdoel we use to do the modeling e.g. expected parameters vs pool of distribution of parameters

Should focuse on the spread of potential outcomes instead of best estimate

* Add flexibility and let the parameters reflect the uncertainty in the assumptions

## Introduction {#era-3.2-intro}

Parameter risk is key for large companies since process risk is reduced with a large book

Model projection risk, measure estimation risk in parameter selection and discuss model risk

## Impact of Parameter Risk

CoV for total losses:  
$CV^2(S) = \underbrace{CV^2(N)}_{\text{# of Claims}} + \underbrace{\dfrac{CV^2(X)}{\mu_N}}_{\text{Severity}}$

* For large company, $\mu_N$ is large so the 2^nd^ term is small

However if we add systemic factor that impacts all claims (e.g. inflation), it doesn't diversify away as the # of expected claims $\uparrow$

For small company there is a large amount of process risk so the overall variability does not increase very much

## Projection Risk {.tabset}

Different ways to project trends

### Simple Trend

Forecast one trend historically and into the future

Additional uncertainty due to estimate of ultimate losses is uncertain

### Severity Trend and Inflation

Claim severity $\neq$ general inflation

[Claim Severity Trend] $\approx$ [General Inflation] + [Superimposed Inflation]

2 approaches:

1) Model the severity trend independently
2) Adjust the data for general inflation and model the residual superimposed inflation

For ERM where general inflation is modeled, severity trend should be dependent on the general inflation

### Trend as a Time Series

More realistic, allow for trends to change over time

AR(1) with mean reverting form:  
$\begin{array}{ccccc}
  r_{t+1} &= &m + \alpha_1(r_t - m) &+ &\epsilon_{t+1} \\
  &= &(m - \alpha_1  m) + \alpha_1 r_t &+ &\epsilon_{t+1} \\
  &= &\alpha_0 + \alpha_1 r_t &+ &\epsilon_{t+1} \\
\end{array}$

* $m$ is the long term mean estimated from data

* $\epsilon_t \sim N(0,\sigma)$

Simple trend understate the projection risk especially for long tail LoBs

* Simple trend had a 10 year forecast 99th percentile of 45% above the mean

## Estimation Risk

Estimation risk is due to the uncertainty in selecting parameters (freq, sev, trend, variability) and specifically the downstream impact of this

**Maximum Likelihood Estimate**

For large data sets it has the lowest estimation error of unbiased estimators

Uncertainty depends on the slop of the likelihood surface; Steeper the slope near the MLE, the more confident we are in the estimate

* 2^nd^ derivative of the likelihood w.r.t. each parameters

* Negative of the 2^nd^ derivative of the log likelihood = information matrix

    * $I = \dfrac{\partial [\overbrace{ -LL }^{\text{Neg Log Likelihoo}}]}{\partial \vec{\underbrace{\alpha}_{\text{Parameters}}}}$

* Inverse of the information matrix = co-variance matrix

    * $\Sigma = I^{ -1 }$
    
* If slope of $-LL$ is steep near the selection $\Rightarrow$ More confidence in the selection

***

Model parameter uncertainty by:

* Assume join lognormal distribution for the parameters and use the correlation from $\Sigma$ from the MLE

* A new set of parameters are selection for each iterations in the simulation

***

Joint LogNormal distribution is selected because:

* Eliminates negative losses
* Parameters estimates themselves are heavy tailed for heavy tailed distribution like Pareto
* e.g. $\alpha$ in simple Pareto $F(x) 1 - \left( \frac{\theta}{x} \right)^{\alpha}$ follows inverse gamma, which is similar to LogNormal
* Results from simulations on small data sets showed that joint lognormal is reasonable

## Model Risk

When considering different distributions, need to adjust the log likelihood with penalty for the number of parameters

Hannan-Quinn Information Criterion (HQIC) is recommended because it is a compromise of other information criteria which add large or smaller penalties for # of parameter

Create a pool of dist^n^ that are consider reasonable:

1) Randomly select a mean for the parameters and a $\Sigma$ from the pool of dist^n^
2) Randomly draw the parameters based on the selected dist^n^
3) For each claim that is simulated, draw from the distribution with parameters from (2)

## Projection Models

The form of the model is very important in measuring variability

* e.g. simple trend vs time series; expected parameters vs pool of dist^n^ for those parameters

Use common sense to ensure model is structurally consistent with the underlying process

Risk modeling we are more focused on the spread of potential outcomes instead of the best estimate

* Selecting models using parsimony will lead to unrealistically stable results

Add flexibility and let the parameters reflect the uncertainty in the assumptions

## Conclusion

Parameter risk is a key source of variability, especially for large companies

Projection risk, estimation risk, model risk can all be quantified and applied i a simulation model



