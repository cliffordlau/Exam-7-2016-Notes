[
["index.html", "CAS Exam 7 Study Notes Overview", " CAS Exam 7 Study Notes Cliff Lau 2017-02-26 Overview CAS Exam notes for the 2017 Spring sitting. Notes are broken down in to 3 main sections: I. Estimation of Policy Liabilities (14 chapters) Learning Objective 1: 10-14% Brosius Loss Development Using Credibility Mack (2000) Credible Claims Reserve: The Benktander Method Hürlimann Credible Claims Reserve: Benktander, Neuhaus and Mack Learning Objectives 2 &amp; 3: 16-18% Mack (1994) Measuring the Variability of Chain Ladder Reserve Estimate Venter Factors Clark Learning Objective 4: 5-7% Siewert Sahasrabuddhe Learning Objectives 5-10: 22-24% Shapland (new 2017) Verall Meyers (new 2016) Marshall et al Learning Objectives 11-13: 6-9% Patrik Reinsurance Loss Reserving Learning Objective 14: 4-5% Teng &amp; Perkins Insurance Company Valuation (1 chapter) Learning Objective 1-3: 8-12% Goldfarb Enterprise Risk Management (11 chapter) Learning Objectives 1-6: 13-17% ERA 1 ERA 2.1 ERA 2.2 ERA 2.3 ERA 2.4 ERA 2.5 ERA 3.1 ERA 3.2 ERA 3.3 Learning Objectives 7 &amp; 8: 4-6% ERA 4.1 &amp; 4.2 ERA 5.4 "],
["loss-development-using-credibility-e-brosius.html", "Chapter 1 Loss Development Using Credibility - E. Brosius", " Chapter 1 Loss Development Using Credibility - E. Brosius Least Square Least Square formula (1.1) Know how to do them on the calculator Remember to adjust for exposure if needed Theoretical disn formulas: Poisson - Binomial (1.2) and Negative Binomial - Binomial (1.3) Know the conclusions on LS Know the caveat on least square Method comparisons Formula for each given methods (Table 1.1) Pros and cons of each method The different \\(a\\) and \\(b\\) for each methods Bayesian Credibility Best linear estimator formula (1.4) Know which method is best given the Cov and Var relationships (Table 1.2) Development Formula: Can use the credibility formula to get the LS results Case load effect Additional considerations for calculations "],
["method-assumptions.html", "1.1 Method Assumptions", " 1.1 Method Assumptions Table 1.1: Formula for a given method Method Formula Restrictions Least Squares \\(y = a + bx\\) No Restriction Chainladder \\(y = bx\\) a = 0 BF \\(y = a + x\\) b = 1 ELR \\(y = a\\) b = 0 Proposition 1.1 (Least Squares Formula) \\(b = \\dfrac{\\overline{xy} - \\bar{x}\\bar{y}}{\\overline{x^2}-\\bar{x}^2} = \\dfrac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)}\\) \\(a = \\bar{y} - b\\bar{x}\\) Notes: In the example here we are looking at the ATA development This is effectively a credibility weighting system (giving more or less weight to the observed x as appropriate) Above calculation can be done with the table features on TI-30XS 1.1.1 Caveat with Traditional Method Chainladder: Difficult to select LDF when they vary greatly from year to year BF: Doesn’t work well with negative development ELR: Ignores actual experience Difficulty with parameter estimation when loss patterns are changing When the nature of loss experience is changing the use of unadjusted data can lead to errors Variability within a stable book still have sampling error \\(\\Rightarrow\\) b that doesn’t reflect the true underlying characteristics 1.1.2 Least Squares Pros &amp; Cons Pros Good when distn is the same across multiple years As we are assuming a common \\(Y\\) and \\(X\\) over the years Good when there is little data and fluctuations in the year to year losses Good when the randomness of the data is primarily driven by process variance Cons Bad when systemic shift year to year e.g. inflation, legal environment (Tort reform) \\(\\therefore\\) best to adjust for inflation and putting all the years on a constant dollar basis before using the LS method Also should adjust for exposure As the expected value increase \\(\\propto\\) exposure and the covariances increase \\(\\propto\\) squared exposure 1.1.3 Practical Considerations for the Least Square Method Normalize the losses by dividing with premium since LS assume constant distribution and also adjust for inflation Calculates ATU, so needs the tail factor first and also start from the oldest period Recursively go backwards using the previous estimates \\(Z = \\dfrac{b}{c}\\); \\(c\\) is the ATU Notes on parameters: \\(a\\) = Projected ultimate if no losses are reported AY matures \\(\\Rightarrow a \\: \\downarrow\\); \\(Z \\uparrow\\); \\(c \\downarrow\\) If a &lt; 0 \\(\\Rightarrow\\) Use Chainladder to make a = 0 If b &lt; 0 \\(\\Rightarrow\\) Use ELR If b = 1 \\(\\Rightarrow\\) Same as BF "],
["best-estimate-based-on-bayes-with-theoretical-distribution.html", "1.2 Best Estimate based on Bayes (with Theoretical Distribution)", " 1.2 Best Estimate based on Bayes (with Theoretical Distribution) Proposition 1.2 (Poisson - Binomial) Assume the loss reporting follows a theoretical distribution \\(Y =\\) (Ultimate) # of claims incurred each year \\(\\sim Poi(\\mu)\\) \\(X =\\) # of claims reported by year end \\(\\sim Bin(y,d)\\) i.e. each claim have probability \\(d\\) of being reported in the first year Ultimate claims = \\(Q(x) = x + \\mu(1-d)\\) Unreported claims = \\(R(x) = \\mu(1-d)\\) (Expected # of claims) \\(\\times\\) (Expected % Unreported) Similar to the BF Method Proof. \\(Q(x)\\) is the estimator of \\(Y\\). It’s the sum of all possible \\(y\\)’s \\(\\times\\) probability of the result being \\(y\\) given \\(x\\) \\(\\begin{align} Q(x) &amp;= \\sum \\limits_{y = x}^{\\infty} y \\Pr(Y = y \\mid X = x) \\\\ Q(x) &amp;= \\sum \\limits_{y = x}^{\\infty} y \\dfrac{\\Pr(Y = y)\\Pr(X = x \\mid Y = y)}{\\sum_i \\Pr(Y = i) \\Pr(X = x \\mid Y = i)} \\\\ \\end{align}\\) And we get Ultimate claims = \\(Q(x) = x + \\mu(1-d)\\) Proposition 1.3 (Negative Binomial - Binomial) Assume the loss reporting follows a theoretical distribution \\(Y =\\) (Ultimate) # of claims incurred each year \\(\\sim NB(r, p)\\) \\(Y =\\) # of failures until \\(r\\) success with success probability = \\(p\\) \\(\\mathrm{E[Y]} = \\dfrac{r(1-p)}{p}\\) \\(X =\\) # of claims reported by year end \\(\\sim Bin(y,d)\\) Unreported claims = \\(R(x) = \\dfrac{s}{1-s}(x + r)\\) \\(s = (1-d)(1-p)\\) 1.2.1 Comparing Loss Development Methods Simulate loss based on one of the theoretical distribution Apply the various loss development method and calculate their respective parameters (\\(y = a + b x\\)) Compare the estimated parameters with the true parameters based on the underlying theoretical distribution Also compare the MSE from different methods "],
["bay-cred.html", "1.3 Bayesian Credibility", " 1.3 Bayesian Credibility We can’t use the Bayes theorem as in the previous section if we don’t know the underlying distribution Proposition 1.4 Estimate the ultimate losses using the best linear estimator of Y|X, \\(L(x)\\) \\(L(x) = (x - \\mathrm{E[X]})\\dfrac{Cov(X,Y)}{Var(X)} + \\mathrm{E[Y]}\\) Y = Ultimate Losses; X = Reported Losses Use this when we don’t know the distribution of the random variable \\(L(x) = Q(x)\\) when \\(Q(x)\\) is linear Remark. This is like the Bühlmann method, where \\(L\\) is a linear function that minimizes \\(\\mathrm{E}_X\\left[\\left(Q(X) - L(X)\\right)^2\\right]\\) If \\(L(x) = a + b x\\) then we minimize \\(\\mathrm{E}_X\\left[\\left(Q(X) - a - bX \\right)^2\\right]\\) Table 1.2: Intuitive interpretation of \\(L(x)\\) Scenarios Implications Interpretation \\(x = \\mathrm{E[X]}\\) \\(L(X) = \\mathrm{E[Y]}\\) Losses are coming in as expected, estimate of ultimate losses is unchanged \\(\\mathrm{Cov(X,Y)} \\approx 0\\) \\(L(X) \\cong \\mathrm{E[Y]}\\) \\(X\\) and \\(Y\\) are only loosely related \\(\\Rightarrow\\) Use ELR Method \\(\\mathrm{Cov(X,Y)} \\ll \\mathrm{Var(X)}\\) \\(L(X) \\cong \\mathrm{E[Y]}\\) \\(X\\) and \\(Y\\) don’t vary together \\(\\mathrm{Cov(X,Y)} \\approx \\mathrm{Var(X)}\\) \\(L(X) \\approx x + \\left[\\mathrm{E}[Y] - \\mathrm{E}[X] \\right]\\) Same as BF Method \\(\\mathrm{Cov(X,Y)} \\gg \\mathrm{Var(X)}\\) \\(X\\) and \\(Y\\) move together, \\(Y\\) is significantly influenced by \\(X\\) Use Dev Method 1.3.1 Practical Application (LS Development) Proposition 1.5 (Development Formula 1) Estimate \\(\\mathrm{E}[X]\\), \\(\\mathrm{Var}(X)\\), and \\(\\mathrm{Cov}(X,Y)\\) from data (i.e. a series of past years) assuming a common \\(Y\\) and \\(X\\) This \\(L(x)\\) here is the same as the least-square estimate as in 1.1 Proof. Start with\\(y = a + bx\\) and plug in \\(a\\) and \\(b\\) from proposition 1.1 We get: \\(\\begin{align} y &amp;= (\\bar{y} - b\\bar{x}) + bx \\\\ &amp;= \\bar{y} + b \\left(x - \\bar{x}\\right) \\\\ &amp;= \\left(x - \\bar{x}\\right) \\dfrac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)} + \\bar{y} \\\\ \\end{align}\\) Which equals \\(L(x)\\) \\(\\therefore\\) the least-square estimate is the best linear estimate of \\(Q(x)\\) Remark. If not for sampling error, the least square method will give us the best linear approximation to the Bayesian estimate, regardless of the distributions of \\(X\\) or \\(Y\\) 1.3.2 Credibility Form of the Dev’ Formula Alternative way to express \\(L(x)\\), following Bühlmann credibility, we express \\(L(x)\\) in terms of: Expected Value of the Process Variance (\\(EVPV\\)) \\(\\mathrm{E}_Y\\left[\\mathrm{Var}(X \\mid Y )\\right]\\) Variance of the Hypothetical Mean (\\(VHM\\)) \\(\\mathrm{Var}_Y(\\mathrm{E}\\left[X \\mid Y \\right])\\) We can read \\(VHM\\) as distrust in underwriters and EVPV distrust in the claims department Use the method below when the least-square assumption fails (i.e. Year to year changes in loss and loss distributions are small, or can be corrected for) Formula below requires additional hypothesis (in paper appendix?) Proposition 1.6 (Development Formula 2) Suppose that there is a real number \\(d \\neq 0\\) such that \\(\\mathrm{E}\\left[X \\mid Y = y \\right] = dy\\) for all \\(y\\) \\(L(x) = Z \\underbrace{\\dfrac{x}{d}}_{\\begin{array}{c} \\text{Dev&#39;}\\\\ \\text{Method}\\\\ \\end{array}} + (1-Z)\\underbrace{\\mathrm{E[Y]}}_{ELR}\\) Formula is the credibility weighting of the chainladder estimate and ELR estimate If \\(EVPV = 0\\) \\(\\Rightarrow\\) Full weight to the chainladder If \\(VHM = 0\\) \\(\\Rightarrow\\) Full weight to the \\(\\mathrm{E}[Y]\\) Proof. Start with the proposition 1.4 and we set \\(\\dfrac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)} = \\dfrac{1}{d} \\dfrac{VHM}{VHM + EVPV}\\) \\(\\begin{align} L(x) &amp;= (x - \\mathrm{E[X]})\\dfrac{Cov(X,Y)}{Var(X)} + \\mathrm{E[Y]} \\\\ &amp;= (x - \\mathrm{E[X]}) \\left \\{ \\dfrac{1}{d} \\dfrac{VHM}{VHM + EVPV} \\right \\} + \\mathrm{E[Y]} \\\\ &amp;= (x - \\mathrm{E[X]}) \\left \\{ \\dfrac{1}{d} Z \\right \\} + \\mathrm{E[Y]} \\\\ &amp;= Z \\dfrac{x}{d} - Z \\dfrac{\\mathrm{E}[X]}{d} + \\mathrm{E}[Y] \\\\ &amp;= Z \\dfrac{x}{d} - Z \\mathrm{E}[Y] + \\mathrm{E}[Y] \\\\ &amp;= Z \\dfrac{x}{d} + (1 - Z) \\mathrm{E}[Y] \\\\ \\end{align}\\) Which is what we have above in proposition 1.6 Proposition 1.7 (Method for Z) Calculate Z to use with formula in proposition 1.6 \\(Z = \\dfrac{VHM}{VHM + EVPV} = \\dfrac{\\mathrm{Var_Y(E[X|Y])}}{\\mathrm{Var_Y(E[X|Y])}+\\mathrm{E_Y[Var(X|Y)]}}\\) \\(VHM = d^2 \\sigma^2_Y\\) \\(EVPV = \\sigma^2_d[\\sigma^2_Y + \\mathrm{E[Y]}^2]\\) \\(d =\\) % reported Remark. We use the above \\(Z\\) when underlying distribution not stable (historical not a good predictor) LS only works when the underlying distn are stable We assumes the following \\(d \\: {\\perp\\!\\!\\!\\!\\perp} \\: X\\): reporting speed does not vary with the volume of claims \\(D = \\dfrac{X}{Y}\\) Here we typically assume the \\(\\sigma_{\\frac{X}{Y}}\\) does not depend on \\(Y\\) Results sensitive to \\(\\mathrm{E[Y]}\\) and \\(\\mathrm{E[D]}\\) but not the \\(\\sigma\\) Remark. Alternatively, \\(Z = \\dfrac{b}{c}\\), where \\(c\\) is the CDF and \\(b\\) is from the LS This yield the LS results (Where we assume there’s no change in the underlying year to year data) Assume same \\(d\\) for any size of \\(y\\); Not necessarily true for large or small \\(y\\) Proof. Start with \\(\\dfrac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)} = \\dfrac{1}{d} \\dfrac{VHM}{VHM + EVPV}\\) Based on 1.7 we have \\(Z = \\dfrac{VHM}{VHM + EVPV}\\) And \\(b\\) from 1.1 \\(\\dfrac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)}\\) Then we have \\(b = \\dfrac{1}{d} Z\\) Next, \\(\\dfrac{1}{d} = \\dfrac{1}{\\text{% Reported}} = \\text{Reported CDF} = c\\) Finally we have \\(Z = \\dfrac{b}{c}\\) Proposition 1.8 (Poisson-Binomial Special Case) \\(L(x) = x + (1-d)\\mathrm{E(Y)}\\) \\(Z = d\\) Same as BF BF is optimal when claim counts follow Poi - Bin Proof. Start with \\(EVPV\\) and \\(VHM\\) under the Poisson-Binomial case as discussed in proposition 1.2 \\(EVPV = \\mathrm{E}[yd(1-d)] = \\mu d(1-d)\\) \\(VHM = \\mathrm{Var}(yd) = \\mu d^2\\) Then \\(Z = \\dfrac{\\mu d^2}{\\mu d^2 + \\mu d(1-d)} = d\\) And we get what we have from proposition 1.8 \\(L(x) = x + (1-d)\\mathrm{E(Y)}\\) Remark. Note that \\(L(x) = Q(x)\\) (as in proposition 1.2) since \\(Q(x)\\) is linear, so the best linear estimate = the bayesian estimate Proposition 1.9 (Negative Bin-Binomial Special Case) \\(L(x) = \\dfrac{x}{d + p(1-d)} + \\dfrac{\\mu p (1-d)}{d + p(1-d)}\\) \\(Z = \\dfrac{d}{d + p(1-d)}\\) Remark. \\(VHM\\) is larger here than in the Poi-Binomial case while the \\(EVPV\\) is the same \\(\\therefore\\) \\(Z\\) is larger \\(\\Rightarrow\\) Chainladder method gets more weight Also note that since \\(Q(x)\\) is still linear, \\(L(x) = Q(x)\\) 1.3.3 Caseload Effect In proposition 1.6 we assumed the expected number of claims reported is \\(\\propto\\) number of claims incurred Not necessarily true e.g. claim is more likely to be reported in a timely fashion when the caseload (case reserve) is low, and we expect the development ratio \\(\\dfrac{\\mathrm{E}[X \\mid Y = y]}{y}\\) to be not a constant decreasing function of \\(y\\) When \\(D\\) and \\(Y\\) not independent the credibility-based development formula still works (i.e. constant development ratio is not essential for a credibility-based development formula) e.g. \\(d\\) larger for small \\(y\\) since small claims are reported more timely, so settle faster e.g. the opposite situation when large \\(y\\) has as larger \\(d\\): For a property book when large weather event happens, report quicker Proposition 1.10 (Development Formula 3) Supposed there are real numbers \\(d \\neq 0\\) and \\(x_0\\) such that \\(\\mathrm{E}[X \\mid Y= y] = dy + x_0\\) for all \\(y\\) \\(L(x) = Z \\dfrac{x - x_0}{d} + (1-Z)\\mathrm{E[Y]}\\) \\(Z = \\dfrac{VHM}{VHM + EVPV}\\) Remark. Assumptions: \\(\\mathrm{E[X|Y=y]} = dy + x_0\\) \\(x_0\\) is for the fixed salary \\(d \\neq 0\\) Development ratio \\(= d + \\dfrac{x_0}{y}\\) Which does decrease as \\(y\\) gets larger This gives \\(\\mathrm{E}[X \\mid Y = 0] = x_0 &gt; 0\\) Which is okay consider claims department in real life Impossible to determine \\(x_0\\) and \\(d\\) in practice but this shows that the least square methods still make sense when development ratio varies with caseload "],
["conclusions.html", "1.4 Conclusions", " 1.4 Conclusions LS is the best linear estimate of \\(Q(x)\\), the theoretical best estimate of the ultimate loss LS produce more reasonable results when year to year fluctuations are severe LS does not do well when the variations are due to underlying changes in the payment pattern (due to internal or external changes) e.g. When there is a systematic shift in the business Subject to sampling error when estimating parameters similar to other methods "],
["past-exam-questions.html", "1.5 Past Exam Questions", " 1.5 Past Exam Questions Haven’t done TIA practice questions Concepts 1996 - #28: Poi-bin process 1996 - #50: When to use LS 2000 - #2: Method assumptions 2001 - #22: Which method is better given the Cov(X,Y) and Var(X) relationships 2003 - #3: \\(a\\) and \\(b\\) for different methods \\(\\star\\) 2006 - #4: Cov(X,Y) and Var(X) relationships (phrased a bit differently) 2006 - #15 d: LS assumptions 2007 - #42: Method assumptions 2009 - #3 b: LS assumptions (LS not best given \\(b &lt; 0\\)) 2014 #2: Know assumptions of methods for Dev (fitted line and residuals) Full Calculations \\(\\star\\) 2000 - #41(fig 1.1): LS, dev and ELR from triangle; Weight the 2 with \\(Z = \\dfrac{b}{c}\\) to get to LS \\(\\star\\) 2001 - #31(fig 1.2): Use VHM and EVPV for 2002 - #21: LS, dev and ELR from triangle and weight for LS 2008 - #9: LS, dev and ELR from triangle \\(\\star\\) 2008 - #10a: VHM and EVPV calc Double check the \\(\\sigma_y\\) calc make sense \\(\\star\\) 2011 #1: a) LS w/ exposure adjustment b) Assess LS parameters resonability c) Draw the relationship of losses to date vs ultimate for the 3 methods 2012 ‐ #1 b: Solving equations to back out numbers using BK and BF method 2012 - #4: LS and know that LS gives the optimal linear weight based on \\(\\dfrac{b}{c}\\) 2014 #1: a) VHM and EVPV b) Why LS is not appropriate Simple Plug and Play 1998 - #19: BF with LS \\(\\star\\) 2003 - #22: Dev and ELR, key is to adjust for exposure 2005 - #12: LS calc by \\(a\\) and \\(b\\) 2006 - #5: LS calc by \\(a\\) and \\(b\\) 2006 - #15 a-c: Dev, BF, and cred 2009 - #3 a: LS Calc 2012 ‐ #1 a: Dev, BF, Benktander 1.5.1 Question Highlights Figure 1.1: 2000 Question 41 Figure 1.1: 2000 Question 41 Figure 1.2: 2001 Question 30 Figure 1.2: 2001 Question 30 -->"],
["credible-claims-reserve-the-benktander-method-t-mack.html", "Chapter 2 Credible Claims Reserve: The Benktander Method - T. Mack", " Chapter 2 Credible Claims Reserve: The Benktander Method - T. Mack Know the GB method formula (2.1)and properties Recognize actual emergence faster "],
["gunnar-benktander-method-gb.html", "2.1 Gunnar Benktander Method (GB)", " 2.1 Gunnar Benktander Method (GB) Proposition 2.1 (Benktander Reserve) Reserve under the Benktaner Method \\(R_{GB} = q_kU_{BF}\\) \\(q_k = (1 - p_k)\\) = % unpaid loss @ \\(k\\) \\(U_{BF} = C_k + q_kU_0\\) Weight on \\(U_0\\) is \\(q_k^2\\) Remark. Benktander recognizes actual emergence faster \\(\\Rightarrow\\) Less weight on the a-priori BF reduces the use of actual loss to the extent of the complement credibility \\(\\begin{align} U_{GB} &amp;= (1-q_k)U_{CL} + q_kU_{BF} &amp; \\cdots (1)\\\\ &amp;= (1-q_k^2)U_{CL} + q_k^2 U_0 &amp; \\cdots (2)\\\\ \\end{align}\\) Crediblity weight \\(U_{BF}\\) and \\(U_{CL}\\) Crediblity weight \\(U_{CL}\\) and a priori 2.1.1 Method Comparison Table 2.1: Comparison of Ultimate Loss and Reserve for Different Methods Method Ultimate (\\(U\\)) Reserve (\\(R\\)) Chain Ladder \\(\\dfrac{C_k}{p_k}\\) \\(q_k U_{CL} = q_k \\dfrac{C_k}{p_k}\\) BF Method \\(C_k + q_kU_0\\) \\(q_k U_0\\) GB Method \\(C_k + q_kU_{BF}\\) \\(q_k U_{BF}\\) Theorem 2.1 For an arbitrary starting point \\(U^{(0)} = U_0\\) and the iteration rule \\(R^{(m)} = q_k U^{(m)}\\) and \\(U^{(m+1)} = C_k + R^{(m)}\\), \\(m = 0, 1, 2, ...\\) gives credibility mixtures \\[U^{(m)} = (1-q^m_k)U_{CL} + q^m_k U_0\\] \\[R^{(m)} = (1-q^m_k)R_{CL} + q^m_k R_{BF}\\] between \\(BF\\) and \\(CL\\) which start at \\(BF\\) and lead via \\(GB\\) to \\(CL\\) for \\(m= \\infty\\) 2.1.2 MSE MSE of Benktander is almost as small as the MSE of the optimal credibility in most cases \\(MSE(R_{GB}) &lt; MSE(R_{BF})\\) When \\(p_k \\in [0, 2c^*]\\); \\(c^*\\) is the optimal credibility \\(R_{GB}\\) doesn’t have the lowet \\(MSE\\) only when \\(p_k &gt; 2c^*\\) Doesn’t hold if \\(c^*\\) is small and \\(p_k\\) is large Remark. \\(MSE(R_c) = \\mathrm{E}[R_c - R]^2\\) \\(R_c = c R_{CL} + (1-c)R_{BF}\\) \\(R = U - C_k = C_n - C_k\\) Where: \\(c = 0\\) for \\(BF\\) \\(c = p_k\\) for \\(GB\\) \\(c = c^*\\) for optimal credibility where \\(c \\in [0, 1]\\) "],
["notation.html", "2.2 Notation", " 2.2 Notation \\(U =\\) Ultimate Loss \\(R =\\) Estimate of Unpaid losses \\(U_0 =\\) a priori estimate of Ultimate Losses \\(p_k =\\) % of total losses paid at \\(k\\) \\(q_k = 1 - p_k =\\) % of total losses unpaid a \\(k\\) \\(C_k =\\) Actual paid losses at \\(k\\) "],
["past-exam-questions-1.html", "2.3 Past Exam Questions", " 2.3 Past Exam Questions Haven’t done TIA practice questions Concepts 2006 - #16 b c: GB method vs BF and dev Lower MSE and gives weight to both a-priori and emerged 2007 - #46 b-c: BF weight, BF drawback, GB advantages over BF 2008 - #10 b-c: Comparison of methods and GB as credibility weight 2013 - #4 b: GB approaches CL as more iterations are done Simple Plug and play 2004 - #31: GB method ultimate 2005 - #16: GB method ultimate 2006 - #16 a: GB method ultimate 2012 - #1 a: GB BF Dev \\(\\star\\) 2014 - # 5: Credibility weight methods from the Clark paper on LDF curve fitting Weight to the Dev method is \\((1-q_k)\\) Other 2012 - #1 b (fig 2.1): Minor arithmetic 2013 - #4 a: Back out LDFs with BF and GB methods 2.3.1 Question Highlights Figure 2.1: 2012 Question 1 Figure 2.1: 2012 Question 1 -->"],
["credible-claims-reserve-benktander-neuhaus-and-mack-w-hurlimann.html", "Chapter 3 Credible Claims Reserve: Benktander, Neuhaus and Mack - W. Hurlimann", " Chapter 3 Credible Claims Reserve: Benktander, Neuhaus and Mack - W. Hurlimann Everything is based on a special method to calculate the \\(ELR\\) and \\(LDFs\\) \\(ELR\\) is based on the whole triangle \\(\\sum\\)-ing up column LRs \\(LDFs\\) are based \\(\\dfrac{\\text{Col LRs}}{ELR}\\) Key is just to watch out for incremental vs cumulative Know the \\(Z\\) for different methods (Table 3.1) The \\(Z\\) is for weighting the reserve Optimal credibility formula assumes \\(U_i^{BC} {\\perp\\!\\!\\!\\!\\perp} C_i\\) and \\(R_i\\) "],
["loss-ratio-claims-reserve.html", "3.1 Loss Ratio Claims Reserve", " 3.1 Loss Ratio Claims Reserve \\(m_k\\): Expected loss ratio @ each age \\(k\\) Based on incremental column paid loss ratios \\(k \\in \\{1, ..., n \\}\\) For \\(n\\) development periods \\(ELR\\): Expected loss ratio: \\(ELR = \\sum \\limits_{k=1}^n m_k\\) a priori ELR for collective loss ratio approach Use for the entire triangle \\(p_k\\): % Losses emerged for exposure period \\(k\\) \\(p_k = \\dfrac{\\sum \\limits_{j=1}^{n} m_j}{ELR}\\) Based on column loss ratios \\(m_k\\) Loss ratio payout factor or loss ratio lag-factor \\(q_k = 1 - p_k\\) is the loss ratio reserve factor 3.1.1 Loss Ratio Claims Reserve Summary \\[R_i^c = Z_i \\times R_i^{ind} + (1-Z_i) \\times R_i^{coll}\\] Table 3.1: Comparison of \\(Z_i\\) for Different Methods \\(\\mathbf{Z_i}\\) Method 1 Chainladder; Individual LR 0 BF; Collective LR \\(p_k\\) Benktander (GB) \\(p_k \\times ELR\\) Neuhaus (WN) \\(\\dfrac{p_k}{p_k + \\sqrt{p_k}}\\) Optimal Credibility Remark. Neuhaus gives low credibility to lines with low loss ratios Since Neuhaus use loss ratio, \\(\\Delta\\) exposure base will \\(\\Delta\\) result Neuhaus credibility = expected loss ratio to date Optimal credibility is capped @ 0.5 Benktander and Neuhaus reduce the MSE of the reserve estimate nearly to an optimal level outperforming individual and collective Proposition 3.1 (Individual Loss Ratio Claims Reserve) Analogous to chainladder \\(\\begin{align} R_i^{ind} &amp;= \\dfrac{C_{ik}}{p_k} \\times q_k \\\\ &amp;= \\dfrac{C_{ik}}{p_k} - C_{ik} \\\\ &amp;= U_i^{ind} - C_{ik} \\\\ \\end{align}\\) Proposition 3.2 (Collective Loss Ratio Claims Reserve) Analogous to BF \\(\\begin{align} R_i^{Coll} &amp;= q_k(V_i \\times ELR) \\\\ &amp;= q_k(U_i^{BC}) \\\\ \\end{align}\\) BC = Burning Cost 3.1.2 Optimal Credibility Weights Optimal credibility weights for loss ratio claims reserve \\(Z^*_i\\) is the credibility that minimizes the \\(MSE(R_i^c) = \\mathrm{E}[(R_i^c - R_i)^2]\\) Theorem 3.1 Optimal credibility factor \\(c^*\\) that minimizes \\(MSE(R_i^c) = \\mathrm{E}[(R_i^c - R_i)^2]\\) is \\(Z^*_i = \\dfrac{p_i}{q_i} \\dfrac{Cov(C_i, R_i) + p_i q_i Var(U_i^{BC})}{Var(C_i) + p_i^2 Var(U_i^{BC})}\\) Table 3.2: Impact of different components on \\(Z_i^*\\) Impact on \\(\\mathbf{Z_i^*}\\) Comments Losses emerge Increase Since \\(\\dfrac{p_i}{q_i}\\) increases as losses emerge \\(\\mathrm{Cov}(C_i, R_i)\\) increase Increase Large covariance implies that \\(C_i\\) is predictive of \\(R_i\\) \\(\\Rightarrow\\) More weight on \\(CL\\) method \\(\\mathrm{Var}(C_i)\\) increase Decrease If \\(C_i\\) is volatile, we want to rely less on \\(CL\\) method \\(\\mathrm{Var}(U_i^{BC})\\) increases Increase Trust \\(CL\\) method more when a-priori is volatile Remark. Assumes \\(U_i^{BC} {\\perp\\!\\!\\!\\!\\perp} C_i\\) and \\(R_i\\) Large \\(Var(U_i^{BC})\\) \\(\\Rightarrow\\) \\(Z \\approx \\dfrac{p}{q} \\times \\dfrac{pq}{p^2} = 1\\) Assumes \\(\\mathrm{E}\\left[ \\dfrac{C_{ik}}{U_i} \\mid U_i \\right] = p_k\\) and \\(\\mathrm{Var}\\left( \\dfrac{C_{ik}}{U_i} \\mid U_i \\right) = p_k q_k \\beta^2(U_i)\\) Theorem 3.2 Under the additional assumptions above, we have \\(Z_i^* = \\dfrac{p_k}{p_k + t_k}\\) Where \\(t = \\dfrac{\\mathrm{E}[\\alpha^2(U)]}{\\mathrm{Var}(U_0) + \\mathrm{Var}(U) - \\mathrm{E}[\\alpha^2(U)]}\\) Theorem 3.3 If we assume \\(\\mathrm{Var}(U_i) = \\mathrm{Var}(U_i^{BC})\\) then \\(Z_k^* = \\dfrac{p_k}{p_k + \\sqrt{p_k}}\\) Where the above assumption lead to \\(t_k \\sim \\sqrt{p_k}\\) "],
["remark-6-1.html", "3.2 Remark 6.1", " 3.2 Remark 6.1 Doing all the above with more “Traditional” method Chainladder Replace \\(p_k\\) with \\(p_k^{CL}\\), inverse of the CDF \\(R^{ind}_i = \\dfrac{C_i}{p_k} - C_i\\) Cape Code \\(ELR = \\dfrac{\\sum\\limits_{i,k}S_{ik}}{\\sum\\limits_i V_i \\times p_i^{CL}}\\) Sum of cumulative paid loss \\(\\div\\) used up premium \\(R^{Coll}_i = q_k \\times (V_i \\times ELR)\\) Benktander with \\(Z_i = p_k\\) \\(R^{GB}_i = Z_i \\times R^{ind}_i + (1 - Z_i) \\times R^{coll}_i\\) Optimal Cape Cod with \\(Z_i = \\dfrac{p_k}{p_k+\\sqrt{p_k}}\\) BF use some other a-priori that varies by AY and \\(Z = 0\\) Note that we are talking about applying \\(Z\\) to the formula above (Not the weighting between ultimates) "],
["notation-1.html", "3.3 Notation", " 3.3 Notation For \\(n \\times n\\) triangle and losses fully developed at \\(n\\) Where \\(i\\) is exposure period and \\(k\\) is the age \\(S_{ik} =\\) Incremental Paid \\(C_{ik} =\\) Cumulative Paid \\(U_i =\\) Ultimate loss \\(V_i =\\) Exposure base \\(m_k =\\) expected loss ratio \\(\\hat{m}_k =\\) estimate of \\(m_k\\) "],
["past-exam-questions-2.html", "3.4 Past Exam Questions", " 3.4 Past Exam Questions Haven’t done TIA practice questions Full Calculation 2013 #2 (fig 3.1): Reserve calc \\(\\star\\) 2015 #1: Neuhaus and optimal Why is this needed? \\(\\operatorname{Var}(U_i) = \\operatorname{Var}(U_i^{BC})\\) for the optimal credibility formula 3.4.1 Question Highlights Figure 3.1: 2013 Question 2 Figure 3.1: 2013 Question 2 -->"],
["measuring-the-variability-of-chain-ladder-reserve-estimate-t-mack.html", "Chapter 4 Measuring the Variability of Chain Ladder Reserve Estimate - T. Mack", " Chapter 4 Measuring the Variability of Chain Ladder Reserve Estimate - T. Mack Know the 3 assumptions of chain ladder Know the 3 different weight and variance assumptions Weight \\(w_{j,k}\\) Description Variance Residual 1 Simple Average \\(\\alpha_k^2 \\times \\mathbf{c_{j,k}^2}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{j,k}^2}}}\\) \\(c_{j,k}\\) Weighted Average \\(\\alpha_k^2 \\times \\mathbf{c_{j,k}}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{j,k}}}}\\) \\(c_{j,k}^2\\) Least Square \\(\\alpha_k^2 \\times \\mathbf{1}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{1}}}\\) Mean squared error calculation \\(\\begin{align} MSE(\\hat{c}_{i,I}) = \\hat{c}_{i,I}^2 \\Bigg \\{ \\sum_{k = I + 1 - i}^{I-1} \\frac{\\hat{\\alpha}_k^2}{\\hat{f_k}^2} \\Bigg ( \\frac{1}{\\hat{c}_{i,k}} + \\underbrace{\\frac{1}{\\sum_{j=1}^{I-k}c_{j,k}}}_{\\text{Column x latest}}\\Bigg ) \\Bigg \\} \\end{align}\\) \\(\\begin{align} \\hat{\\alpha}_k^2 = \\frac{1}{I - k - 1} \\sum_{j=1}^{I-k} c_{j,k} \\Big ( \\underbrace{\\frac{c_{j,k+1}}{c_{j,k}}-\\hat{f_k}}_{\\text{AY LDFs - Selected}} \\Big )^2 \\end{align}\\) \\(\\hat{\\alpha}_{I-1}^2 = \\operatorname{min} \\left( \\dfrac{(\\hat{\\alpha}_{I-2}^2)^2}{\\hat{\\alpha}_{I-3}^2},\\hat{\\alpha}_{I-3}^2 \\right) = \\begin{cases} \\hat{\\alpha}_{I-3}^2 &amp; \\text{if } \\hat{\\alpha}_{I-3}^2 &lt; \\hat{\\alpha}_{I-2}^2 \\\\ \\hat{\\alpha}_{I-2}^2 \\times \\dfrac{\\hat{\\alpha}_{I-2}^2}{\\hat{\\alpha}_{I-3}^2} &amp; \\text{else}\\\\ \\end{cases}\\) Confidence Interval Normal: equation (4.6) Log-normal: equation (4.7) Know the 4 test of assumptions Test 1. Intercept Test 2. Residuals Test 3. CY Test Rank small and large \\(\\operatorname{E}[z_n] = \\dfrac{n}{2} - c_n\\) \\(c_n = {n - 1 \\choose m}\\frac{n}{2^n}\\) \\(m = \\operatorname{floor}\\left[ \\dfrac{n-1}{2} \\right]\\) \\(\\operatorname{Var}(z_n) = \\dfrac{n(n-1)}{4} - c_n (n-1) + \\operatorname{E}[Z_n] - \\operatorname{E}[z_n]^2\\) \\(Z = \\sum_{diagonal} z\\) Since \\(Z \\sim\\) Normal, can sum the mean and variance assuming \\(\\perp\\!\\!\\!\\perp\\) Test 95% CI: \\(\\operatorname{E}[Z] \\pm 2 \\times \\sigma\\) Test 4. Adjacent LDF Correlation \\(S = \\sum \\limits_{\\in rows} \\Big \\{ [Rank \\: Col \\: i \\: LDF] - [Rank \\: Col \\: j \\: LDF] \\Big \\}^2\\) \\(T_k = 1 - \\dfrac{S}{n(n^2-1)/6}\\) \\(T = \\dfrac{\\sum T_k (n_k - 1)}{\\sum (n_k-1)} = \\dfrac{\\sum_k (I - k -1)T_k}{\\sum_k I - k -1}\\) \\(\\operatorname{E}[T] \\pm Z \\sqrt{\\operatorname{Var}(T)}\\) \\(\\operatorname{E}[T] = 0\\) \\(\\operatorname{Var}[T] = \\dfrac{1}{(I-2)(I-3)/2}\\) Use \\(Z = 0.67\\) for range of [25%, 75%] "],
["cl-ass.html", "4.1 Chain Ladder Assumptions", " 4.1 Chain Ladder Assumptions Definition 4.1 Notations use for Mack \\(c_{i,k} =\\) cumulative losses for AY \\(i\\) @ age \\(k\\) \\(f_k =\\) LDF from \\(k\\) to \\(k + 1\\), \\(k \\in [1:I-1]\\) \\(I =\\) size of the triangle Proposition 4.1 (Chain Ladder Assumption 1) \\[\\operatorname{E}\\left [c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ] = c_{i,k} \\times f_k\\] Remark. Expected incremental losses are \\(\\propto\\) losses to date Proportion depends on the age \\(k\\) of AY Our best estimate of ultimate depends only on the losses to date Ignores prior period losses Corollary 4.1 Restating Chain Ladder Assumption 1: \\[\\mathrm{E}\\left[ \\dfrac{c_{i,k+1}}{c_{i,k}} \\mid c_{i,1},...,c_{i,k} \\right]\\] Remark. Expected LDF is unbiased Implies that development to \\(c_{i,k+1}\\) is independent of the size of losses at \\(c_{i,k}\\) Implies that adjacent LDFs are independent Proposition 4.2 (Chain Ladder Assumption 2) \\[\\left \\{c_{i,1} \\cdots c_{i,I} \\right \\} \\: {\\perp\\!\\!\\!\\!\\perp} \\: \\left \\{c_{j,1} \\cdots c_{j,I} \\right \\} \\:\\: : \\:\\: i \\neq\\ j\\] Remark. Losses in each AYs are \\({\\perp\\!\\!\\!\\!\\perp}\\) of the losses in other AYs This assumption make our estimate unbiased Proposition 4.3 (Chain Ladder Assumption 3) \\[\\operatorname{Var}\\left (c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ) = \\alpha_k^2 \\: c_{i,k}\\] Remark. Variance of the incremental losses is \\(\\propto\\) losses reported to date Proportion depends on the age (\\(k\\)) of AY (i.e. same for each column but varies for each column) \\(\\hat{f}_k\\) is selected to minimize the variance 4.1.1 Implicit Assumptions When using CL, we are making assumptions on how we select the factor \\(\\hat{f}_k\\) and the application \\(\\hat{c}_{i,k+1} = c_{i,k} \\times \\hat{f}_k\\) Which requires the following assumptions: Unbiased estimate of each \\(f_k\\) \\(\\mathrm{E}\\left[ \\hat{f}_k \\right] = f_k\\) \\(\\hat{f}_k\\) are representative of the true \\(f_k\\) Based on assumption 1 &amp; 2 (prop. 4.1 &amp; prop. 4.2) Unbiased estimate of Ultimate \\(\\mathrm{E}\\left[ \\hat{c}_{iI} \\right] = c_{ik} \\times \\hat{f}_k \\times \\cdots \\times \\hat{f}_{I-1} = \\mathrm{E}\\left[ c_{iI} \\right]\\) Multiplying the \\(\\hat{f}_k\\)’s by the paid to date will give us an unbiased estimate of the future losses Based on assumption 1 &amp; 2 (prop. 4.1 &amp; prop. 4.2) To use volume weighted average LDF Based on assumption 3 (prop. 4.3) To calculate the confidence interval Based on assumption 3 (prop. 4.3) 4.1.2 Proof for Assumption 3 Work in progress section To estimate \\(f_k\\) we can weight the historical LDFs in many different ways, putting it in general terms: \\[\\begin{equation} \\hat{f}_k = \\sum_i \\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right) \\times w_{i,k} \\:\\:\\:\\: : \\:\\:\\:\\: \\sum_i w_{i,k} = 1 \\tag{4.1} \\end{equation}\\] Remark. We assume each of the \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) are an unbiased estimate of \\(f_k\\) From assumption 1 &amp; 2? \\(\\therefore\\) Any weighting of them is also unbiased Remark. When we use weighted volume average \\(w_{i,k} = \\dfrac{c_{i,k}}{\\sum \\limits_{j} c_{j,k}}\\) The weights that minimize the variance is inversely proportional to the variance of the item we are estimating We want weights \\(w_{i,k}\\) for each \\(i\\) on \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) The weight we apply to each \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) varies based on the variance of \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) And if \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) has high variance its weight will be lower to minimize the total variance of our estimate of \\(\\hat{f}_k = \\left( \\dfrac{c_{1,k+1}}{c_{1,k}} \\right) \\times w_{1,k} + \\cdots + \\left( \\dfrac{c_{I,k+1}}{c_{I,k}} \\right) \\times w_{I,k}\\) High variance estimate get lower weight: \\[\\dfrac{1}{w_{i,k}} \\propto \\mathrm{Var}\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\] Since \\(c_{i,k}\\) is known, we can pull it out of the variance term \\[\\dfrac{1}{w_{i,k}} \\propto \\dfrac{\\mathrm{Var}(c_{i,k+1})}{c_{i,k}^2}\\] and we get \\[\\begin{equation} w_{i,k} \\times \\mathrm{Var}(c_{i,k+1}) \\propto c_{i,k}^2 \\tag{4.2} \\end{equation}\\] Recall the weight for the volume weighted average is: \\[\\begin{align} w_{i,k} &amp;= \\dfrac{c_{i,k}}{\\sum \\limits_{j} c_{j,k}}\\\\ w_{i,k} &amp;\\propto c_{i,k}\\\\ \\end{align}\\] Applying the above to equation (4.2) we get: \\[\\begin{align} c_{i,k} \\times \\mathrm{Var}(c_{i,k+1}) &amp;\\propto c_{i,k}^2 \\\\ \\mathrm{Var}(c_{i,k+1}) &amp;\\propto c_{i,k} \\\\ \\mathrm{Var}(c_{i,k+1}) &amp;= \\alpha^2_k \\times c_{i,k} \\\\ \\end{align}\\] And we have chainladder assumption 3 (Prop. 4.3) 4.1.3 LDF Selections Assumptions Recall equation (4.1) and (4.2) Table 4.1: Relationships between weight, variance and residual Weight \\(w_{i,k}\\) Description Variance Residual (4.9) 1 Simple Average \\(\\alpha_k^2 \\times \\mathbf{c_{i,k}^2}\\) \\(\\varepsilon_{i,k} = \\dfrac{c_{i,k+1} - c_{i,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{i,k}^2}}}\\) \\(c_{i,k}\\) Weighted Average \\(\\alpha_k^2 \\times \\mathbf{c_{i,k}}\\) \\(\\varepsilon_{i,k} = \\dfrac{c_{i,k+1} - c_{i,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{i,k}}}}\\) \\(c_{i,k}^2\\) Least Square2 \\(\\alpha_k^2 \\times \\mathbf{1}\\) \\(\\varepsilon_{i,k} = \\dfrac{c_{i,k+1} - c_{i,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{1}}}\\) Note the least square here we are forcing the intercept through the origin Assumption for LS is that variance is the same for each exposure year (See Brosius) Use different method for LDF selection based on variance assumption Variance and the weight always multiply to \\(c_{j,k}^2\\) 4.1.4 Violation of Assumptions Correlation between on AYs and another Assumption 2 (Prop. 4.2) is violated e.g. Strong calendar year effects (i.e. faster payment, changing inflation) will lead to correlation along a diagonal Check using calendar year test A single \\(\\hat{f}_k\\) is not appropriate for all years \\(i\\) Assumption 1 (Prop. 4.1) is violated Dependence among columns Assumption 1 (Prop. 4.1) is violated Not necessarily assumption 2 (Prop. 4.1) If losses in the follow period are inversely correlated to the losses in the current period, then we’ll have correlation between adjacent LDFs but still maintain independence of AYs If residuals are not random around zero Assumption 3 (Prop. 4.3) is violated If we see any trends or change in magnitude Check with residual test "],
["mean-squared-error.html", "4.2 Mean Squared Error", " 4.2 Mean Squared Error We wish to measure the average error between the ultimate losses \\(c_u\\) and the estimate \\(\\hat{c}_u\\) \\[\\begin{align} MSE(\\hat{c}_{i,I}) = \\mathrm{E}\\left[(c_{i,I}-\\hat{c}_{i,I})^2 \\mid D\\right] \\end{align}\\] Remark. Above represents the average error between estimate and actual utlimate given data to date \\(D\\) Standard error: \\(s.e. = \\sqrt{MSE}\\) Appling the properties of \\(\\mathrm{Var}(X) = \\mathrm{E}[X^2] - \\mathrm{E}[X]^2\\) we get \\[MSE(\\hat{c}_{i,I}) = \\mathrm{E}\\left[(c_{i,I}-\\hat{c}_{i,I})^2 \\mid D\\right] = \\mathrm{Var}\\left((c_{i,I}-\\hat{c}_{i,I}) \\mid D\\right) + \\left[\\mathrm{E}\\left[(c_{i,I}-\\hat{c}_{i,I}) \\mid D\\right]\\right]^2\\] We can pull out \\(\\hat{c}_{i,I}\\) from the variance and expectation term to get \\[MSE(\\hat{c}_{i,I}) = \\underbrace{\\mathrm{Var}\\left(c_{i,I} \\mid D\\right)}_{\\text{Process Variance}} + \\underbrace{\\left[\\mathrm{E}\\left[c_{i,I} \\mid D\\right] - \\hat{c}_{i,I} \\right]^2}_{\\text{Parameter Variance}}\\] \\(R_i\\) is the unpaid for year \\(i\\): \\(R_i = c_{i,I} - c_{i,k}\\) So the estimate of the unpaid claim is \\(\\hat{R}_i = \\hat{c}_{i,I} - c_{i,k}\\) Since the difference between \\(\\hat{R}_i\\) and \\(\\hat{c}_{i,I}\\) is just paid to date (constant) so the MSE are the same \\[MSE(\\hat{R}_i) = MSE(\\hat{c}_{i,I})\\] 4.2.1 Applying the MSE Formula Important formulas below \\[\\begin{equation} MSE(\\hat{c}_{i,I}) = \\left[ s.e.(\\hat{c}_{i,I}) \\right]^2 = \\hat{c}_{i,I}^2 \\Bigg \\{ \\sum_{k = I + 1 - i}^{I-1} \\frac{\\hat{\\alpha}_k^2}{\\hat{f}_k^2} \\Bigg ( \\frac{1}{\\hat{c}_{i,k}} + \\underbrace{\\frac{1}{\\sum_{j=1}^{I-k}c_{j,k}}}_{\\text{Column x latest}}\\Bigg ) \\Bigg \\} \\tag{4.3} \\end{equation}\\] Remark. The big \\(\\sum\\) sum over the remaining years till ultimate For bit outside the \\(\\sum\\) is same for the row 4.2.1.1 Estimating \\(\\alpha\\) Important formulas below \\(\\alpha\\) is the proportions for the variance \\[\\begin{equation} \\hat{\\alpha}_k^2 = \\frac{1}{I - k - 1} \\sum_{j=1}^{I-k} c_{j,k} \\Big ( \\underbrace{\\frac{c_{j,k+1}}{c_{j,k}}-\\hat{f_k}}_{\\text{AY LDFs - Selected}} \\Big )^2 \\tag{4.4} \\end{equation}\\] Remark. This one varies by age (columns), for each age: Calculate the difference2 of the LDFs for each AY and the selected Then multiply by each row’s respective cumulative loss for that age Sum the above and then divide by the number of rows minus 1 \\(\\alpha^2_{I-1}\\) (e.g. \\(\\alpha^2_9\\) for a 10 \\(\\times\\) 10 triangle) \\[\\begin{equation} \\hat{\\alpha}_{I-1}^2 = \\operatorname{min} \\left( \\hat{\\alpha}_{I-2}^2 \\times \\dfrac{\\hat{\\alpha}_{I-2}^2}{\\hat{\\alpha}_{I-3}^2},\\hat{\\alpha}_{I-3}^2 \\right) = \\begin{cases} \\hat{\\alpha}_{I-3}^2 &amp; \\text{if } \\hat{\\alpha}_{I-3}^2 &lt; \\hat{\\alpha}_{I-2}^2 \\\\ \\hat{\\alpha}_{I-2}^2 \\times \\dfrac{\\hat{\\alpha}_{I-2}^2}{\\hat{\\alpha}_{I-3}^2} &amp; \\text{else}\\\\ \\end{cases} \\tag{4.5} \\end{equation}\\] Remark. For the last \\(\\alpha\\): If the 3rd last \\(\\alpha\\) is lower than the 2nd last \\(\\alpha\\), use the 3rd last \\(\\alpha\\) (If the final \\(alpha\\)’s are not trending down, take the lower one) If the final \\(\\alpha\\)’s are decreasing, then just take the same % of decrease to get the last \\(\\alpha\\) Also note that if we believe the claims development have stopped at some age \\(j\\) then we can set \\(\\alpha^2_j = 0\\) Finally, you can also fit a logarithmic curve to the \\(\\alpha^2_k\\)’s to estimate \\(\\alpha^2_{I-1}\\) "],
["confidence-intervals.html", "4.3 Confidence Intervals", " 4.3 Confidence Intervals Can have different assumptions on the distribution of the unpaid Important formulas below Normal Estimation \\[\\begin{equation} \\hat{R}_i \\pm Z_\\alpha \\: s.e.(\\hat{R}_i) \\tag{4.6} \\end{equation}\\] Remark. Under CLT we can assume that \\(\\hat{R}_i\\) is normally distributed given that the outstanding claims are large We can get the 95% CI with \\(Z_{0.975} = 1.96\\) Log-Normal Estimation \\[\\begin{equation} e^{\\mu_i + Z_{\\alpha} \\sigma_i} = \\hat{R}_i \\times \\exp\\left \\{ -\\dfrac{\\sigma_i^2}{2} \\pm \\: Z_\\alpha \\sigma_i \\right \\} \\tag{4.7} \\end{equation}\\] \\[\\sigma_i^2 = \\operatorname{ln} \\left [ 1 + \\left ( \\dfrac{s.e.(\\hat{R}_i)}{\\hat{R}_i} \\right)^2 \\right]\\] \\[\\mu_i = \\operatorname{ln}(\\hat{R}_i) - \\dfrac{\\sigma_i^2}{2}\\] Remark. Use log-normal when the true distribution of \\(R_i\\) is skewed Especially true when \\(s.e.(\\hat{R}_i) &gt; \\dfrac{R_i}{2}\\) This would give us negative value for the bottom end of the CI if we use normal distribution 4.3.1 CI for All Years Reserves Mean is easy: \\(\\hat{R} = \\sum \\limits_{i} \\hat{R}_i\\) But since \\(\\hat{R}_{i}\\) rely on the same LDFs they are not independent and we need to include a correlation factor for the \\(MSE(\\hat{R})\\) \\[\\begin{equation} [s.e.(\\hat{R})]^2 = \\sum \\limits_{i=2}^I \\left\\{ [s.e.(\\hat{R}_i)]^2 + \\hat{c}_{i,I} \\left( \\sum \\limits_{j = i+1}^{I} \\hat{c}_{j,I} \\right) \\left( \\sum \\limits_{k = I + 1 - i}^{I - 1} \\dfrac{2 \\hat{\\alpha}^2_k \\big/ \\hat{f}^2_k}{\\sum_{n=1}^{I-k}c_{n,k}}\\right) \\right\\} \\tag{4.8} \\end{equation}\\] If we want to simplify things we can use the square root rule to sum up the different AYs if we assume independence 4.3.2 CI Application Total CI Consider the ratio \\(\\dfrac{s.e.(\\hat{R}_i)}{\\hat{R}_i}\\) Can be high for older years since reserve is small but absoluate s.e. is also small \\(\\therefore\\) not important Good to calculate the ratio since we need it for \\(\\sigma_i^2\\) Tend to be high for most recent AY as well Driven by the large uncertainty around the development from age 12 - 24 Recall from previous section we mentioned that we should use log-normal distributio for the CI if \\(s.e.(\\hat{R}_i) &gt; \\dfrac{R_i}{2}\\), this is equivalent to \\(\\dfrac{s.e.(\\hat{R}_i)}{\\hat{R}_i} &gt; 50%\\) Then calculate the total reserve CI Allocate CI to each AY After calculating the all year CI, we can allocate the upperbound the total CI to each AYs Through trial &amp; error to figure out the \\(Z_{\\alpha}\\) for each AY that would yield the total upperbound CI Empirical Limits "],
["chain-ladder-assumptions-test.html", "4.4 Chain Ladder Assumptions Test", " 4.4 Chain Ladder Assumptions Test Tests on the various Chain Ladder assumptions Intercept Residuals Calendar year test Correlation of adjacent LDFs 4.4.1 Intercept Test for assumption 1 (prop. 4.1) Test Procdeure Plot the losses at adjacent ages Do this for every age \\(k\\) vs age \\(k+1\\) Results Interpretation We expect to see the line of best fit goes through the origin if the chain ladder assumption holds 4.4.2 Residuals Test for assumption 3 (prop. 4.3) Test Procdeure For each age \\(k\\), plot the \\(c_{i,k}\\) with the residuals \\(\\varepsilon_{i,k}\\) \\[\\begin{equation} \\varepsilon_{i,k} = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\operatorname{Var}(c_{j,k})}} \\tag{4.9} \\end{equation}\\] Remark. We can take out the \\(\\alpha^2_k\\) term since it’s constant for the same \\(k\\) e.g. \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{c_{j,k}}}\\) for weighted average assumption For residuals @ \\(k\\), you need LDFs from \\(k-1\\) to \\(k\\) Results Interpretation Residuals should vary randomly around zero across \\(c_{i,k}\\) Test can be used to test the various variance assumptions by calculating the \\(\\varepsilon\\) differently (See Table 4.1) If passed \\(\\Rightarrow\\) expected losses are linear w.r.t. cumulative losses paid to date 4.4.3 Calendar Year Test Test for assumption 1 (prop. 4.2) Step 1) Rank the LDFs in each column Step 2) Label them S and L and the median is discarded Step 3) For each diagonal with at least 2 elements, \\(z = \\operatorname{min}(\\text{# of S}, \\text{# of L})\\) Step 4) Calculate \\(\\operatorname{E}[z_n]\\) and \\(\\operatorname{Var}(z_n)\\) \\(\\operatorname{E}[z_n] = \\dfrac{n}{2} - c_n\\) Memorize Formulas \\(n =\\) # of elements in each diagonal excluding the throw away value \\(c_n = {n - 1 \\choose m}\\frac{n}{2^n}\\) \\(m = \\operatorname{floor}\\left[ \\dfrac{n-1}{2} \\right]\\) \\(\\operatorname{Var}(z_n) = \\dfrac{n(n-1)}{4} - c_n (n-1) + \\operatorname{E}[Z_n] - \\operatorname{E}[z_n]^2\\) Memorize Formulas \\(z \\sim\\) Normal Step 5) See if \\(Z\\) is in the CI based on the the above \\(Z = \\sum_{diagonal} z\\) Since \\(Z \\sim\\) Normal, can sum the mean and variance by assuming independence Test 95% CI: \\(\\operatorname{E}[Z] \\pm 2 \\times \\sigma\\) If the observed is outside the range \\(\\Rightarrow\\) There is calendar year effects, fail assumption (2) CY effect can be caused by changing or high inflation; change in claims handling; change in legal environment 4.4.4 Correlation of Adjacent LDFs Test assumption (1) (prop. 4.1) where expected only depends on most recent Use a relatively low threshold of 50% Step 1) Calculate Spearman’s correlation for each pair of adjacent LDFs Memorize Formulas \\(S = \\sum \\limits_{\\in rows} \\Big \\{ [Rank \\: Col \\: i \\: LDF] - [Rank \\: Col \\: j \\: LDF] \\Big \\}^2\\) Rank from low to high (i.e. lowest is 1) \\(T_k = 1 - \\dfrac{S}{n(n^2-1)/6}\\) \\(n =\\) # of rows For a 10 x 10 triangle, \\(k\\) is at most 7 because there’s only 9 LDFs so 8 pairs. And down to 7 because we don’t use the pair with only 1 row Step 2) Calculate \\(T\\) for the whole triangle Memorize Formulas \\(T = \\dfrac{\\sum T_k (n_k - 1)}{\\sum (n_k-1)} = \\dfrac{\\sum_k (I - k -1)T_k}{\\sum_k I - k -1}\\) \\(I =\\) size of triangle \\(k\\) starts at 2 Formula gives more weight to \\(T_k\\) with more data Step 3) Compare \\(T\\) with CI based on distribution Memorize Formulas \\(\\operatorname{E}[T] = 0\\) \\(\\operatorname{Var}[T] = \\dfrac{1}{(I-2)(I-3)/2}\\) \\(\\operatorname{E}[T] \\pm Z \\sqrt{\\operatorname{Var}(T)}\\) Use \\(Z = 0.67\\) for range of [25%, 75%] Do not reject the \\(H_0\\) of uncorrelated LDFs if the \\(T\\) is in the CI Venter has a method too "],
["past-exam-questions-3.html", "4.5 Past Exam Questions", " 4.5 Past Exam Questions Haven’t done TIA practice questions No historical questions on the MSE yet Concepts 2011 #3 b: CL method assumptions 2012 #5: CL assumptions and whether they are met 2014 #2: CL assumptions on intercept and residuals 2014 #4 b: CL LS assumptions Assumption test \\(\\star\\) 2011 #3 a (fig 4.1): CY Test 2013 #3: Residuals test 2013 #1: CY test and potential cause \\(\\star\\) 2014 #4 a (fig 4.2): Adjacent LDFs correlation test Has to use method from Venter? 2015 #3: Residuals test and plot, remember to label them 2015 #4: CY Test 4.5.1 Question Highlights Figure 4.1: 2011 Question 3 Figure 4.1: 2011 Question 3 Figure 4.2: 2014 Question 4 Figure 4.2: 2014 Question 4 -->"],
["a7-testing-the-assumptions-of-age-to-age-factors-g-venter.html", "Chapter 5 A7 Testing the Assumptions of Age-to-Age Factors - G. Venter ", " Chapter 5 A7 Testing the Assumptions of Age-to-Age Factors - G. Venter "],
["cliffs-summary.html", "5.1 Cliff’s Summary", " 5.1 Cliff’s Summary Standards used in this paper The \\(n\\) for this paper excludes the first column Coefficient \\(&gt; 2 \\sigma\\) is significant Know the adj SSE, AIC and BIC Know the 6 implications Statistical significance of \\(f(d)\\) Is there a better estimate for \\(q\\) than \\(f \\times c\\) Number of parameters BF parameters Check residuals against \\(c(w,d)\\) Stability of \\(f(d)\\) down the column No correlation among columns Know the calculation for test No particularly high or low diagonals 5.1.1 Types of Exam Questions Haven’t done TIA practice questions Concepts 2011 #4: Implication 1 2012 #5: Mack assumptions Implication Tests 2014 #4: Implication 5 correlation tests 2015 #5: Implication 5 correlation tests "],
["definitions-and-assumptions.html", "5.2 Definitions and Assumptions", " 5.2 Definitions and Assumptions Definitions Notations Definitions \\(w\\) AYs \\(d\\) Dev period; \\(d=0\\) is age @ end of year 1 \\(c(w,d)\\) Cumulative losses for AY \\(w\\) age \\(d\\) \\(q(w, d+1)\\) Inc losses for AY \\(w\\) age \\(d\\) to \\(d+1\\) \\(f(d)\\) Col parameters; LDFs or % paid; applies to whole col \\(h(w)\\) Row parameters; Ult losses; applies to whole row Mack’s assumptions \\(\\operatorname{E}[q(w,d+1) \\mid \\text{Data to } w+d] = f(d) \\: c(w,d)\\) Expected incremental development is \\(\\propto\\) reported losses \\(c(w,d)\\) is \\(\\perp\\!\\!\\!\\perp\\) of \\(c(v,g)\\) \\(\\forall\\) \\(d,g,v,w\\) given \\(v \\neq w\\) AY’s losses are \\(\\perp\\!\\!\\!\\perp\\) of other AYs’ losses \\(\\operatorname{Var}[q(w,d) \\mid \\text{Data to } w+d] = a[d,c(w,d)]\\) Variance of incremental losses is a function of reported losses and age Different \\(a(\\cdots)\\) change the \\(\\hat{f}\\) estimate Mack assumes \\(\\operatorname{Var}(q) = ac\\), variance is \\(\\propto\\) cumulative losses \\(f = \\dfrac{\\sum_w c \\frac{q}{c}}{\\sum_w c}\\) Volume weighted LDF - 1 "],
["testable-implications.html", "5.3 6 Testable Implications", " 5.3 6 Testable Implications Compare different fit of the models based on adjusted \\(SSE\\) (actual vs projected x 1st column) \\(\\dfrac{SSE}{(n-p^2)}\\) \\(AIC \\approx SSE \\times e^{2p/n}\\) Can be too permissive of over-parameterization for large data sets \\(BIC \\approx SSE \\times n^{p/n}\\) \\(n =\\) # of predicted data points EXCLUDING 1st column Exclude because when we do reserving we don’t predict anything from the first column \\(p =\\) # of parameters \\(SSE = \\sum (A - E)^2\\) Here you exclude the first column when calculating the difference 6 Testable Implications Statistical significance of \\(f(d)\\) Is there a better estimate for \\(q\\) than \\(f \\times c\\) Check residuals against \\(c(w,d)\\) Stability of \\(f(d)\\) down the column No correlation among columns No particularly high or low diagonals "],
["implication-1-significance-of-factors.html", "5.4 Implication 1: Significance of Factors", " 5.4 Implication 1: Significance of Factors Check if the coefficient is \\(&gt; 2 \\sigma\\) for 95% sure they are \\(\\neq 0\\) Can do \\(1.65 \\sigma\\) for 90% confidence "],
["implication-2-superiority-of-alternative-emergence-patterns.html", "5.5 Implication 2: Superiority of Alternative Emergence Patterns", " 5.5 Implication 2: Superiority of Alternative Emergence Patterns 5.5.1 Parameters Alternative projection on a \\(m \\times m\\) triangle \\(\\mathbf{q(w,d)}\\) Parameters Comments \\(f(d) c(w,d) + g(d)\\) \\(2m - 2\\) e.g. Least Squares Chainladder \\(m - 1\\) \\(f(d)h(w)\\) \\(2m-2\\) e.g. BF \\(f(d)h\\) \\(m-1\\) e.g. Cape Cod The -2 for the BF is due to \\(f(0)\\) and constant Can further reduce parameters by combining some row and column parameters If BF is better \\(\\Rightarrow\\) Loss emergence is more accurately represented as fluctuating around a proportion of expected ultimate losses rather than proportion of reported losses Cape Cod works when the loss ratio is stable 5.5.2 Variance Assumptions Minimize sum of squared error for the optimal parameters: e.g. error term for BF: \\(\\varepsilon(w,d) = q(w,d) - f(d)h(w)\\) Different variance assumption for \\(\\varepsilon\\) \\(\\Rightarrow\\) different parameters \\(\\operatorname{Var}(\\varepsilon) = f^p h^q\\) \\(p\\) &amp; \\(q\\) typically in (0,1,2) Weights will be \\(\\dfrac{1}{f^p h^q}\\) (inversely proportional to variance) \\(f(d)\\) = weighted average of \\(\\frac{q}{h}\\) \\(h(w)\\) = weighted average of \\(\\frac{q}{f}\\) 5.5.2.1 BF Method \\(\\mathbf{\\operatorname{Var}(q)}\\) \\(\\mathbf{f(d)}\\): Col Parameters \\(\\mathbf{h(w)}\\): Row Parameters BF (Constant Var, Least Square) \\(a(d); p=q=0\\) \\(f(d) = \\dfrac{\\sum_w h^2 \\frac{q}{h}}{\\sum_w h^2}\\) \\(h(w) = \\dfrac{\\sum_d f^2 \\frac{q}{f}}{\\sum_d f^2}\\) Cape Cod \\(a(d); p=q=0\\) \\(f(d) = \\dfrac{\\sum_w h^2 \\frac{q}{h}}{\\sum_w h^2}\\) \\(h = \\dfrac{\\sum_\\Delta f^2 \\frac{q}{f}}{\\sum_\\Delta f^2}\\) BF (Var \\(\\propto\\) \\(fh\\)) \\(a(d) \\cdot f \\cdot h; p=q=1\\) \\(f^2(d) = \\dfrac{\\sum_w h (\\frac{q}{h})^2}{\\sum_w h}\\) \\(h^2(w) = \\dfrac{\\sum_d f (\\frac{q}{f})^2}{\\sum_d f}\\) \\(f(d)\\) is % losses paid @ \\(d\\) Estimate by \\(\\frac{q}{h}\\) \\(h(w)\\) is estimate of ultimate losses Estimate by \\(\\frac{q}{f}\\) \\(f(d)\\) \\(\\sum \\downarrow\\); \\(h(w)\\) \\(\\sum \\rightarrow\\) (for the constant var BF) Need to seed one of them and iterate until convergence Use the above to estimate the parameters and then calculate the unpaid When combining parameters, don’t count the \\(f(0)\\) and always subtract 1 5.5.2.2 Chainladder \\(\\operatorname{E}[q(w,d+1)] = f(d)c(w,d)\\) \\(n = \\sum \\limits_{i=1}^{m-1} i = \\dfrac{m(m-1)}{2}\\) = predicted data point? \\(p=m-1\\) since we dont’ predict the first column \\(m =\\) dimension \\(\\mathbf{\\operatorname{Var}(q)}\\) \\(\\mathbf{f(d)}\\) \\(a(d) c\\) \\(f(d) = \\dfrac{\\sum c \\frac{q}{c}}{\\sum c}\\) \\(a(d) c^2\\) \\(f(d) = \\dfrac{\\sum 1 \\frac{q}{c}}{\\sum 1}\\) \\(a(d)\\) \\(f(d) = \\dfrac{\\sum c^2 (\\frac{q}{c})}{\\sum c^2}\\) \\(f(d)\\) LDF minus 1 Estimate by \\(\\frac{q}{c}\\) "],
["implication-3-linearity.html", "5.6 Implication 3: Linearity", " 5.6 Implication 3: Linearity The forecast incremental losses doesn’t have to be linear Test for linearity to make sure the residuals are not a sequence of positive then negative and vice versa "],
["implication-4-stability.html", "5.7 Implication 4: Stability", " 5.7 Implication 4: Stability Look at empirical LDFs \\(f(d)\\) down a column Use the entire history if factors are stable Take more recent average if unstable or follow a trend "],
["implication-5-no-correlation-on-columns.html", "5.8 Implication 5: No Correlation on Columns", " 5.8 Implication 5: No Correlation on Columns Calculate Pearson correlation for every pair of columns with at least 3 LDFs This is a test on the LDFs Test with only 2 LDFs will either be 1 or -1 Correlation \\(r = \\dfrac{\\sum \\tilde{x} \\tilde{y}}{\\sqrt{\\sum \\tilde{x}^2\\sum \\tilde{y}^2}}\\) \\(\\tilde{x} = x - \\bar{x}\\) \\(\\tilde{y} = y - \\bar{y}\\) Test statistics: \\(T = r \\sqrt{ \\dfrac{n-2}{1-r^2} }\\) \\(T \\sim t_{n-2}\\) Look up the t-value from table for 90% If the absolute value of \\(T &lt;\\) table value \\(\\Rightarrow\\) Not correlated Perform test for all columns (with 3 or more LDFs pair) Correlations within the triangle if more than \\(0.1 m + \\sqrt{m}\\) pairs are correlated m = # of pair tested "],
["implication-6-no-high-of-low-diagonals.html", "5.9 Implication 6: No High of Low Diagonals", " 5.9 Implication 6: No High of Low Diagonals Run regression that includes a variable for each diagonal Each \\(q(w,d)\\) is regressed against the cumulative losses at the prior period + dummy variable for which diagonal it is in \\(q(w,d) ~ c(w,d-1) + diagonal_{year}\\) If diagonal significatly high or low \\(\\Rightarrow\\) Dummy variable should have a statistically significant coefficient Same criteria where it is significant if coefficient is double the \\(\\sigma\\) Deficiceny of this method is that the diagonal effect is additive More likely to see multiplicative impact. e.g. from inflation This can be implement with a regression on the logarithm of the losses 5.9.1 Diagonal Trend as Inflation Model \\(q(w,d)\\) with a diagonal parameter \\(w+d\\) \\(\\operatorname{E}[q(w,d)] = f(d)h(w)g(w+d)\\) Model constant CY trend to reduce the parameters: \\(g(w+d) = (1+j)^{w+d}\\) "],
["past-exam-questions-4.html", "5.10 Past Exam Questions", " 5.10 Past Exam Questions n/a -->"],
["a8-ldf-curve-fitting-and-stochastic-reserving-a-maximum-likelihood-approach-d-clark.html", "Chapter 6 A8 LDF Curve-Fitting and Stochastic Reserving: A Maximum Likelihood Approach - D. Clark ", " Chapter 6 A8 LDF Curve-Fitting and Stochastic Reserving: A Maximum Likelihood Approach - D. Clark "],
["cliffs-summary-1.html", "6.1 Cliff’s Summary", " 6.1 Cliff’s Summary Memorize the CDF for loglogistic and weibull We use the average age of the period here Know how to calculate reserves given the \\(G(x)\\) and for each of the 2 methods given: Growth function distribution and parameter (might have to estimate) LDF or Cape Cod Data (e.g. paid to date for each AY) Test for truncation by looking at age twice the triangle Other use of the model 6.1.1 Types of Exam Questions Haven’t done TIA practice questions Reserve Calculation \\(\\star\\) 2011 #2: Cape Cod truncated and reserve CoV 2012 #2: Cape Cod method and reserve CoV \\(\\star \\star\\) 2013 #3: LDF method with \\(\\sigma^2\\) calc 2013 #10: Stanard-Buhlmann 2014 #5: plug and play LDF and Cape Cod method with Benktander 2015 #2: LDF Method and concept where if we switch to Cape Cod the CoV should go down as it incorporates additional information Other 2012 #3: Residual plot from projection and assumptions "],
["expected-loss-emergence.html", "6.2 Expected Loss Emergence", " 6.2 Expected Loss Emergence \\(x =\\) average age of AY (e.g. 6mo for the most recent instead of 12mo) Note % paid to date growth function: \\(G(x \\mid \\omega, \\theta)\\) Memorize Formula Loglogistic: \\(G(x \\mid \\omega, \\theta) = \\dfrac{x^{\\omega}}{x^{\\omega} + \\theta^{\\omega}}\\) Weibull: \\(G(x \\mid \\omega, \\theta) = 1- \\operatorname{exp}\\left\\{ { - \\left( \\dfrac{x}{ \\theta } \\right)^{\\omega}} \\right \\}\\) The curves move smoothly from 0 to 1 Advantages: Uses only 2 column parameters: \\(\\theta\\) for mean; \\(\\omega\\) for s.d. Can use data @ different age Output is a smooth curve \\(\\Rightarrow\\) Can interpolate between ages and estimates a tail Expected Ultimate Loss Method 1: Cape Cod \\(Premium_{AY} \\times ELR\\) Having a single row parameter \\(h\\) Method 2: LDF \\(ULT_{AY}\\) Having a \\(h(w)\\) for each row Estimate Future Emergence Method 1: \\([G(y \\mid \\omega, \\theta) - G(x \\mid \\omega, \\theta)] \\times [Premium_{AY} \\times ELR]\\) Method 2: \\([G(y \\mid \\omega, \\theta) - G(x \\mid \\omega, \\theta)] \\times ULT_{AY}\\) "],
["distribution-of-actual-loss-emergence-and-maximum-likelihood.html", "6.3 Distribution of Actual Loss Emergence and Maximum Likelihood", " 6.3 Distribution of Actual Loss Emergence and Maximum Likelihood Parameter estimation with MLE 6.3.1 Process Variance For each incremental loss, \\(\\operatorname{Var}(c_i) \\propto \\operatorname{E}[c_i]\\) \\(\\operatorname{Var}(c_i) = \\sigma^2 \\operatorname{E}[c_i]\\) \\(c_i\\) is the incremental loss here Note \\(\\sigma^2\\) is the same for the entire triangle \\(\\dfrac{Variance}{Mean} = \\sigma^2 = \\dfrac{1}{n-p}\\sum\\limits_{i \\in \\Delta}^n\\dfrac{(c_i - \\mu_i)^2}{\\mu_i}\\) Memorize Formula \\(n =\\) # of data points in triangle \\(p =\\) # of parameters Cape Cod \\(p=3\\) (\\(\\omega, \\theta, ELR\\)) LDF \\(p=2 +\\) # of AYs (\\(\\omega, \\theta,\\) row parameters) \\(c_i =\\) actual incremental loss emergence \\(\\mu_i =\\) expected incremental loss emergence Variance relationship in other papers Mack (1994): CL method assumes \\(\\operatorname{Var}\\left (c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ) = \\alpha_k^2 \\: c_{i,k}\\) Constant is same for each column (development period) n includes all here? Venter Factors: BF method assumed variance \\(\\propto\\) expected losses in a cell Constant varied by column n = predicted? \\(C_i \\sim ODP(\\lambda_i, \\sigma^2)\\) Use ODP so that variance \\(\\neq\\) mean \\(C_i = \\sigma^2 X\\) where \\(X \\sim Poi(\\lambda_i)\\) \\(\\operatorname{E}[C_i] = \\sigma^2 \\lambda_i = \\mu_i\\) \\(\\operatorname{Var}(C_i) = \\sigma^2 \\mu_i\\) Potential issue with ODP is that some granularity is lost since reserves are estimated in multiple of \\(\\sigma^2\\). However \\(\\sigma^2\\) is generally small so little precision is lost 6.3.2 MLE Not super testable \\(l = \\sum \\limits_{i \\in \\Delta} c_i \\operatorname{ln}(\\mu_i) - \\mu_i\\) Memorize Formula \\(\\sigma^2\\) was assumed to be a known constant Plug in \\(\\mu_i\\) for the different methods and then maximize \\(l\\) by \\(\\dfrac{\\partial l}{\\partial ELR} = 0\\) or \\(\\dfrac{\\partial l}{\\partial ULT_{AY}} = 0\\) 6.3.3 Parameter Variance Not super testable Covariance matrix = \\(\\Sigma = -\\sigma^2 \\times I^{-1}\\) Information matrix \\(I\\): Use 2nd derivative matrix of \\(l\\) vs each parameter \\(3 \\times 3\\) matrix for Cape Cod \\((n+2) \\times (n+2)\\) for LDF "],
["variance-of-the-reserves.html", "6.4 Variance of the Reserves", " 6.4 Variance of the Reserves Process Variance of \\(R\\) \\(\\sigma^2 \\sum_i \\mu_i\\) Technically testable if given \\(\\sigma^2\\) Not sure how Parameter Variance of \\(R\\) \\(\\operatorname{Var}(\\operatorname{E}[R]) = (\\partial R)&#39;\\Sigma (\\partial R)\\) \\(\\partial R\\) = vector that is the derivative of the reserve by each parameter Calculation heavy, not testable "],
["key-assumptions.html", "6.5 Key Assumptions", " 6.5 Key Assumptions Incremental losses \\(iid\\) Can test this by looking at residuals \\(\\dfrac{Variance}{Mean}\\) scale parameter \\(\\sigma^2\\) is fixed and known Technically this should be estimated with the other parameters but will makes things intractable Variance estimates are based on the approximation to the Rao-Cramer lower bound Variance based on information matrix \\(I\\) \\(I\\) is exact only when using linear functions In our case this is simply a lower bound We are using approximated parameters The above temper the volatility in the model, actual results can be more variable Model only works for positive expected incremental losses. But a negative loss here or there is fine as well "],
["ldf-method-method-2.html", "6.6 LDF Method (Method 2)", " 6.6 LDF Method (Method 2) Can use either Loglogistic or Weibull for \\(G(x)\\) Estimate \\(\\theta\\) and \\(\\omega\\) with MLE \\(\\mu = ULT_{AY} \\times [G(y) - G(x)]\\) Calculate \\(\\sigma^2 = \\dfrac{1}{n-p}\\sum\\limits_{i \\in \\Delta}^n\\dfrac{(c_i - \\mu_i)^2}{\\mu_i}\\) n = all data points (not just the predicted like in Venter) p = 2 + 10 AYs This is for incremental losses Residual Review Residuals = \\(r_i = \\dfrac{c_i - \\mu_i}{\\sigma \\sqrt{\\mu_i}}\\) Divide by the square root of the variance \\(\\sigma^2 \\mu\\) for ODP? Plot \\(r\\) vs age, expected loss (test constant var/mean assumption) 6.6.1 Reserve Estimate Untruncated Get G(x): % paid(reported) to date Paid to date (reported to date) \\(\\div\\) G(x) = Ultimate Truncated @ age \\(x_t\\) To cut of the tail at some point and stop the development Remember to subtract 6 months Use \\(G&#39;(x) = \\dfrac{G(x)}{G(x_t)}\\) instead just like above Process Variance = \\(\\sigma^2 \\sum_i \\mu_i = \\sigma^2 \\times \\text{Unpaid}\\) Parameter variance is huge and computational intensive as it requires inverting a big matrix "],
["cape-cod-method-method-1.html", "6.7 Cape Cod Method (Method 1)", " 6.7 Cape Cod Method (Method 1) Needs exposure base (e.g. on-level and trended EP) that allow us to assume a constant ELR across AYs Estimate \\(\\theta\\) and \\(\\omega\\) with MLE Estimate ELR with below: \\(ELR = \\sum_{AY} \\dfrac{\\text{Losses Paid to Date}}{\\underbrace{G(x)}_{\\text{Expected portion paid}} \\times Premium}\\) Do not truncate when calculating ELR \\(\\mu = ULT_{AY} \\times [G(y) - G(x)]\\) Calculate \\(\\sigma^2 = \\dfrac{1}{n-p}\\sum\\limits_{i \\in \\Delta}^n\\dfrac{(c_i - \\mu_i)^2}{\\mu_i}\\) n = all data points (not just the predicted like in Venter) P = 3 Check if the assumption of one expected LR is reasonable by looking for any upward or downward trends in the ultimate LR 6.7.1 Reserve Estimate Untruncated Reserve = On-level Premium \\(\\times ELR \\times [1 - G(x)]\\) Truncated @ age \\(x_t\\) Reserve = On-level Premium \\(\\times ELR \\times [(G(x_t) - G(x)]\\) Similar story for the process variance and parameter variance as the LDF method. The Covariance matrix is smaller just \\(3 \\times 3\\) Parameter variance is smaller than the LDF method since we have more information in the Cape Cod method (?) "],
["other-use-of-model.html", "6.8 Other Use of Model", " 6.8 Other Use of Model 6.8.1 Variance of Prospective Losses Estimate variance for the next u/w year Need to be given the \\(\\operatorname{Var}(ELR)\\) Process Variance: \\(\\sigma^2 R\\) Parameter Variance: \\(\\left(\\sqrt{\\operatorname{Var}(ELR)}\\times Premium \\right)^2\\) Total Variance: Process Variance + Parameter Variance CoV: \\(\\dfrac{\\sqrt{Total \\: Variance}}{R}\\) 6.8.2 Calendar Year Development Calculate \\([G(x+12) - G(x)] \\times Ultimate\\) No truncation here Sum for all AYs and compare with actual calendar year emergence Can calculate the s.d. to see if it’s in range (process var is still \\(\\sigma^2 \\times\\) estimate) 6.8.3 Variability in the Discounted Reserves CV will be smaller since the tail with the most variability gets discounted the most "],
["comments-and-conclusion.html", "6.9 Comments and Conclusion", " 6.9 Comments and Conclusion Can use data in table format Use the CV from the model even if select a different reserve Not okay since you don’t trust the reserve estimate Okay since the s.d. is a selection and the CoV from this model is a reasonable basis Curve was selected because: The move smoothly from 0 to 1 Closely match empirical data 1st and 2nd derivative are calculable Others can be used as well "],
["past-exam-questions-5.html", "6.10 Past Exam Questions", " 6.10 Past Exam Questions -->"],
["a4-a-model-for-reserving-workers-compensation-high-deductibles-j-siewert.html", "Chapter 7 A4 A Model for Reserving Workers Compensation High Deductibles - J. Siewert ", " Chapter 7 A4 A Model for Reserving Workers Compensation High Deductibles - J. Siewert "],
["cliffs-summary-2.html", "7.1 Cliff’s Summary", " 7.1 Cliff’s Summary Know the 6 methods and their pros and cons: Loss Ratio Method Implied Development Direct Development \\(XSLDF_t^{L} = \\dfrac{Ult_{XS}}{S_t^{XS}} = \\dfrac{Ult \\cdot \\chi}{\\frac{Ult}{LDF_t} - \\frac{Ult\\cdot(1-\\chi)}{LDF_t^L}}\\) Credibility Weight Method Development Method Severity needs to be trended Claim counts are developed separately ground up Severity LDF formulas, know them well to manipulate and know what formula requires what \\(LDF_t^L = LDF_t \\dfrac{R^L}{R_t^L}\\) \\(XSLDF_t^L = LDF_t \\dfrac{(1-R^L)}{(1-R_t^L)}\\) \\(LDF_t = R^L_t \\cdot LDF^L_t + (1 - R^L_t) \\cdot XSLDF^L_t\\) \\(\\dfrac{ILDF^L_t}{ILDF_t} = \\Delta R^L_t = \\dfrac{R^L_{t+1}}{R^L_t}\\) \\(\\dfrac{IXSLDF^L_t}{IXSLDF_t} = \\Delta (1 - R^L_t) = \\dfrac{1 - R^L_{t+1}}{1 - R^L_t}\\) Distribution Method 7.1.1 Types of Exam Questions Concepts 1998 - #41: Discuss methods and pros/cons 2000 - #4: Relativities relationship 2001 - #11: Development method process 2002 - #28 b: Pros/cons implied development 2005 - #19 b c: Inflation index limit, implied dev advantages 2006 - #6: LR approach advantage 2008 - #12 b: LR method pros and cons 2011 - #6 c: BF pros (more stable and can be tied to pricing estimates) Simple Plug and Play Calculations 1999 - #29: Service revenue calc = utl \\(\\times\\) loss multiplier - known recoverables 2001 - #23: Loss ratio method for aggregate loss charge 2002 - #28 a c: XS deductible IBNR reserve and service revenue 2003 - #8: Loss ratio method for losses XS aggregate limit \\(\\star\\) 2004 - #21: IBNR using LR, implied, direct, BF 2005 - #19 a: Implied LDF method 2008 - #12 a: LR method both occ and aggregate 2010 - #3: Back out limited LDF \\(\\star\\) 2010 - #9: XS limited LDF based on relativity 2012 - #6: Get unlimited with limited and XS and relativity with the formula \\(\\star\\) 2014 #6: Use incremental formula and also the combined formula 2015 #6 a b: LR and implied dev method Full Calcualation \\(\\star\\) 2007 - #41: XS and limited implied LDF calc from ground up with aggregate limit 2009 - #7: implied and direct development from triangle of unlimited and XS 2011 - #6 a b: Direct development and BF (slightly erroneous question) \\(\\star \\star\\) 2015 - #6 c: Calculate layer between "],
["introduction.html", "7.2 Introduction", " 7.2 Introduction WC high deductible reserving with occurrence and/or aggregate deductible "],
["loss-ratio-method.html", "7.3 1. Loss Ratio Method", " 7.3 1. Loss Ratio Method Per Occurrence Expected XS loss \\(P \\cdot E \\cdot \\chi\\) \\(P\\) = Premium \\(E\\) = Expected ground up loss ratio \\(\\chi\\) = Occurrence charge = % of losses above deductible Expected Loss XS Aggregate \\(P \\cdot E \\cdot (1-\\chi) \\cdot \\varphi\\) \\(\\varphi\\) = Aggregate charge = % of losses in deductible layer that exceed the aggregate \\(\\chi\\) and \\(\\varphi\\) are from industry tables Specific to the deductible, aggregate, and size of expected losses These are ultimate loss estimate Advantages Useful when little data is available Ties to pricing Can include industry experience Disadvantages Ignores actual emergence May not properly reflect account characteristics "],
["implied-development.html", "7.4 2. Implied Development", " 7.4 2. Implied Development Ultimate excess loss = ultimate unlimited loss - ultimate limited loss Need to make sure LDFs and development are consistent with the different layers Adjust the deductible for inflation when selecting limited LDFs; Else it distorts the LDFs Advantages Get estimate for early period with no losses Limited factors are more stable Disadvantages Weakness: Does not directly estimate the XS loss "],
["direct-development.html", "7.5 3. Direct Development", " 7.5 3. Direct Development Determine LDFs for XS layer with unlimited LDFs, limited LDFs and \\(\\chi\\) \\(XSLDF_t^{L} = \\dfrac{Ult_{XS}}{Y_t^{XS}} = \\dfrac{Ult \\cdot \\chi}{\\frac{Ult}{LDF_t} - \\frac{Ult\\cdot(1-\\chi)}{LDF_t^L}} = \\dfrac{\\chi}{\\frac{1}{LDF_t} - \\frac{(1-\\chi)}{LDF_t^L}}\\) Based on \\(\\underbrace{Y_t \\cdot LDF_t}_{100\\%} = \\underbrace{Y^L_t \\cdot LDF^L_t}_{100\\% - \\chi} + \\underbrace{XSY_t^L \\cdot XSLDF^L_t}_{\\chi}\\) Can’t use the actual losses to date for this as it’ll just be equal to the implied method mathematically Disadvantages LDFs large and volatile, not recommend this method "],
["credibility-weighting-techniques-bornhuetter-ferguson.html", "7.6 4. Credibility Weighting Techniques / Bornhuetter-Ferguson", " 7.6 4. Credibility Weighting Techniques / Bornhuetter-Ferguson Ultimate Loss: \\(L = Z\\underbrace{(O_t \\cdot XSLDF_t^{L})}_{\\text{Inc&#39;d to Date } \\times \\text{ XS LDF}} + (1 - Z)\\underbrace{E}_{\\text{Method 1}}\\) \\(Z = \\frac{1}{XSLDF_t^L}\\) then we have the BF method "],
["development-method.html", "7.7 5. Development Method", " 7.7 5. Development Method Data Ground up WC claims level data Indemnity, medical and ALAE together Can split the data by account, injury, and state as a future step 7.7.1 Severity Trend Important to account for loss trend when capping losses Apply different limits to the historical data by AYs or else there will be more and more losses piercing the limiting layer \\(\\Rightarrow\\) Distorting the LDFs Look at average severity of unlimited losses by AY and fit an exponential curve Can use different trend for different years Can adjust for large losses as well Cap the historical losses at lower amount to compensate for the loss trend by detrending the historical layer 7.7.2 Claim Count Development Split the development of claim count dev from severity dev and focusing on ground up claims Advantages: Most claims are reported not too deep in the tail even if the full severity in not yet known If you only count claims once they pierce the deductible layer you have to deal with uncertain claim count development deep into the tail If your claim counts depend on the limits, every time you update the severity trend assumptions the claim counts will change 7.7.3 Severity LDFs Definition \\(R_t^L = \\dfrac{\\text{Limited Sev @ t}}{\\text{Unlimited Sev @ t}}\\) Relativity starts close to 1.00, and drops over time before reaching the ultimate relativity The losses limited at higher limits start with a very high relativity (close to 1.00), and take longer to come down \\(R^L = \\dfrac{\\text{Limited Sev @ Ult}}{\\text{Unlimited Sev @ Ult}}\\) Key Relationships \\(LDF_t^L = LDF_t \\dfrac{R^L}{R_t^L}\\) Memorize Formula \\(\\cdots = \\dfrac{C \\cdot S^L}{C_t \\cdot S^L_t} = \\dfrac{C \\cdot S \\cdot R^L}{C_t \\cdot S_t \\cdot R^L_t} = \\dfrac{C \\cdot S}{C_t \\cdot S_t } \\times \\dfrac{R^L}{R^L_t} = \\cdots\\) \\(C\\) is claim count and \\(S\\) is severity \\(XSLDF_t^L = LDF_t \\dfrac{(1-R^L)}{(1-R_t^L)}\\) Memorize Formula \\(LDF_t = R^L_t \\cdot LDF^L_t + (1 - R^L_t) \\cdot XSLDF^L_t\\) Memorize Formula This formula doesn’t require \\(R^L\\) Formula only work once claim reporting is finished Relativity based on average from historical losses Incremental LDFs \\(\\dfrac{ILDF^L_t}{ILDF_t} = \\Delta R^L_t = \\dfrac{R^L_{t+1}}{R^L_t}\\) Difference of limited and unlimited incremental LDFs is driven by the change in relativity With \\(R_t\\) and \\(R_{t+1}\\) we can get the limited or unlimited LDF given one or the other \\(\\dfrac{IXSLDF^L_t}{IXSLDF_t} = \\Delta (1 - R^L_t) = \\dfrac{1 - R^L_{t+1}}{1 - R^L_t}\\) "],
["distribution-model.html", "7.8 6. Distribution Model", " 7.8 6. Distribution Model Model severity of losses @ each age with Weibull Parameters different @ different ages; make sure they are consistent This will maintain the relationships of the limited and unlimited over time Can easily interpolate among limits and years Mean of Weibull = \\(\\theta \\cdot \\Gamma (1 + \\frac{1}{\\omega})\\) Can then use the distribution and then applies all the relationships discussed in 5 "],
["aggregate-limits.html", "7.9 Aggregate Limits", " 7.9 Aggregate Limits Use collective risk model with Poisson and Weibull to build out a table with excess loss for each deductible and aggregate Need 4 inputs: expected unlimited, age, deductible and aggregate limit The paper shows calculating the reserve as just the expected aggregate loss \\(\\times\\) % unreported Doesn’t take into account if the aggregate is about to be pieced of not Appendix discuss calculation with Table M "],
["past-exam-questions-6.html", "7.10 Past Exam Questions", " 7.10 Past Exam Questions -->"],
["a5-claims-development-by-layer-r-sahasrabuddhe.html", "Chapter 8 A5 Claims Development by Layer - R. Sahasrabuddhe ", " Chapter 8 A5 Claims Development by Layer - R. Sahasrabuddhe "],
["cliffs-summary-3.html", "8.1 Cliff’s Summary", " 8.1 Cliff’s Summary Setup base triangle and then convert the base LDFs to any layer Main formulas to memorize: \\(LEV(X; \\Phi \\sim Exp(\\theta)) = \\theta \\: \\left[ 1 - \\operatorname{exp}\\left\\{-\\left(\\dfrac{x}{\\theta}\\right)\\right\\} \\right]\\) \\(\\begin{align} F_{ij}^X = F_{nj}^B \\times \\frac{LEV(X;\\Phi_{i\\infty})\\div LEV(B;\\Phi_{n\\infty})}{LEV(X;\\Phi_{ij}) \\div LEV(B;\\Phi_{nj})} \\end{align}\\) Know the assumptions used 8.1.1 Types of Exam Questions Haven’t done TIA practice questions Setup Base Triangle 2013 #5: set up base triangle with trend and all Can start trend from bottom left cell "],
["introduction-1.html", "8.2 Introduction", " 8.2 Introduction 2 Key Ideas: Convert a triangle to one level of trend and layer of losses This is then used to determine LDFs at this base layer Convert LDFs at a Base layer to LDFs at any other layer Claim size models is the distribution for individual claims Exponential in this paper Different distribution for each column in the triangle Requires a distribution of losses at each age, which can be difficult (Last section try to address this) Notation \\(B\\): Base layer, LDFs are determined at this layer \\(X\\): Layer of interest, ultimately we want LDFs for this layer \\(\\Phi_{ij}\\): Cumulative loss distn in row \\(i\\) age \\(j\\) \\(LEV(X; \\Phi_{ij})\\): Limited expected value, average loss with distn \\(\\Phi_{ij}\\) capped at \\(X\\) \\(F_{ij}^X\\): LDF to ultimate for cell \\(ij\\) with losses capped at \\(X\\) 8.2.1 1. Setup Base Layer Triangle Setup a consistent base layer triangle for LDFs selection Step 1: Setup the trend triangle: Trend = AY Trend \\(\\times\\) CY Trend Adjust for CY trend before calculating No adjustment needed if only AY trend is present Really? These are ground up trend, which is consistent if taken from external sources Trend to the last row of the triangle Don’t stop just at the diagonal The 1.000 typically starts at the top left corner but it doesn’t have to This applies the trend to cumulative loss which is problematic Cumulative trend depends on the incremental trend and the emergence pattern Step 2: Determine unlimited mean for each cell in triangle (Avg sev paid to date) Based on mean for the latest AY (bottom row) \\(\\operatorname{E}[C_{nj}] = \\theta_j\\) \\(C_{nj} \\sim \\Phi_{nj} = Exp(\\theta_j)\\) Detrend back up from the bottom row to fill the whole square Usually just need four points per the LDF conversion formula Step 3: Calculate LEV for each cell in triangle \\(L\\) and the last row for \\(B\\) Memorize Formula \\(LEV(X; \\Phi \\sim Exp(\\theta)) = \\theta \\: \\left[ 1 - \\operatorname{exp}\\left\\{-\\left(\\dfrac{x}{\\theta}\\right)\\right\\} \\right]\\) Use the “square” of mean \\(\\theta\\) calculated from Step 2 Step 4: Calculate the adjusted triangle Convert triangle of actual losses by dividing it’s LEV at layer \\(L\\) then times it’s LEV at layer \\(B\\) Memorize Formula \\(C_{ij}&#39; = \\underbrace{C_{ij}^L}_{\\text{Cum paid @ L}} \\times \\underbrace{\\dfrac{LEV(B;\\Phi_{nj})}{LEV(L;\\Phi_{ij})}}_{\\text{ILF w/ on-level to }n}\\) Step 5: Select base layer LDFs 8.2.2 2. Convert LDFs from Base Layer Convert LDFs from base layer to LDFs at any other layer At this step we should already have \\(F_{nj}^B\\) selected from the base triangle created above Memorize Formula \\(\\begin{align} F_{ij}^X = F_{nj}^B \\times \\frac{LEV(X;\\Phi_{i\\infty})\\div LEV(B;\\Phi_{n\\infty})}{LEV(X;\\Phi_{ij}) \\div LEV(B;\\Phi_{nj})} \\end{align}\\) Without trend, you can basically just take the middle part as the LDFs However, we trust the LDF from the analysis of losses limited to \\(B\\) more and less so with the distribution function \\(\\Phi\\) "],
["ldfs-for-xs-layers.html", "8.3 LDFs for XS Layers", " 8.3 LDFs for XS Layers \\(\\begin{align} F_{ij} = F_{nj}^B \\times \\frac{ \\left[{\\color{blue}{LEV(Y;\\Phi_{i\\infty}) - }} LEV(X;\\Phi_{i\\infty})\\right]\\div LEV(B;\\Phi_{n\\infty})}{ \\left[ {\\color{blue}{LEV(Y;\\Phi_{ij}) - }} LEV(X;\\Phi_{ij}) \\right] \\div LEV(B;\\Phi_{nj})} \\end{align}\\) "],
["other-practical-uses.html", "8.4 Other Practical Uses", " 8.4 Other Practical Uses If we don’t have the severity distribution by age, we can work with the severity at ultimate and estimate \\(R_j(X,B)\\), for lower layers \\(X &lt; B\\) \\(\\begin{align} F_{ij}^X = F_{nj}^B \\times \\frac{LEV(X;\\Phi_{i\\infty})\\div LEV(B;\\Phi_{ {\\color{red}i} \\infty})}{R_j{(X,B)}} \\end{align}\\) Note the \\(i\\) in the second part of the numerator (I guess it’s to be consistent with formula below) \\(R_{j}(X,B) = \\dfrac{LEV(X;\\Phi_{ij})}{LEV(B;\\Phi_{ij})}\\) \\(j\\) is on the diagonal (?) This needs to be estimated Assumes \\(\\dfrac{LEV(B; \\Phi_{n \\infty})}{LEV(B; \\Phi_{nj})} \\approx \\dfrac{LEV(B; \\Phi_{i \\infty})}{LEV(B; \\Phi_{ij})}\\) Ratio of losses at different cost layers is immaterial The expected LDF in different AYs are similar when losses are capped at \\(B\\) Reasonable assumption with low inflation See how this change pictorially Estimate \\(R_j(X,B)\\) \\(U_i \\leq R_j(X,B) \\leq U_i \\cdot F_{ij}^{\\infty} \\leq 1\\) \\(U_i = R_{i\\infty}(X,B) = \\dfrac{LEV(X;\\Phi_{i\\infty})}{LEV(B;\\Phi_{i \\infty})}\\) We expect \\(U_i\\) to \\(\\downarrow\\) for more recent AYs due to loss trend; Larger % of losses pierce the lower layer \\(U_i\\) increases as we move cross maturity (not sure how this works as these are ult…) \\(U_i\\) is the ratio at ultimate of limited means; same as \\(R_{i \\infty}\\) Example from text: Select a decay from 1.000 and approaches \\(U_i\\) as maturity increase Overlay with empirical \\(R_j(X,B)\\) along the diagonal Estimate doesn’t work when: There is expected negative development XS layer develops more quickly than a working layer "],
["issues-with-assumptions.html", "8.5 Issues with Assumptions", " 8.5 Issues with Assumptions Three assumptions that are likely reasonable Must select a basic limit \\(B\\) Needs an ultimate claim size model Or use ILFs Just have to accurate around the basic limit and policy limit since that’s where we calculate expectations Need to create a triangle of losses at basic limit and one cost level We do this in the step converting the base triangle Two assumptions that are more tenuous Requires robust claim size model at each age Ultimately we need the ratio of limited losses at different layers right, not the absolute value Trend Trend should be applied to the incremental losses Not clear how to apply trend to reported losses (by day of reserve or payment?) "],
["past-exam-questions-7.html", "8.6 Past Exam Questions", " 8.6 Past Exam Questions n/a -->"],
["a10-bootstrap-modeling-beyond-the-basics-shapland-leong.html", "Chapter 9 A10 Bootstrap Modeling: Beyond the Basics - Shapland Leong ", " Chapter 9 A10 Bootstrap Modeling: Beyond the Basics - Shapland Leong "],
["cliffs-summary-4.html", "9.1 Cliff’s Summary", " 9.1 Cliff’s Summary Flow of the paper: Parametize \\(\\rightarrow\\) Bootstrap \\(\\rightarrow\\) Practical Issues \\(\\rightarrow\\) Correlations Remember this uses incremental triangles Parametize \\(m_{wd} = \\operatorname{exp} \\left [\\alpha_w + \\sum_j \\beta_j \\right]\\) \\(\\operatorname{Var}[q(w,d)] = \\phi m_{wd}^z\\) Parametize with GLM or Wtd average Bootstrap Create sample trianlge from mean and randomly sampled residuals \\(\\Rightarrow\\) Estimate parameters from sampled triangle \\(\\Rightarrow\\) Calculate mean and variance of the sampled triangle \\(\\Rightarrow\\) Draw losses from gamma Dispersion factor with standard, England &amp; Verrall, and standardized Practical Issues Negative incremental values (fit and simulate) Non-zero sum of residuals N-year wtd average Missing values Outliers Heteroskedasticity Partial latest year exposure or partial diagonal Expsoure adjustment Parametric bootstrap Diagnostics Other Multiple models Model outputs Correlations 9.1.1 Types of Exam Questions Exercises \\(\\star\\) Use simplied GLM and then back out the GLM parameters Reduce parameters Minimize square error for GLM Benefit of simplified GLM Residuals Dispersion Dispersion with hat matrix adj Simulate loss Setup GLM Negative values Simulat ngative Partial triangle Stratified Dealing with correlation Outliers Practical Issues 2013 #7: negatvie values 2014 #7: List 4 practical issues and solutions 2015 #10: Heteroscedasticity, why important, adjustments description 2015 #11: Negative values, outliers, exposure level Diagnostics \\(\\star\\) 2014 #9: Evaluate the results given mean unpaid, s.e., CoV by AYs "],
["introduction-2.html", "9.2 Introduction", " 9.2 Introduction Same notations as Venter \\(q(w,d)\\): incremental loss for AY \\(w\\) from age \\(d-1\\) to \\(d\\) \\(c(w,d)\\): cumulative loss Advantages Generates a distribution of the estimate of unpaid claims Can be tailored to statistical features of our data Reflects that loss distn are usually skewed to the right Disadvantages Takes more time to create, but okay once set up Chainladder assumes 1) LDFs are the same for each row 2) Each AY has a parameter representing it’s value, e.g. CL project based on level of losses to date 9.2.1 Loss Distribution Mean losses for \\(q(w,d)\\): \\(m_{wd} = \\operatorname{exp} \\left [\\alpha_w + \\sum_j \\beta_j \\right]\\) \\(\\alpha_i\\) and \\(\\beta_j\\) are selected to minimize error between \\(\\operatorname{ln}(actual) - \\operatorname{ln}(forecast)\\) Equivalence for using Venter notation: \\(h(w) = e^{\\alpha}\\) \\(f(d) = e^{\\sum \\beta}\\) Variance \\(\\operatorname{Var}[q(w,d)] = \\phi m_{wd}^z\\) \\(\\phi\\): Dispersion factor Estimate from residual \\(z\\): Error distribution Paper focus on \\(z = 1\\) for Over Dispersed Poisson (ODP) z Distribution 0 Normal 1 Poisson 2 Gamma 3 Inverse Gaussian "],
["parameterize-with-glm.html", "9.3 Parameterize with GLM", " 9.3 Parameterize with GLM For a \\(3\\times 3\\) triangle: Log Actual incremental losses \\(Y = \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix}\\) Solution Matrix \\(W\\) \\(\\begin{array}{ccccc} W &amp; = &amp; X &amp;\\times &amp; A \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\alpha_2 \\:\\:\\: \\alpha_3 \\:\\:\\: \\beta_2 \\:\\:\\: \\beta_3 &amp; &amp;\\\\ \\begin{bmatrix} ln[m_{11}] \\\\ ln[m_{21}] \\\\ ln[m_{31}] \\\\ ln[m_{12}] \\\\ ln[m_{22}] \\\\ ln[m_{13}] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - &amp; - &amp; - \\\\ - &amp; 1 &amp; - &amp; - &amp; - \\\\ - &amp; - &amp; 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 &amp; - \\\\ - &amp; 1 &amp; - &amp; 1 &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 &amp; 1 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\end{bmatrix} \\end{array}\\) \\(W =\\) Solution Matrix \\(X =\\) Design Matrix Defines the parameters used to estimate the losses \\(A =\\) parameters Solve for \\(A\\) by minimizing the difference2 (SSE) between \\(W\\) and \\(Y\\) Use recursive Newton-Raphson method to find the best fit parameters Paper did no cover in what sense these parameters are best fit (Not SSE?) "],
["simplified-glm.html", "9.4 Simplified GLM", " 9.4 Simplified GLM GLM model = Chainladder w/ volume-weighted averages when: Variance \\(\\propto\\) Mean \\(\\varepsilon \\sim\\) Poisson; Poisson error Parameter for each row and column (x 1st column) Benefits: Replace GLM fitting with much simpler calculation LDFs are easier to explain Still works even when there are negative incremental values 9.4.1 Residuals (Pearson) Memorize Formula \\(\\begin{array} Rr_p &amp; = &amp; \\dfrac{A - E}{\\sqrt{\\operatorname{Var}(A)}} &amp; \\\\ &amp; = &amp; \\dfrac{q(w,d) - m_{wd}}{\\sqrt{\\phi m_{wd}}} &amp; \\text{Mean &amp; Var Assumptions Above}\\\\ &amp; \\propto &amp; \\dfrac{q(w,d) - m_{wd}}{\\sqrt{m_{wd}}} &amp; \\text{Since }\\phi \\text{ is constant for all}\\\\ \\end{array}\\) Mean &amp; Var Note the residual for the 2 corners of the triangle are going to be 0 because only the row parameter or column parameter are used "],
["bootstrap-procedures.html", "9.5 Bootstrap Procedures", " 9.5 Bootstrap Procedures Once we have the mean and residuals in each cell, repeat below: Create a sampled \\(triangle^*\\) from the residuals and the means Sample from residuals since data needs to be \\(iid\\) for bootstrap Use Pearson residuals Simulated loss: \\(q^*(w,d) = m_{wd} + r_p \\sqrt{m_{wd}^z}\\) Memorize Formula Simulate by sampling residuals with replacement Estimate \\(\\phi\\) for step 4: \\(\\phi = \\dfrac{\\sum r^2}{n-p}\\) This can vary see Dispersion Factor section Determine parameters from \\(triangle^*\\): \\((\\alpha_i, \\beta_j)\\) Use either GLM or Simplified GLM to project ultimate loss Calculate mean and variance: \\((m_{wd}, \\phi m_{wd})\\) For GLM use \\(m_{wd} = \\operatorname{exp} \\left [\\alpha_w + \\sum_j \\beta_j \\right]\\) For Simplified GLm, back out the \\(c^*(w,d)\\) by \\(\\dfrac{Ult_w}{CDF_d}\\) then get the \\(m_{wd}\\) For variance \\(\\phi m_{wd}^z\\) Add process variance: draw losses from \\(Gamma(m_{wd}^*,\\phi m_{wd}^*)\\) Randomly draw from the distribution for each cell Calculate simulated unpaid: sum of bottom half of triangle "],
["dispersion-factor.html", "9.6 Dispersion Factor", " 9.6 Dispersion Factor Standard \\(\\phi = \\dfrac{\\sum r_{wd}^2}{n-p}\\) \\(n =\\) # of data points (including first column) \\(p =\\) # of parameters \\(2m-1\\): one for each row, one for each column minus first column England &amp; Verrall Degrees of freedom adjustments \\(\\phi^{EV} = \\dfrac{\\sum \\left(r^{EV}_{wd}\\right)^2}{n-p}\\) \\(\\begin{array}{llllc} r_{wd}^{EV} &amp; = &amp; r_{wd} &amp;\\times &amp;f \\\\ &amp; = &amp; r_{wd} &amp;\\times &amp;\\sqrt{\\dfrac{n}{n-p}}\\\\ \\end{array}\\) Standarized Residuals Pinheiro proposed to standardized residuals so they all have same variance \\(\\phi^H = \\dfrac{\\sum \\left(r_{wd}^H \\right)^2}{n}\\) \\(\\begin{array}{lllc} r_{wd}^H &amp;= r_{wd} &amp;\\times &amp;f_{wd}^H \\\\ &amp;= r_{wd} &amp;\\times &amp;\\sqrt{\\dfrac{1}{1-H_{ii}}} \\\\ \\end{array}\\) \\(H_{ii}\\) is the diagonal of the Hat Matrix Adjustment Factor: \\(H = X (X^TWX)^{-1}X^TW\\) The diagonal is labelled by going down the column of the triangle from left to right The denominator might actually be \\(n-2\\) but for the purpose of exam just use \\(n\\) as stated in the paper "],
["variations-to-the-odp-bootstrap.html", "9.7 Variations to the ODP Bootstrap", " 9.7 Variations to the ODP Bootstrap Use incurred loss triangle Convert ultimate from incurred to a payment stream based on paid analysis Correlate the simulations from the 2 method so that if we have large payment @ an older age, the incurred should be large as well Use BF Ult for recent years Have the a-priori varies based on some \\(\\sigma\\) Generalizing the ODP Model Reduce # of parameters Combine accident years Use just 1 development year since the column parameters is just a measure of decay and can be used for multiple columns Add calendar year trend parameter \\(\\gamma_k\\) for each column except for the first Must use GLM to determine the parameters "],
["practical-issues.html", "9.8 Practical Issues", " 9.8 Practical Issues 9.8.1 Negative Incremental Values How to deal with negative incremental values in the triangle 9.8.1.1 Model Fitting Method 1: \\(-ln(-q(w,d))\\) Works when there are a few cells with negatives Doesn’t work when the column sum to a negative value Method 2: Add a constant \\(\\Psi\\) Add \\(\\Psi\\) to every cell before running GLM then subtract \\(\\Psi\\) from each incremental loss once the means are calculated \\(ln[m_{wd} + \\Psi] = \\eta_{wd}\\) Can use this method combined with method 1 to take care of the extra large negative ones Method 3: Simplified GLM Use Chainladder with volume weighted average LDFs Only if the assumptions fit Need to make use the absolute value for the residual and re-sampling formula: \\(r_{wd} = \\dfrac{q(w,d)-m_{wd}}{\\sqrt{abs(m_{wd})}}\\) \\(q^*(w,d) = m_{wd} + r_p \\sqrt{abs(m_{wd})}\\) 9.8.1.2 Simulating Negative Values Adjustment to the Gamma Distribution Use \\(Gamma(-m_{wd}, -\\phi m_{wd}) + 2m_{wd}\\) Maintaining the right tail but also having the mean of \\(m_{wd}\\) Just doing \\(-Gamma(-m_{wd}, -\\phi m_{wd})\\) doesn’t work since the tail would be flipped Extreme Outcomes from Negative Values Column with negative mean can results in vary large LDFs, 4 options to deal with that: Remove the extreme iterations But beware of understating the the likelihood of extreme outcomes Recalibrate the Model Review data used and parameter selection (e.g. remove the first AY as it does not represent current behavior) Limit Incremental Losses Below 0 Either in the original triangle, sampled or simulated loss (process var step); Replace with 0 Can just do it in certain columns Understand Understand why this is happening. e.g. if due to S&amp;S then you can just model them separately and then correlated them during simulation 9.8.2 Non-Zero Sum of Residuals Since we assume \\(iid\\) and constant \\(\\sigma\\), the sum of residuals should be 0 Not necessarily the case since this is just a sample Consequence is that the simulated outcomes will be higher than the means 2 Options Keep it if we believe this to be characteristics of the data set Add a constant to each residual so that it sums to 0 and then sample from the adjusted residuals 9.8.3 Using N-year Weighted Average GLM Exclude the older diagonals so that we have \\(N+1\\) diagonals of data to get \\(N\\) diagonals of LDFs Will have less CY parameters Simplified GLM Get N-year weighted average Exclude the diagonals not used for the LDFs when determining the residuals Sample residuals for the entire triangle when sampling for bootstrap since you need cumulative losses for each row The 2 methods will results in different results GLM: Models the incremental losses in the trapezoid Simplified GLM: Model the same losses but in relation to the cumulative losses, which include the non-modeled losses in the diagonals excluded 9.8.4 Missing Value Solution Estimate from surrounding values Modify LDFs to exclude missing value Won’t have residual for this cell 9.8.5 Outliers Remove extreme values that are not representative of the variability of the losses Remove a whole row or, Just remove the value and treat them as missing values or, Exclude in LDFs but continue to use them when calculating residuals and use them for the re-sampling 9.8.6 Heteroskedasticity Non constant variance (bootstrap assumes residuals are \\(iid\\)) Stratified Sampling Split the triangle into groups with similar variance and only sample residuals that are in the group Cons: each group may not be that large Hetero-Adjustment Group the residuals then calculate the \\(\\sigma\\) of the residuals in each group and scale up Hetero-adjustment factor: \\(h^i\\) = the largest \\(\\sigma\\) \\(\\div\\) each group’s \\(\\sigma\\) \\(r_{wd}^{i,H} = r_{wd} \\times f_{wd}^H \\times h^i\\) Residual \\(\\times\\) Hat Matrix Factor \\(\\times\\) Hetero Factor Need to divide the sampled residual by \\(h^i\\) to reflect the variability of group \\(i\\) \\(q^{i*}(w,d) = m_{wd} + \\dfrac{r^{i*}}{h^i}\\sqrt{m_{wd}}\\) 9.8.7 Heteroecthesious Data Accident years have different level of exposures Partial First Development Period Only want partial accident year No impact to residuals for bootstrap 2 options: Reduce the mean of the incremental cells by pro ration in the process variance step Prorate after the process variance step Partial Last Calendar Period Latest diagonal is partial Simplified GLM Determine LDF excluding latest diagonal then interpolate LDFs for ultimate GLM Adjust the exposure in the last diagonal to make them consistent with the rest of the triangle (probably means adjusting annualizing the loss) Then prorate the losses similar to the first scenario 9.8.8 Exposure Adjustment Consider dividing the losses by the exposure in each AY if there are significant changes in exposure and model pure premium Multiply the PP results by the exposure after the process variance step 9.8.9 Parametric Bootstrapping Might not have enough data to sufficiently represent the tail Fit a distribution to the residual and sample from the distribution instead "],
["diagnostics.html", "9.9 Diagnostics", " 9.9 Diagnostics Judge the quality of the model Test Assumptions in model Gauge quality of model fit Guide the adjustments of model parameters 9.9.1 Residual Graphs Graph residuals vs CY, AY, Age, forecast loss Want to see random variability around zero 9.9.2 Normality Test Normality is not required, only need this if we’re doing parametric bootstrap with normal distribution Plot residuals against the normal best fit based on the percentiles Use p-value &gt; 5% as the test Or use something that penalize number of parameters \\(AIC = 2p + n \\left [ 1 + \\operatorname{ln}(2\\pi\\dfrac{RSS}{n})\\right]\\) \\(BIC = n \\operatorname{ln}\\left( \\dfrac{RSS}{n}\\right) + p \\operatorname{ln}(n)\\) \\(RSS\\) = actual residual - expected residual from normal 9.9.3 Outlier Remove true outliers but do not want to remove points that are realistic extreme scenarios Use box &amp; whisker plot Shows 25%ile to 75%ile Whiskers are 3 times the inter quartile range 9.9.4 Parameter Adjustments Test model with different sets of parameters Don’t need unique parameter for each row and column 9.9.5 Review Model Results Read summarized output by AYs Mean, s.e., CoV, Min, Max, Median The all year s.e. should be greater than any individual year The all year CoV should be less than the CoV for any individual year CoV should be highest for older years due small mean unpaid CoV also high for most recent AY due to higher volatility Larger parameter uncertainty or volatility from CL method Check min max for reasonability For the triangles: Check incremental means as well in triangle form Check s.d. of incremental values "],
["using-multiple-models.html", "9.10 Using Multiple Models", " 9.10 Using Multiple Models Use different methods (Paid/Inc’d Dev, BF, etc) by assigning weights by AYs Method 1: In the process variance step of bootstrap, use the same underlying U(0,1) to draw from each model then weight the models by the % Method 2: Run each model independently for each simulation (i.e. use different U(0,1)) then for each AY use the weights to randomly select one of the modeled results. Results will be a mixture of the various models Important to review the statistics in the above section for each output Fit the unpaid claim distribution to Normal, LogNormal, and Gamma. Then compare with the fit based on the actual residuals on various statistics Not sure what distribution this is talking about 9.10.1 Other Model Outputs Estimated Cash Flow Results Since bootstrap generates simulation for each cell in the bottom half of the triangle we can use this to get cash flow forecasts by CY and the percentiles as well Estimated Ultimate Loss Ratio Results We can estimate the variability of ultimate loss ratio since we vary and simulate the whole “square” Distribution Graphs Draw a distribution of the simulated unpaid in a histogram Can also smooth the histogram with Kernel density function For each point it takes a weighted average of the points around it; giving less weight to points further from it "],
["correlation.html", "9.11 Correlation", " 9.11 Correlation Correlate the loss distribution over several LoB Multivariate distribution requires the same underlying distribution which doesn’t work here for ODP Location Mapping When sampling the residuals, sample from the same place in the triangle for all the lines we want to correlate Disadvantages: Requires all LoB to have the same size triangle with no missing values or outliers Cannot stress the correlations among the LoBs Re-Sorting Use Iman-Conover algorithms or Copulas Advantages: Can accommodate different shapes and sizes Can make different correlation assumptions Can strengthen the correlation for extreme events (e.g. Copulas) Calculate correlation matrix using Spearman’s Rank Order Re-sorting based on the ranks of unpaid claims by AYs Using residuals to correlate LoBs (Both location mapping &amp; re-sorting) are both liable to create correlations close to zero Reserve Risk: Correlate total unpaid by correlating the incremental paid. May or may not be a reasonable approximation Pricing Risk: Correlate loss ratios over time Not as likely to be close to zero Use different correlation assumption than for reserve risk "],
["miscellaneous.html", "9.12 Miscellaneous", " 9.12 Miscellaneous Model Testing Based on testing from General Insurance Reserving Oversight Committee, the ODP Bootstrap with England &amp; Verrall residual out perform the Mack model by forecasting the 99%-ile better ODP losses only exceed 99%-ile ~3% of the time compare to Mack’s 8-13% Future Research Test ODP bootstrap on realistic data from CAS loss simulation model Expand ODP bootstrap with Munich Chainladder, claim counts and severity Research other risk analysis measures and use for ERM Use for SII requirements Research in correlation matrix (difficult to estimate) "],
["past-exam-questions-8.html", "9.13 Past Exam Questions", " 9.13 Past Exam Questions n/a -->"],
["a11-obtaining-predictive-distributions-for-reserves-which-incorporate-expert-opinions-r-verrall.html", "Chapter 10 A11 Obtaining Predictive Distributions for Reserves Which Incorporate Expert Opinions - R. Verrall ", " Chapter 10 A11 Obtaining Predictive Distributions for Reserves Which Incorporate Expert Opinions - R. Verrall "],
["cliffs-summary-5.html", "10.1 Cliff’s Summary", " 10.1 Cliff’s Summary Know how the model is defined with ODP and stochastic row parameters Know the Bayesian BF calculation and assumptions \\(\\operatorname{E}[c_{ij}] = Z_{ij} \\overbrace{[(\\lambda_j - 1) \\underbrace{D_{ij-1}}_{\\text{Actual up to }j-1}]}^{\\text{CL Results}} + (1- Z_{ij}) \\overbrace{[(\\lambda_j - 1) \\underbrace{M_i p_{j-1}}_{\\text{Expected up to }j-1}]}^{\\text{BF Results}}\\) \\(Z_{ij} = \\dfrac{p_{j-1}}{\\beta_i \\varphi + p_{j-1}}\\) Know the Stochastic column parameters calculation 10.1.1 Types of Exam Questions Haven’t done TIA practice questions Concepts 2012 #8: Explain model as trade off between standard CL and BF 2013 #9: Model specification stuff on variance of the prior, BF bayesian Bayesian vs Bootstrap: Provide expert opinion while maintaining the integrity of the variance estimates Bayesian vs Mack: Provides full distribution of unpaid losses and not just the first 2 moments 2014 #10: Expert opinions in LDFs or row parameters; \\(\\beta\\) impact on the bayesian model Full Calculation \\(\\star\\) 2012 #8: Bayesian model for BF method "],
["introduction-3.html", "10.2 Introduction", " 10.2 Introduction Uses Bayesian techniques to allow incorporation of expert opinion and also maintain the integrity of the prediction error Take into account prior knowledge in setting reserves Calculating prediction error using prior knowledge 10.2.1 Notation Claims \\(c_{ij}\\) Incremental claims for AY \\(i\\) and age \\(j\\) \\(D_{ij}\\) Cumulative claims = \\(\\sum_{k=1}^j c_{ij}\\) Row Parameters \\(x_i\\) Expected ultimate losses for AY \\(i\\) Column Parameters \\(y_j\\) Expected % reported in each period; \\(\\sum y_i = 1\\) \\(p_j\\) Expected reported to date \\(\\sum_{k=1}^j y_k\\) \\(\\lambda_j\\) Expected development from \\(D_{ij-1}\\) to \\(D_{ij}\\) (\\(= \\dfrac{p_j}{p_{j-1}}\\)?) \\(\\hat{\\lambda}_j\\) Weighted average LDFs \\(= \\dfrac{\\sum_i D_{ij}}{\\sum_i D_{ij-1}}\\) 10.2.2 Incremental loss distribution \\(c_{ij} \\sim ODP(x_i \\cdot y_j, \\varphi)\\) \\(\\mu = x_i \\cdot y_i\\) \\(\\sigma^2 = \\varphi \\cdot x_i \\cdot y_i\\) \\(c_{ij} \\sim NB(\\lambda_j D_{i, \\: j-1}, \\varphi \\lambda_j (\\lambda_j -1)D_{i, \\: j-1})\\) NB has the exact same results as the ODP so we can use them interchangeably Recall Mack method: \\(\\operatorname{E}[D_{ij}] = \\lambda_i D_{ij-1}\\) \\(\\operatorname{Var}[D_{ij}] = \\sigma^2 D_{ij-1}\\) "],
["model-specification.html", "10.3 Model Specification", " 10.3 Model Specification Steps: Assign prior distn to the column parameters Incremental Loss \\(\\sim NB\\) (or ODP) If vague priors (large variances) \\(\\Rightarrow\\) Bayesian chainladder Prediction error will be similar to CL or slightly larger If strong priors (small variances) \\(\\Rightarrow\\) We believe the prior means are appropriate Prediction error will decrease 10.3.1 Chain Ladder Intervention Adjust LDFs for a development period for the recent years only Vauge Priors Large variance \\(\\Rightarrow\\) Parameter based on data Based on recent years average Strong Priors Small variance \\(\\Rightarrow\\) Prior mean has greater influence 10.3.2 Last 3 Diagonals for LDFs Use the last 3 diagonals for forecasting to account for CY effects or inflation etc Pick large variance so data drive the parameter selection 10.3.3 BF Intervention Intervention on the level of each row \\(x_i\\) (a-priori) Vauge Priors Large variance, less strong prior information \\(\\Rightarrow\\) Results between BF and CL Lowers the prediction error but not as much Strong Priors Small variances, approximate the BF method Prediction error comes down due to low variance in the prior "],
["bayesian-model-for-bf-method.html", "10.4 Bayesian Model for BF Method", " 10.4 Bayesian Model for BF Method Assumptions: \\(c_{ij} \\sim ODP(x_i \\cdot y_j, \\varphi)\\) \\(x_i \\sim \\Gamma(\\alpha_i, \\beta_i)\\) \\(\\operatorname{E}[x_i] = \\dfrac{\\alpha_i}{\\beta_i} = M_i\\) \\(\\operatorname{Var}[x_i] = \\dfrac{\\alpha_i}{\\beta_i^2}\\) Gamma because the numerical procedures work well (LogNormal is good too) Mean and variance of \\(x_i\\) is selected from outside the model and back out the parameters \\(\\beta_i = \\dfrac{\\operatorname{E}[x_i]}{\\operatorname{Var}[x_i]}\\) \\(\\alpha_i = \\dfrac{(\\operatorname{E}[x_i])^2}{\\operatorname{Var}[x_i]}\\) Model Results: Memorize Formula \\(\\operatorname{E}[c_{ij}] = Z_{ij} \\overbrace{[(\\lambda_j - 1) \\underbrace{D_{ij-1}}_{\\text{Actual up to }j-1}]}^{\\text{CL Results}} + (1- Z_{ij}) \\overbrace{[(\\lambda_j - 1) \\underbrace{M_i p_{j-1}}_{\\text{Expected up to }j-1}]}^{\\text{BF Results}}\\) \\(Z_{ij} = \\dfrac{p_{j-1}}{\\beta_i \\varphi + p_{j-1}}\\) Large \\(\\beta\\) means small variance for the prior \\(\\Rightarrow\\) smaller \\(Z_{ij}\\) \\(\\Rightarrow\\) less credibility on the actual loss \\(\\varphi\\), which represents process variance also has impact on the credibility Larger \\(p_{j-1}\\) means more developed losses \\(\\Rightarrow\\) more credibility to actual losses Notes: Benktander with different weights This is applied to incremental loss in the triangle \\(\\varphi\\) will be given "],
["stochastic-column-parameters.html", "10.5 Stochastic Column Parameters", " 10.5 Stochastic Column Parameters BF above uses stochastic row parameters and deterministic column parameters We can use stochastic process for both by first estimating the column parameters then the row parameters For columns, prior distn is gamma with wide variance \\(\\operatorname{E}[c_{ij}] = (\\gamma_i - 1) \\sum \\limits_{m=1}^{i-1} c_{m,j}\\) \\(\\sum \\downarrow\\) \\(\\gamma_i\\) is the new row parameters This tells you the level of losses in the row relative to the rows above Similar to LDFs but just looking at it at the different angle 10.5.1 Calculate Gamma Know how to calculate this pictorially Need to first use CL method to get ultimate loss and % unpaid by AY \\(\\gamma_1 = 1.000\\) always First calculate LDFs and calculate % unpaid and a-priori for each AY \\(U_i\\) = a-priori ultimate based on LDF for AY \\(i\\) \\(q_i\\) = % unpaid for AY \\(i\\) Note that in step 4 above if you don’t need the individual cells you can just take the \\(U_3 q_3\\) "],
["past-exam-questions-9.html", "10.6 Past Exam Questions", " 10.6 Past Exam Questions -->"],
["a14-stochastic-loss-reserving-using-bayesian-mcmc-models-g-meyer.html", "Chapter 11 A14 Stochastic Loss Reserving Using Bayesian MCMC Models - G. Meyer ", " Chapter 11 A14 Stochastic Loss Reserving Using Bayesian MCMC Models - G. Meyer "],
["cliffs-summary-6.html", "11.1 Cliff’s Summary", " 11.1 Cliff’s Summary Know how to read KS-test, \\(p-p\\) plot and Freq vs Count plot Know the form of all the methods in the model 11.1.1 Types of Exam Questions n/a "],
["introduction-synopsis.html", "11.2 Introduction &amp; Synopsis", " 11.2 Introduction &amp; Synopsis Estimate from Mack and ODP do not have enough variability Incurred losses with variable row parameters and AY correlation \\(\\rho\\) is sufficient Paid losses requires variable row parameters and change in settlement rate parameter \\(\\gamma\\) Two reasons model doesn’t fit: Insurance loss environment has experienced changes that are not observable at the valuation date Other models can validate If we review many models and none of them validate it gives 1) credence but does not confirm "],
["data-set.html", "11.3 Data Set", " 11.3 Data Set 200 triangles from Sch P 1997 and reviewed 10 years later Paid and incurred 50 triangles from 4 LoB (Commercial Auto, Personal Auto, WC, Other Liab) Potential pitfalls in sample triangle selections Insurer with significant changes to their books over the exposure period would violate the assumptions of the model and should be excluded Solution: Use consisteny of NEP and Net:Gross Premium to establish stability in book Use CoV to establish consistency Pick triangles with CoV below a threshold Need to avoid selecting datasets that best suit the model e.g. removing “outliers” from the data Solution: Choose the company in an automated and well defined manner Losses are considered fully developed at 10 years so in practice the paid and incurred ultimate is slightly different "],
["testing-procedure.html", "11.4 Testing Procedure", " 11.4 Testing Procedure We are testing the procedure of each method and not the results of any one distribution generated from the method Generate 50 sets of distribution and determine the actual percentile based on the predicted distribution The distribution of the 50 predicted percentiles (\\(p_i\\)) should follow a uniform distribution if the model is accurate Expected percentiles \\(e_i\\) run from \\(\\frac{1}{n+1}\\) to \\(\\frac{n}{n+1}\\) 11.4.1 Kolmogorov-Smirnov Test \\(H_0\\): Distribution of \\(p_i\\) is uniform \\(D = \\operatorname{max} \\limits_i \\mid p_i - e_i \\mid\\) Maximum difference between the predicted and expected percentiles Reject \\(H_0\\) if \\(D &gt; \\dfrac{136}{\\sqrt{n}}\\%\\) @5% confidence level e.g. for \\(n = 50\\): 19.2%; \\(n=200\\): 9.6% Anderson-Darling focuses on the tail and failed all the models therefore we do not use it as it does not help in model comparison 11.4.2 p-p Plot We plot the \\(p-p\\) plot with \\(e_i\\) vs \\(p_i\\) to diagnosis Light blueline is the critical value for a given \\(n\\) Model is too light tailed: Shallow slope near corner and steep in the middle Model is too heavy tailed: Steep slope near corner and shallow in the middle Model is biased upwards: Bow down 11.4.3 frequency vs percentile Blue line is based on the uniform \\(e_i\\) set at \\(\\frac{n}{10}\\) for the 10 deciles "],
["models-comparison-summary.html", "11.5 Models Comparison Summary", " 11.5 Models Comparison Summary Mack is the only one that does not have a base form of \\(\\mu_{wd} = \\alpha_w + \\beta_d\\) ODP is the England &amp; Verall Bootstrap 11.5.1 Mack Model Variance is the product of mean in the cell and a constant that varies by column \\(\\operatorname{Var}\\left (c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ) = \\alpha_k^2 \\: c_{i,k}\\) Incurred Light on both tail \\(\\Rightarrow\\) Does not have enough variability in it’s predicted distribution Paid Similar to ODP, biased high on personal auto and light left tail on WC 11.5.2 ODP Bootstrap EV ODP forecasts log incremental losses \\(\\Rightarrow\\) Only suitable for paid losses Can handle occasional negative losses as long as the \\(\\sum\\) column is positive Same procedure as Shapland Leong paper Overall shows biased high "],
["bayesian-models-cumulative.html", "11.6 Bayesian Models (Cumulative)", " 11.6 Bayesian Models (Cumulative) Inputs Prior distribution is needed for each parameters Wide priors (diffuse) Narrow priors: Use expert knowledge in selecting mean and variance of the parameters Parameters: \\(\\alpha_w\\): row parameters \\(\\beta_d\\): column parameters \\(\\sigma_d\\): variance parameters (mostly constant across columns) \\(\\tau\\): trend \\(\\gamma\\): change in closure rate Data: Paid or incurred Triangle Output The posterior distribution of the parameters is expressed as simulated outputs (not closed form distribution) 11.6.1 Leveled Chain Ladder LCL with incurred data Multiplicative model (based on additive exponential) Each cell is \\(e^{\\mu_{wd}} = e^{\\alpha_w}e^{\\beta_d}\\) Log mean of each cell is \\(\\mu_{wd} = \\alpha_w + \\beta_d\\) \\(\\beta_{10} = 0\\) so we have 100% at 10 years \\(\\beta_d &lt; 0\\) most of the time, represents % paid to date Uses cumulative data \\(C_{wd}\\) Same variance parameter (\\(\\sigma_d\\)) for each column of cumulative loss \\(\\hookrightarrow\\) Highest variability @ early ages \\(\\sigma_1 &gt; \\sigma_2 &gt; \\cdots &gt; \\sigma_{10}\\) Variance varies by column only (not by AYs) \\(\\alpha_w\\) is a random variable Not value on the diagonal (incurred to date) Model select an \\(\\alpha_w\\) for each instance of the simulation based on wide priors Main feature of this model for adding variability Can compare the variability (s.d.) with Mack by plotting the log(s.d.) of the 2 models Model still does not capture the tail appropriately 11.6.2 Correlated Chain-Ladder Build upon the Leveled Chain-Ladder by adding \\(\\rho\\) to create correlation of losses in one AY and the previous AY Uses cumulative data \\(C_{wd}\\) \\(\\mu_{wd} = \\alpha_w + \\beta_p + \\rho \\cdot \\left[ \\operatorname{ln}\\left(C_{w-1, d}\\right) - \\mu_{w-1,d} \\right]\\) Higher losses in one row \\(\\rightarrow\\) higher expected losses in the following row Prior is still wide priors The correlation here is what drives the additional variability Incurred Results and k-s test show that this model is sufficient Paid Worst than ODP and Mack, biased high for all lines 11.6.3 Changing Settlement Rate Based on LCL with \\(\\gamma\\) that allows for speed up in claim payments Uses cumulative losses Logmean for each cell: \\(\\mu_{wd} = \\alpha_w + \\left[ \\beta_d \\cdot (1-\\gamma)^{w-1}\\right]\\) \\(\\gamma &gt;0\\) reflects increase in payment speed as \\((1-\\gamma)^{w-1} &lt; 1\\) \\(\\gamma\\) has less impact further out in the tail as there are less payments happening out there Model fits one \\(\\gamma\\) for the whole triangle Results Overall fits well, slightly biased high on Personal Auto but is a big improvement over the other models "],
["skewed-distribution.html", "11.7 Skewed Distribution", " 11.7 Skewed Distribution Use incremental data as trends act on incremental loss, which has the following properties: Skewed right Occasionally negative 11.7.1 Skewed Normal Distribution Blend of normal and truncated normal \\(X = \\mu + (\\omega \\cdot Z) \\cdot \\delta + (\\omega \\cdot \\varepsilon) \\cdot \\sqrt{1 - \\delta^2}\\) \\(\\varepsilon \\sim Normal(0,1)\\) \\(Z \\sim Truncated \\: Normal_{[0,\\infty]} (0,1)\\) \\(\\delta\\) is the weight between \\(\\varepsilon\\) and \\(Z\\) \\(\\omega\\) is the standard deviation Skewness = 0.995; Not used by the Meyers as it is not skewed enough 11.7.2 Mixed Lognormal-Normal \\(X \\sim Normal(Z,\\delta)\\) \\(Z \\sim Lognormal(\\mu,\\sigma)\\) Mixed \\(ln - n\\) Distribution This can create distribution more skew than the skewed normal and can also have negative values "],
["bayesian-models-incremental.html", "11.8 Bayesian Models (Incremental)", " 11.8 Bayesian Models (Incremental) Model applies trend and therefore uses incremental data Correlated Incremental Trend Model Changing Settlement Rate Model 11.8.1 Correlated Incremental Trend Single CY trend parameter \\(\\tau\\) Mixed lognormal-normal distribution Include correlation between AY similar to CCL method Steps for the method: Uncorrelated log mean of each cell with CY trend \\(\\mu_{wd} = \\alpha_w + \\beta_d + \\tau \\cdot(w+d-1)\\) Draw \\(Z_{wd} \\sim Lognormal(\\mu_{wd},\\sigma_d)\\) \\(\\sigma_1 &gt; \\sigma_2 &gt; \\cdots &gt; \\sigma_{10}\\) Smaller less volatile claims should be settled early \\(\\tilde{I}_{wd} \\sim Normal(Z_{wd},\\delta)\\) Add correlation between AYs for rows after the first \\(\\tilde{I}_{wd} \\sim Normal(Z_{wd} + \\rho \\cdot (\\tilde{I}_{w-1,d} - Z_{w-1,d})\\cdot e^{\\tau},\\delta)\\) Parameters restrictions \\(\\tau\\): Prior \\(\\sim Normal(0,3.2%)\\) Without restriction it was forecasting very negative trend which is offset by higher \\(\\alpha\\) and \\(\\beta\\) \\(\\sigma_d\\): Prior \\(\\sigma_1 \\sim Uniform(0,0.5)\\) Prior \\(\\sigma_d \\sim Uniform(\\sigma^2_{d-1},\\sigma^2_{d-1} +0.1)\\) Limit the speed \\(\\sigma_d\\) can increase, very high \\(\\sigma_d\\) can lead to unreasonably high simulate results Results Losses not much smaller than CCL while we would like it to be much smaller as CCL was biased high \\(\\rho\\) is lower than from CCL Strong negative correlation between trend \\(\\tau\\) and level parameters \\(\\alpha_w + \\beta_d\\) With small data set it is hard for the model to distinguish the AY level + development vs trend Model showed no improvement over Mack or ODP 11.8.2 Leveled Incremental Trend Same as CIT but with \\(\\rho = 0\\) Results similar to CIT with lower standard deviation "],
["process-parameter-and-model-risk.html", "11.9 Process, Parameter, and Model Risk", " 11.9 Process, Parameter, and Model Risk \\(\\underbrace{\\text{Variance}}_{\\operatorname{Var}(X)} = \\underbrace{\\operatorname{E}[\\text{Process Variance}]}_{\\operatorname{E}_{\\theta}[\\operatorname{Var}[X|\\theta]]}+\\underbrace{\\operatorname{Var}[\\text{Hypothetical Mean}]}_{\\operatorname{Var}_{\\theta}[\\operatorname{E}[X|\\theta]]}\\) Typically the parameter risk is much larger than process risk Model risk is the risk of not selecting the right model For known unknown, weight average of multiple models If the weight vary a lot in the posterior distribution than this could be an indication of model risk This turns into more or less parameter risk Should focus on the total risk "],
["conclusion.html", "11.10 Conclusion", " 11.10 Conclusion Goal of the paper was to test the predictive accuracy of various models, both mean and distribution of outcomes Not on the reserve estimate for individual insurers 11.10.1 Results Summary Incurred Data Mack understates variability as it assumes AYs are independent CCL introduces AY correlation and does relatively well Paid Data Mack and ODP were biased high as well as CCL There were change in environment that is not captured Calendar year trend: LIT and CIT still biased high CSR: significantly less bias than LIT and CIT (except for PA still failed) Mack and ODP did better than CCL, LIT and CIT 11.10.2 Final Comments Results were for specific annual statement year 1997 Possible the speed up was specific to the period \\(\\Rightarrow\\) CSR could potentially useless for another year Could use more narrow prior to incorporate knowledge of insurer’s business operation and obtain superior results "],
["past-exam-questions-10.html", "11.11 Past Exam Questions", " 11.11 Past Exam Questions n/a -->"],
["a9-a-framework-for-assessing-risk-margins-k-marshall-et-al-.html", "Chapter 12 A9 A Framework for Assessing Risk Margins - K. Marshall et al. ", " Chapter 12 A9 A Framework for Assessing Risk Margins - K. Marshall et al. "],
["cliffs-summary-7.html", "12.1 Cliff’s Summary", " 12.1 Cliff’s Summary This had some pretty good past exam questions, need to know the qualitative items quite well Know why we need qualitative analysis Independent Parameter and process variance model with stochastic model Internal Systemic Know the 3 components Score against best practice and calibrate to CoV Know how to score given actual examples as in past exam Hindsight analysis Extnernal Systemic Know the different risk categories And what lines they impact most Use benchmark similar to internal but select CoV directly Correlation Risk sources are independent of each other Independent: assume independence across lines, weight by liabilities Internal: base on correlation matrix \\(\\Sigma\\), again weighted by liabilities External: correlation between each valuation group and risk categories \\(\\Rightarrow\\) then roll up to the risk categories and assume they are independent of each other Risk Margin \\(\\text{Risk Margin} = \\underbrace{\\mu}_{\\text{Expected Loss}} \\times \\underbrace{\\phi }_{\\text{CoV}} \\times \\underbrace{Z_{\\alpha}}_{0.67\\text{ for the }75^{th}\\text{ percentile}}\\) Addiation analysis Sensitivity, scenario testing Internal benchmarking, important to know the relationships and consistency, been heavily tested in the past External benchmarking Hindsight and mechanical hindsight Regularity of review 12.1.1 Types of Exam Questions Haven’t done TIA practice questions Concepts \\(\\star\\) 2013 #8 b f g h: Correlations, internal benchmarking \\(\\star\\) 2014 #8: internal systemic risk 3 components; risk indicator for each LoB for each risk component; Score the risk indicators \\(\\star\\) 2014 #11: External systemic risk and which lines impact the most \\(\\star\\) 2015 #8: Internal source, external source \\(\\star\\) 2015 #9: Internal benchmark Calculations \\(\\star\\) 2013 #8 a c d e: Correlations, risk margins 2015 #8 d: Independent CoV "],
["introduction-4.html", "12.2 Introduction", " 12.2 Introduction Estimate risk margin for unpaid losses and premium liabilities Risk margin = 75th percentile - mean (an Australian regulatory standard) Based on CoV Need framework because quantitative is not enough, need both quantitative and qualitative measures to examine the uncertainty Quantitative analysis requires lots of data Only captures historical risk Does not capture risk that did not have an episode (of systemic risk) in the experience period Portfolio for the analysis should be split into reasonably homogeneous groups Might combine or further split groups from what was used for liability valuation Balance the practical benefits from large groups vs insight gained from smaller groups Might want to further split if a LoB has a portion that has significantly more uncertainty than another part (e.g. HO CAT/non CAT) "],
["three-sources-of-uncertainty.html", "12.3 Three Sources of Uncertainty", " 12.3 Three Sources of Uncertainty \\(\\phi^2 = \\underbrace{\\phi_{indep}^2}_{\\text{Independent Risk}} + \\underbrace{\\phi_{internal}^2}_{\\text{Internal Systemic Risk}} + \\underbrace{\\phi_{external}^2}_{\\text{External Systemic Risk}}\\) \\(\\phi =\\) CoV Systemic Risk: Risks that can vary across valuation classes Qualitative approaches is recommended for systemic risk Need to make assumptions about the correlation of the different risks Most quantitative methods are effective for independent risk and past episodes of external systemic risk. But not very effective with internal systemic risk and external systemic risk that has not yet occurred Good stochastic model will fit away past systemic episodes (e.g. high inflation) while we still want to hold a margin for the future Outcome dependent significantly on actual episodes of risk, not all potential ones (for systemic risk that was not fitted away) Need to judge if episodes in data are representative going forward Model is unlikely to pick up internal systemic risk from the actuarial valuation process 12.3.1 Independent Risk Randomness inherent to the insurance process Parameter Risk: ability to select correct parameters and appropriate model Process Risk: randomness e.g. tossing a die Use stochastic modeling to analyze independent risk Focus on periods where episodes of systemic risk were non-existent or minimal so to reduce the impact of the historical external episodes and allow us to focus on the independent risk Supplement with internal and external bench marking 12.3.1.1 CoV for Independent Risk Model the parameter and process risk together Mack, bootstrap, stochastic CL, GLM, Bayesian Ideally the model adequately model away the systemic risk so all that is left is the independent risk Residual should only have independent risk Complexity of the model should be commensurate with the importance of the total risk margin For small data set, difficult to model away the external systemic risk \\(\\Rightarrow\\) Use as starting point and then add a margin for external systemic risks not in data Models for outstanding claims liabilities: GLM and bootstrapping are particularly useful as they can isolate the independent risks; graphing residuals again AY, age, experience period can used to identify past systemic episodes Models for premium liabilities: GLM, bootstrap, Bayesian Can model frequency and severity CoV then combine Remove past systemic episodes from the frequency Adjust for inflation and seasonality for severity 12.3.2 Internal Systemic Risk Uncertainty arising from the liability valuation process/actuarial valuation models. E.g. process the actuary goes through to estimate the liabilities This can be fro sources anywhere along the chain of valuation: Data record/collection/organization (e.g. not collecting the right data; not using the right data) \\(\\Rightarrow\\) Analysis, AJ \\(\\Rightarrow\\) reserve selection (e.g. management overrides actuary’s opinion) 3 components of internal systemic risk: Specification Error: From not perfectly modeling the insurance process because it’s too complicated or just don’t have the data Parameter Selection Error: Trend is particularly difficult to measure Data Error: Lack of data, lack of knowledge of the underlying pricing, u/w, and claim management process, inadequate knowledge of portfolio 12.3.2.1 CoV for Internal Systemic Risk Use bench marking technique: Need to define risk indicators, score them against best practice, map the scores to a CoV Score Against Best Practice For each valuation group, assign a score (1-5) for each risk indicator Specification Error: # of independent models; range of results from model; ability to detect trends Parameter Selection Error: Identify best predictors; stability of best predictors (or responsive to process change); Predictors used are close to best predictors Data Error: Knowledge of past processes affecting predictors; Extent, timeliness, consistency, and reliability of information from the business; data is reconciled and has quality control; freq and sev of past misestimation due to revision of data Assign weight for each risk indicator (can vary by valuation group) Average the scores using the selected weights We score the 3 components for each valuation group (OCL and PL) and then roll up the score and assign the average grade for each valuation group to a CoV Calibrate Score to CoV Significant amount of judgement supplement by quantitative analysis CoV \\(\\in [5\\%, 20\\%]\\) Analysis of past model performance should aid in estimating the potential variability Hindsight Analysis: Compare valuation of liabilities at prior point in time to the current view, to gain insight into how a better model can reduce volatility Mechanical Hindsight: Mechanically do various ex post analysis, and see how prediction error can be reduced; e.g. do a detailed vs crude and see the difference Additional comments: Improvement from poor to fair is greater than from fair to good Longer tail line will have higher CoV due to difficulty in estimating the parameters Lager liability will have smaller CoV when all else being equal OCL and OL might not necessarily be on the same scale PL may have additional uncertainty as it’s for future business For short tail lines ELR might be sufficient for PL but might not be the best practice for OCL Can always just add a load on top if justified 12.3.3 External Systemic Risk Systemic risk that are not internal We don’t want to only consider actual episodes of systemic risk in the data set List of risk categories to consider: Economic and Social Risks: Inflation, social trends Legislative, Political Risks, Claims Inflation Risks: Change in law, frequency of settlement vs suits to completion, loss trend (Long tail lines) Claim Management Process Change Risk: Change in process of managing claims e.g. case reserve practice Expense Risk: Cost of managing a run-off book Event Risk: natural or man-made CAT (Premium liabilities for property) Latent Claim Risk: Claim from source not currently considered to be covered Recovery Risk: Recoveries from reinsurers or non-reinsurers A handful of these risk categories will dominate the uncertainty for that valuation group Useful to rank the risk categories in order of impact on the uncertainty of a valuation group Lots of the above should be something the valuation actuary already discussed with the business and with claims 12.3.3.1 CoV for External Systemic Risk Use bench marking technique similar to internal systemic risk Straight up select the CoV, rank the risk in order of importance to help with the selection Quantitative approach can provide in insight but we need to also consider possible future sources of external systemic risk Consider risk that affect u/w and risk selection, claims management, expense management, economic/legl environment Economic and Social Risks: Inflation; unemployment; GDP growth; interest rates; driving patterns For inflation we are concern with the systemic shifts not just randomness (randomness is in the indepedent risk) Legislative, Political Risks, Claims Inflation Risks All grouped together since each catergory needs to be uncorrelated with each other Short tail lines: can impact premium if there are sudden shifts in law of inflation Long tail lines: Current or potential changes to law; medical technology costs; legal costs Claim Management Process Change Risk: Reporting patterns; payment patterns; reopen rates More important to OCL, only impact PL when a change in process change the cost level of claims Expense Risk: Claim handling expense and policy maintenance expense CoV should be small Event Risk: Mostly premium risk Can model from experience, CAT modeling or input from reinsurers Latent Claim Risk: Unlikely for most LoB but can be severe Due to low probability, likely not worth to commit substantial resources to estimate the risk Recovery Risk: S&amp;S for non reinsurer Reinsurers, consider reinsurance contracts in place specifically for reinsurers where a large amount of premium is ceded "],
["correlation-aggregating-the-cov.html", "12.4 Correlation (Aggregating the CoV)", " 12.4 Correlation (Aggregating the CoV) Overall Assumes the 3 main pieces are independent of each other \\(\\phi = \\sqrt{\\phi_{indep}^2 + \\phi_{internal}^2 + \\phi_{external}^2}\\) Quantitative method to measure correlation can be valuable, however: Complexity maybe outweigh the benefit Results heavily influenced by past correlations while future correlation may differ Difficult to separate past episodes of independent risk and systemic risk Internal systemic risk cannot be modeled using standard correlation modeling techniques Likely won’t aligned with our definitions of independent, internal/external systemic risks Practical guidance: Just bucket into 0%, 25%, 50%, 75%, 100%, any finer will likely lead to spurious accuracy Introduce dummy variables and see their impact in each risk/valuation group pair (inflation, unemployment, propensity to suit, freq of CAT, fraud) 12.4.1 Independent Risk Cov Correlation Assume independence across liabilities, where \\(i\\) is the different valuation groups \\(\\phi_{indep}^2 = \\sum_i (\\phi_i w_i)^2 = (\\vec{\\phi w})(\\vec{\\phi w})^T\\) \\(w_i\\) can be just claims liabilities or total liabilities (including premium liabilities) 12.4.2 Internal Systemic Risk Correlation \\(\\phi_{internal}^2 = (\\vec{\\phi w}) \\times \\Sigma \\times (\\vec{\\phi w})^T\\) \\(\\Sigma\\) is the correlation matrix Again the \\(\\vec{w}\\) is the % of total liabilities 12.4.3 External Systemic Risk We measure the CoV for each risk category for each valuation group \\(\\Sigma_c\\) is the correlation matrix between valuation groups for each risk category \\(c\\) For a given risk category \\(c\\), the CoV is: \\(\\phi_{external, c}^2 = (\\vec{\\phi_{c} w}) \\times \\Sigma_c \\times (\\vec{\\phi_{c} w})^T\\) Then assume independence between risk categories: \\(\\phi_{external}^2 = \\sum \\limits_{c \\in risk \\: category} \\phi_{external, c}^2\\) Important to pick risk categories that are likely to be independent of each other "],
["risk-margin.html", "12.5 Risk Margin", " 12.5 Risk Margin \\(\\text{Risk Margin} = \\underbrace{\\mu}_{\\text{Expected Loss}} \\times \\underbrace{\\phi }_{\\text{CoV}} \\times \\underbrace{Z_{\\alpha}}_{0.67\\text{ for the }75^{th}\\text{ percentile}}\\) \\(\\alpha\\) %-ile = \\(\\mu(1+\\phi Z_{\\alpha})\\) "],
["additional-analysis.html", "12.6 Additional Analysis", " 12.6 Additional Analysis Sensitivity Testing: By varying the CoV and correlations Scenario Testing: Consider what assumptions need to change in our mid point to eat up the risk margin Internal Benchmarking: Compare CoVs within between OCL and PL and also with other valuation groups Independent risk: Large liability will have smaller CoV due to law of large numbers Short tail line will have a small CoV as well due to less volatility Therefore: Outstanding Claims Liability: \\(\\phi_{short \\: tail} &lt; \\phi_{long \\: tail} &lt; \\phi_{long \\: tail, \\: small \\: book}\\) Premium Liability - Long Tail: OCL &gt; PL \\(\\Rightarrow\\) \\(\\phi_{OCL} &lt; \\phi_{PL}\\) Because there are many AYs of claims in the reserves Premium Liability - Short Tail: OCL &lt; PL \\(\\Rightarrow\\) \\(\\phi_{OCL} &gt; \\phi_{PL}\\) Because OCL is small LoB with significant event risk will have different risk profiles for the PL and OCL Internal Systemic: If the methods to estimate liabilities is similar across valuation groups, we would expect the CoV to be similar for classes that have similar claim payment patterns External Systemic: Main sources are higher for long tail lines; Event risk is higher for property; Liability for HO can also be significant External Benchmarking: Compare selected CoV with external sources Such as APRA 2008 Use just as a sanity check, not appropriate to simply take the risk margins from benchmarks Independent risk CoV depend on the size of liabilities \\(\\Rightarrow\\) Similar sized liability today would represent a small book due to inflation \\(\\Rightarrow\\) Adjust CoV upward Estimate PL with a scale up factor from OCL Hindsight Analysis: Compare the actual results with expected Blend of all 3 risks Useful as a guide if the CoVs are reasonable Mechanical Hindsight: Value the liabilities today using a mechanical method and repeat with information at older evaluation date Measure independent risk over stable periods By using multiple methods you can measure the internal systemic risk By measuring over long periods of time, you can measure risk from all sources "],
["documentation-and-regularity.html", "12.7 Documentation and Regularity", " 12.7 Documentation and Regularity Full study to support CoV should be done every 3 years Key assumptions should be examined in the interim: Emerging trends Emerging systemic risks Change in valuation methods Should consider applying the key steps to any new portfolio "],
["past-exam-questions-11.html", "12.8 Past Exam Questions", " 12.8 Past Exam Questions 2013 #8 2014 #8 2014 #11 2015 #8 2015 #9 -->"],
["reinsurance-loss-reserving-patrik.html", "Chapter 13 Reinsurance Loss Reserving - Patrik", " Chapter 13 Reinsurance Loss Reserving - Patrik Know the 7 problems of reinsurance reserving Longer claims report lag Persistent upward development of most claims reserves Reporting pattern differ greatly Industry statistics not useful Reports received lack important information Data coding and IT system problems Reserve to surplus ratio is higher for reinsurer General theme: Heterogeneity, low frequency &amp; high severity, lack of detailed (exposure &amp; claims) information, longer lag Know the components of reinsurance reserve 4 steps of setting reinsurance reserve: Partition \\(\\Rightarrow\\) Development Patterns \\(\\Rightarrow\\) Estimate \\(\\Rightarrow\\) Monitor and AvE Partition priority Considerations for short vs medium vs long tail lines Know SB well and how to adjust the premium Know the credibility weighted method AvE "],
["prob-of-re.html", "13.1 7 Problems with Reinsurance Reserving", " 13.1 7 Problems with Reinsurance Reserving Problem 1: Longer claims report lag Claim must be perceived as report-able to the reinsurer by the cedant (e.g. half of the attachment point) and then flow through cedent’s system to reinsurer Cedant’s reporting system \\(\\Rightarrow\\) Cedant’s reinsurance accounting \\(\\Rightarrow\\) Intermediary (broker) \\(\\Rightarrow\\) Reinsurer books the claims \\(\\Rightarrow\\) Reinsurer’s claim system Cedant may undervalue the claim for a long time and thus not reported to the reinsurer See Problem 2 on the cedant reserving to the modal value Extreme delays in discovery or reporting for mass tort claims (e.g. asbestos) Problem 2: Persistent Upward Development of Most Claim Reserves Economic and social inflation Tendency to underestimate LAE Tendency of claims to reserve at the modal value Not initially clear which of a group of similar claims will become large so choose the same mostly likely value for all Modal value likely under attachment point of treaty so reinsurer does not learn about claim until it is known to be large, which may be several years after reported to primary insurer Problem 3: Reporting Pattern Differ Greatly Pattern differs by: LoB, contract type, contract terms, cedant, and intermediary Extremely heterogeneous exposures Extreme fluctuation in historical loss data due to low frequency and report lags Less information on the underlying exposure than primary carrier Problem 4: Industry Statistics Not Useful Due to heterogeneity exposures Sch P doesn’t break down the exposure fine enough and ISO not directly applicable Reporting lag grows with attachment point Problem 5: Reports Received Lack Important Information Proportional covers require only summary claims info Might only have UY or CY info (no AY or PY) Limitation of reinsurance premium by primary LoB as an exposure base Reinsurance premium is assigned to line according to a % breakdown estimate made at the beginning of the contract and based upon the distribution of subject premium by line To the degree that these % do not accurately reflect the reinsurer’s loss exposure by primary line, comparisons of premiums and losses by line maybe be distorted Mostly a problem for multi-line proportional treaties Reporting is typically done with a quarter lag so always missing a quarter of premium Problem 6: Data Coding and IT System Problems Again due to the heterogeneity in coverage Might have grown in size and complexity faster than data system could handle Problem 7: Reserve to Surplus Ratio is Higher for Reinsurer More of a management problem Management may underestimate level of reserves need (due to problem 1-6) until claims emerge. Difficulty with actuary convincing management to post the appropriate reserve Standard techniques requires: Homogeneous book, sufficient frequency, and detailed exposure info (Problems 3, 4, 5); Difficult to supplement with industry info Side note on tax impact, reinsurer liabilities are much longer tail than primary and there for the discounted liabilities are smaller and will incur greater income tax "],
["components-to-a-reinsurance-loss-reserve.html", "13.2 6 Components to a Reinsurance Loss Reserve", " 13.2 6 Components to a Reinsurance Loss Reserve 6 components of a reinsurer’s loss reserve Case reserve reported by cedent Can individual case reports (XS contracts) or reported in bulk (proportional contracts) Additional Case reserve from reinsurer (ACR) From reinsurer’s claim department’s review of individual case reserve reports May vary considerably by contract and cedant IBNER Pure IBNR Discount for future investment income For tabular discount as well as for tax purposes Tabular discount is on (WC permanent total, auto PIP annuity claims and medical PL) Risk Load For adverse deviation so uncertain income doesn’t flow into profits too quickly Can be implicitly built into the reserving assumptions or explicitly "],
["reinsurance-reserving-procedure.html", "13.3 Reinsurance Reserving Procedure", " 13.3 Reinsurance Reserving Procedure Partition \\(\\Rightarrow\\) Development Patterns \\(\\Rightarrow\\) Estimate \\(\\Rightarrow\\) Monitor and AvE Partition into homogeneous exposure groups that are consistent over time with respect to mix of business Analyze historical development; Ideally, consider case and emergence of IBNR claims separately Estimate future development; Ideally, estimate IBNER and pure IBNR separately Monitor and testing predictions (AvE) 13.3.1 Step (1): Portfolio Partition Data needs to be split into reasonably homogenous exposure groups that are relatively consistent over time with respect to mix of business (exposures) Group policies with similar report lags and development profile Variables to consider when partitioning data in approximately priority order: LoB Contract type (fac, treaty, finite) Types of Cover (QS, SS, XS per risk/occ, agg XS, CAT, LPT) Primary LoB (for cas) Attachment Point (for cas) Contract terms (flat-rated, retro rated, sunset clause, LAE share, CM, Occ) Type of cedant (Small, large, E&amp;S) Intermediary (brokers) Figure 13.1: Portfolio Partition Example: Start with the LoB or some other major categories Further breakdown into fac or treaty All significant XS exposure should be further broken down by type of retention (e.g. per-occ XS vs agg XS) Treaty casualty XS should be further broken down by attachment point range and primary LoB (e.g. AL, GL, PL, WC, etc) Treaty casualty proportional should be broken down by first dollar primary layers (group-up) vs higher excess layer (excess) Do we also breakdown by primary line? Facultative casualty: breakdown by automatic primary programs (pro rata share of group-up exposure) vs automatic nonprimary programs (excess) Certificate exposure should ideally be split by attachment point range Or at least buffer vs umbrella layer and then further by primary line For property and other exposure: split by pricing categories Need to balance homogeneity with credibility of data Review loss statistics and reporting pattern to see if data is still credible Leverage u/w-er, claim handlers and data processors to determine which variables are the most important 13.3.2 Step (2) &amp; (3): Analyze Distorical Data and Projection Backward looking of historical data (Step 2): Analyze the historical development patterns. If possible, consider individual case reserve development and the emergence of IBNR claims separately Use long periods where practical and curve fit where practical Forward looking and projection (Step 3): Estimate the future development. If possible, estimate the bulk reserve for IBNER and pure IBNR separately 13.3.2.1 Claim Report and Payment Lags We want to find stable expected development pattern for homogeneous categories Definition 13.1 \\(y = Rlag(t) = p_k\\) (\\(p_k\\) is the Clark notation) Report lag \\(= \\dfrac{1}{ATU \\text{at time }t} =\\) % Reported to Date Remark. Benefits of the above definition: We can fit a parametric curve (see Clark) to smooth curves to compute the tail e.g. Gamma with \\(\\mu\\) = 2.5 years and \\(\\sigma\\) = 1.5 years (Figure 13.2) \\(Rlag(t)\\) can be interpret as probability that any particular claims dollar will be reported to the reinsure by time \\(t\\) We can compute statistics such as the expected value to compare one claim report pattern with another ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Figure 13.2: Claims report lag based on Gamma distribution 13.3.2.2 Short Tailed Lines Loss reporting and settlement is quick \\(\\therefore\\) Use method that provide reasonable accuracy for the least effort and cost Table 13.1: Reinsurance categories that are usually short-tailed Category I1 Category II Category III Property Treaty Proportional Property Treaty Cat Property Treaty Excess2 Property Facultative3 Fidelity Proportional For property, beware of recent cat events Exclude high layers for XS property Exclude construction International exposure can have significant reporting delay as well Estimation Methods Set IBNR = % of the last 12 months’ EP Reasonable for non-major-cat “filtered” claims Claims for major cat may not be fully reported and finalized for years even on proportional covers Book LR for new lines Allocate claims to AY for reserving (based on historical information) U/w year (or treat year) will have extra lag Treaty effective 1/1/16 will cover policies inception from 1/1/16 to 12/31/16 and therefore can cover accidents that happens in 12/31/17 \\(\\Rightarrow\\) 2 years after the inception of the treaty 13.3.2.3 Medium Tailed Lines Definition 13.2 Medium-taild exposure Average dollar lag of 1-2 years Almost completely settled within 5 years Over half the ultimate losses are reported within 2 years Table 13.2: Reinsurance categories that are usually medium-tailed Category I Category II Category III Category IV Property Treaty Excess High layers1 Construction2 Surety Fidelity Excess Ocean Marine Inland Marine Property International Non-casualty Excess Aggregate3 Ideally separate from working layers Ultimate value may not be known immediately and will take longer to penetrate higher per-risk XS attachment point Ideally separate from other property exposures Discovery period can extend years beyond the contract period Lags are longer then the underlying exposure For surety should estimate gross and recovery separately as the recovery has a longer tail Consider the ratio of salvage to gross loss for mature years Estimation Methods Chainladder works fine for paid and incurred with or without ACR 13.3.2.4 Long Tailed Lines Definition 13.3 Long-taild exposure Exposures which the average aggregate claims dollar report lag is over 2 years Claims are not settled for many years Table 13.2: Reinsurance categories that are usually long-tailed Category I Category II Category III Category IV Casualty Treaty Excess1 Casualty Treaty Proportional2 Casualty Facultative Casualty Excess Aggregate3 APH4 Longest lags except for APH Some of this exposure maybe medium-tailed Lag are longer than for the underlying exposure Asbestos, pollution, and other health hazard and mass tort claims Longest of the long tail Should be remove from data and analyzed separately Use methods specifically for APH Exclude claims from commuted contracts as they distort development patterns Need to separate the above into finer, more homogeneous categories (e.g. separate claims-made and occurrence coverage) Estimation Methods Chainladder (BF): Don’t use, too much leverage Bornhuetter-Ferguson (CL): Better than CL, correlate future development with an exposure measure (reinsurance premium): Premium \\(\\times\\) ELR Caveat 1): Very dependent on selected LR LR for a given AY is strongly correlated with the underwriting cycle as well as the reported loss to date \\(\\therefore\\) need to consider the u/w cycle in ELR pick May use LR from pricing work Caveat 2): Estimate for each AY does not reflect the reported to date (unless the selected LR is chosen with that in mind) Stanard Buhlmann (SB or Cape Cod in Europe): \\(ELR = \\dfrac{\\sum C_k}{\\sum E_k p_k}\\) \\(C_k\\): reported to date; \\(E_k\\): Adj Prem; \\(p_k\\): expected % reported to date; \\(k\\) is the exposure year Must on-level the historical premium and adjust to be pure premium (\\(E_k\\)) Remove commissions and brokerage and internal expenses (but might not worth the effort) Remove any suspected rate level differences (so each year have the same ELR) SB IBNR \\(= E_k(1-p_k) \\times ELR\\) Remark: This estimate of the ELR use actual incurred loss Method is sensitive to the accurary of the on-level EP Credibility IBNR Estimates: Weight the CL method with SB or BF Reserve for year \\(k\\): \\(\\hat{R}_k = Z_k \\times R^{CL}_k + (1-Z_k)\\times R^{SB}_k\\) \\(Z_k = p_k \\times CF\\); Credibility Factor \\(CF \\in [0,1]\\) \\(CF = 0\\) \\(\\Rightarrow\\) BF \\(CF = 1\\) \\(\\Rightarrow\\) Benktander (GB) Can also weight together the IBNR estimate from paid loss method Case reserve are based on judgement of many people and can vary over time (lack consistency) \\(\\Rightarrow\\) Given long enough paid history, paid estimate might be more stable and credible Or weight with BF based on different a-priori (e.g. based on pricing ELR) Alternative Methods: Fit losses reported to date to a curve (Clark) Practical problem: negative incrementals (get around by grouping over different periods) Model claim counts and use stochastic model (Bootstrap) for the whole claims development process Caveat: difficult to explain to management Advantage: Can contemplate intuitively satisfying models for various lag distributions Time from loss to first report and lag from report to settlement Connect the lags with appropriate models for the dollar reserving and payments on individual claims up through settlement 13.3.3 Step (4): Monitoring and Testing Predictions Compare AvE over quarters and do this for several quarters to see if any trends become apparent Forecast IBNR runoff over the quarters Provides early warning if claims are emerging higher than expected Definition 13.4 Expected reported claims \\(\\dfrac{\\text{Expected % Reported in Period}}{\\text{Expected % Unreported}} \\times IBNR\\) Caveat: Can be difficult to tell if the AvE difference is large or small \\(\\therefore\\) Look for deviations over time and look for trends Deviation could be IBNR is too small, claims are paying faster than expected, or just random fluctuation "],
["past-exam-questions-12.html", "13.4 Past Exam Questions", " 13.4 Past Exam Questions Haven’t done TIA practice questions Concepts 1996 #10: Problems of reinsurance reserving 1999 #9: Partition consideration 1999 #10: Fidelity proportional is short tail 2000 #68 b c: SB incorporates reported loss in the ELR but difficult to adjust ELR to a prior estimate for each year 2001 #54: pros and cons of CL and BF 2002 #42: SB vs BF similarity and difference 2004 #49 c d: SB vs BF pros and cons 2006 - #45: Which method to use SB vs CL 2007 - #21: Discuss how SB is a special case of BF 59 \\(\\star\\) 2008 - #35: Reason of report lag and upward development due to modal value claim reserve \\(\\star\\) 2008 - #37: Industry data vs internal Industry: thin internal data; volatile from rate changes; rate change history not credible; shift in underlying policies Internal: Rate change not reflected in industry; industry composition different from reinsurer; Underlying policies not represented in Sch P data; believe data is credible and representative of the specific business involved \\(\\star\\) 2009 - #32: How to partition data Fac vs treaty \\(\\Rightarrow\\) LoB; Don’t split by cedant as data too thin Diagnostics: Look at reporting pattern and amount of data for credibility; discuss with u/w and claims 2011 #10 b: SB pros and cons 2011 #11: report lag; upward development; standard method not work; industry data not work 2012 #10 b-d: SB vs CL 2012 #11: problems of reinsurance reserving 2013 #10 c: SB vs CL 2014 #13: problems of reinsurance reserving \\(\\star\\) 2015 #12: Discuss problems of reinsurance reserving based on data given Reporting lag (claims below reporting threshold) Persistent upward development (upward development of ALAE by looking at ratio) Heterogeneity patterns (each AYs have different retention) 2015 #13 b: Reason to adjust EP for SB (e.g on-level so LR for each AY is comparable, adjust for varying rate as SB assumes constant ELR) Calculations 1997 #11: SB (Use adjusted on-level premium) 1998 #35: IBNR with CL, BF, SB 2000 #68 a: SB 2001 #53: SB 2003 #44: SB 2004 #49 a b: CL and SB 2005 #38: SB \\(\\star\\) 2007 - #3: Credibility weight SB and CL, remember to use \\(Z_k = p_k \\cdot CF\\) 2007 - #22 a b: CL and SB 2008 - #36: SB 2009 - #31: SB 2011 #10 a c: SB and credibility weighted \\(\\star\\) 2012 #10 a: SB method, need to consider age of policy on risk attaching vs loss occurring basis, also take out expense from premium 2013 #10 a b: SB and credibility weighted SB and CL 2014 #14: SB 2015 #13 a: SB 13.4.1 Question Highlights -->"],
["a13-estimating-the-premium-asset-on-retrospectively-rate-policies-m-teng-and-m-perkins.html", "Chapter 14 A13 Estimating the Premium Asset on Retrospectively Rate Policies - M. Teng and M. Perkins ", " Chapter 14 A13 Estimating the Premium Asset on Retrospectively Rate Policies - M. Teng and M. Perkins "],
["cliffs-summary-8.html", "14.1 Cliff’s Summary", " 14.1 Cliff’s Summary Retro formula \\(\\underbrace{P}_{\\text{Premium}} = [\\underbrace{BP}_{\\text{Basic Premium}} + (\\underbrace{CL}_{\\text{Capped Losses}} \\cdot \\underbrace{LCF}_{\\text{Loss Conversion Factor}})] \\cdot \\underbrace{TM}_{\\text{Tax Multiplier}}\\) Use PDLD to get the premium development based on the loss developement Know what each term means like basic premium factor or charge Know the formula approach for PDLD \\(\\begin{align} PDLD_1 =\\underbrace{\\left(\\frac{BP}{SP} \\right)}_{\\text{Basic Prem Factor}} \\frac{TM}{ELR \\cdot \\%Loss_1} + \\underbrace{\\left( \\frac{CL_1}{L_1}\\right)}_{\\text{Loss Capping Ratio}} \\cdot LCF \\cdot TM \\end{align}\\) To adjust for being too responsive: \\(\\begin{align} P_1 = \\underbrace{\\left( \\frac{BP}{SP} \\right) \\frac{TM}{ELR \\cdot \\%Loss_1}}_{\\text{Not }\\propto\\text{ Loss}_1} \\times \\operatorname{E}[L_1] + \\underbrace{\\left( \\frac{CL_1}{L_1} \\right) \\cdot LCF \\cdot TM}_{\\propto \\text{ Loss}_1} \\times L_1 \\end{align}\\) Subsequent PDLD: \\(\\begin{array}{cccl} PDLD_n &amp;= &amp;\\dfrac{CL_n - CL_{n-1}}{L_n - L_{n-1}}&amp;LCF \\cdot TM \\:\\:\\:\\:\\text{For }n&gt;1\\\\ &amp;= &amp;\\dfrac{\\Delta CL}{\\Delta L}&amp;LCF \\cdot TM\\\\ \\end{array}\\) Know the empirical approach for PDLD Assume premium lags Ratio selection Cumulate PDLD based on a weighted average with expected % future report for each future periods Know the practical application First adjustment period might cover more than one policy period Know the Teng Perkins improvements and assumptions 14.1.1 Types of Exam Questions Haven’t done TIA practice questions Concepts 1998 - #55: formula vs empirical, expected difference given a scenario 2001 - #39 b: PDLD ratio pattern First adjustment the basic premium is included in the retro premium computation; Small portion of loss is limited by the retro; LCF and TM also impacts it At later periods, more losses are capped by the limit 2003 - #9: Loss capping ratio properties Double check 2003 - #10: Feldblum comment on Teng Perkins: premium responsiveness decreases at higher LR for a book of business 2007 - #36: Retro premium and reported losses relationship \\(\\star\\) 2014 #12: incremental losses vs premium \\(\\star\\) 2015 #14: PDLD ratio Calculations \\(\\star\\) 1999 - #55: Calculating CPDLD and expected future premium Remember to apply \\(CPDLD_1\\) to all policies that have not have an adjustment 2001 - #39 a: CPDLP 2002 - #14: Premium asset = Expected Future Premium + Premiums booked from prior adjustment - premium booked as of current 2004 - #7: Empirical PDLD ratio 2005 - #20: Premium asset calc with PDLD \\(\\star\\) 2006 - #23: \\(PDLD\\) from formula and why the loss capping ratios decrease over time 2007 - #5: \\(PDLD_2\\) plug and play 2008 - #14: premium asset 2009 - #6: Premium after 2nd retro adjustment \\(\\star \\star\\) 2012 - #7: Retrospective premium asset, good question, 2 methods to do \\(\\star\\) 2013 #6 a-c: PDLD 1 to 3 and relationship with loss capping ratio, ratio should decrease monotonically State whether you useing the loss cappting ration as cummulative or incremental 2015 #14 a: PDLD 1 and 2 plug and play "],
["introduction-5.html", "14.2 Introduction", " 14.2 Introduction Premium Asset: Asset supported by premium the insurer expect to collect from clients with retrospective policies as losses develops The method discussed here estimate premium asset over a whole book of business Framework based on retro rating formula (requires knowledge of rating parameters for the entire book) and estimating parameters based on empirical data Retospective policies were popular in 1996: For insured Returns premium to insured for good experience Premiums are due with short lag to losses, as opposed to upfront For insurer Plans allow an insurance company to shift a significant portion of the risk to the insured Ultimate Premiums Deviation: \\(\\dfrac{\\text{Ultimate Premium}}{\\text{Standard Premium}}\\) "],
["formula-approach-for-pdld.html", "14.3 Formula Approach for PDLD", " 14.3 Formula Approach for PDLD 14.3.1 Retro Premium Formulas \\(\\underbrace{P}_{\\text{Premium}} = [\\underbrace{BP}_{\\text{Basic Premium}} + (\\underbrace{CL}_{\\text{Capped Losses}} \\cdot \\underbrace{LCF}_{\\text{Loss Conversion Factor}})] \\cdot \\underbrace{TM}_{\\text{Tax Multiplier}}\\) Basic Premium: Covers minimum premium, commissions and expenses; premium when there are no losses Capped Losses: Losses are capped from premium calculation, individually and in aggregate Loss Conversion Factor: Variable expense loading The above can be expressed in proportion to the Standard Premium Standard Premium = Manaul Rate \\(\\times\\) E-Mod \\(\\times\\) Sch-Mod = \\(\\dfrac{\\text{Expected Loss}}{\\text{ELR}}\\) Same formula is used to calculate the cummulative premium due at each adjustment but with the below: \\(P_n\\): Premium @ nth adjustment \\(CL_n\\): Capped losses @ nth adjustment Basic Premium Factor = \\(\\dfrac{BP}{SP}\\) Basic Premium Charge = \\(\\dfrac{BP}{SP} \\cdot TM\\) 14.3.2 Calculating PDLD \\(PDLD = \\dfrac{\\Delta P}{\\Delta L} \\sim \\dfrac{d P}{d L}\\) Relationship between premium development and loss development Apply to expected loss development to determin the expected premium development The \\(L\\) here is not capped \\(PDLD_n = \\dfrac{P_n - P_{n-1}}{L_n - L_{n-1}}\\) Requires details about the policies written Resonable for individual policies but difficult for an entire book 14.3.3 Estimating First Period PDLD \\(\\begin{align} PDLD_1 =\\underbrace{\\left(\\frac{BP}{SP} \\right)}_{\\text{Basic Prem Factor}} \\frac{TM}{ELR \\cdot \\%Loss_1} + \\underbrace{\\left( \\frac{CL_1}{L_1}\\right)}_{\\text{Loss Capping Ratio}} \\cdot LCF \\cdot TM \\end{align}\\) Can you just also write it as \\(PDLD_1 = \\left( \\dfrac{BP}{L_1} + \\dfrac{CL_1}{L_1} \\cdot LCF \\right) \\cdot TM\\) The formula above is too responsive to actual losses in the first period, as only a portion of the formula is related to the \\(L_1\\) as shown below Memorize Formula \\(\\begin{align} P_1 = \\underbrace{\\left( \\frac{BP}{SP} \\right) \\frac{TM}{ELR \\cdot \\%Loss_1}}_{\\text{Not }\\propto\\text{ Loss}_1} \\times \\operatorname{E}[L_1] + \\underbrace{\\left( \\frac{CL_1}{L_1} \\right) \\cdot LCF \\cdot TM}_{\\propto \\text{ Loss}_1} \\times L_1 \\end{align}\\) So technically only the 2nd part of the formula should be appied to \\(L_1\\) while the first part is in relation to the \\(\\operatorname{E}[L_1]\\) 14.3.4 Estimating Subsequent PDLD \\(\\begin{array}{cccl} PDLD_n &amp;= &amp;\\dfrac{CL_n - CL_{n-1}}{L_n - L_{n-1}}&amp;LCF \\cdot TM \\:\\:\\:\\:\\text{For }n&gt;1\\\\ &amp;= &amp;\\dfrac{\\Delta CL}{\\Delta L}&amp;LCF \\cdot TM\\\\ \\end{array}\\) \\(LCF\\) and \\(TM\\) are known at policy inception so we only have to estimate the loss capping ratio \\(\\dfrac{\\Delta CL}{\\Delta L}\\) "],
["empirical-approach-for-pdld.html", "14.4 Empirical Approach for PDLD", " 14.4 Empirical Approach for PDLD Advantages: Easier to do for an entire book of business as not every client will have the same parameters Book should be seperate into homogenous groups Size and typce of rating plan sold Need to assume a lag from the date used to value the losses and when the actual premium is collected Typically 3 to 9 months e.g. if the loss age is 18, the premium will be at 27 if we assume a 9 months lag First evaluation for loss is typically at 18 months e.g. \\(PDLD_2 = \\dfrac{P_2 - P_1}{L_2 - L_1} = \\dfrac{^{39}P - {^{27}P}}{^{30}L - {^{18}L}}\\) 14.4.1 PDLD Ratios Selection Compile PDLD triangle for all effective date groupings and make a selection for each age If you observe increasing trends in the ratios for a give age: More losses are within the loss capping layer Higher agg max or lower agg min or higher per claim limits Improvement in loss experience (fewer large losses so larger portion is within the cap) Higher basic limits (for \\(PDLP_1\\)) PDLD can be negative if \\(\\Delta CL\\) is negative: losses &gt; max increase while claims within retro limit have a reductiion in reserves 14.4.2 Cumulative PDLD Ratios Cumulate PDLD based on a weighted average with expected % future report for each future periods 14.4.3 Application For each quarter Determine reported losses @ prior adjustment Estimate ultimate losses \\(\\Delta L\\) = (2) - (1) \\(\\sum \\limits_{qtr \\in last \\: adj \\: age \\: i} \\Delta L_{qtr}\\) For each year Expected premium emergence = \\(\\Delta P = CPDLD \\times \\Delta L_{yr}\\) Determine premiums booked through prior adjustment Estimated total premium = (1) + (2) Premium asset = Premium booked as of current evaluation - (3) Premium booked as of current eval \\(\\neq\\) premium booked through prior adjustment due to: timing of retro adjustments, minor premium adjustments, interim premium booking Text doesn’t say make adjustment though For the most recent periods need to adjust for only exposure period that is being earned out "],
["further-issues.html", "14.5 Further Issues", " 14.5 Further Issues If loss plan includes ALAE, you should include ALAE in the losses for this as well \\(\\Delta\\) in mix of business will affect the sensitivity of premium to loss and thus the \\(PDLD\\) For premium that is not secured, a provision for bad debt should be held "],
["feldblums-discussion.html", "14.6 Feldblum’s Discussion", " 14.6 Feldblum’s Discussion We do not develop premium directly because: Estimate of ultimate inc’d losses can be obtained sooner Retor premium depends on incurred loss Fitzgibbons and Berry Also create a linear relationship of premium to losses, but they forecast ultimate premium directly, rather than premium development \\(\\Rightarrow\\) can lead to large deviations Premium asset \\(\\propto\\) expected unreported losses Does not respond if the actual premium to loss relationship is different than estimated Improvements from Teng &amp; Perkins Slop of premium to loss changes as the year matures Large losses that pierce the claim cap tend to have their development later No future development one aggregate max is hit Forecast future premium development Projected premium asset is based on projected unreported losses, does not consider losses to date Errors in projecting premium to date are automatically corrected for since it projects premium development and not ultimate premium Teng &amp; Perkins Assumptions Premium responsiveness \\(\\dfrac{\\Delta P}{\\Delta L}\\) in a period is \\(\\perp\\!\\!\\!\\perp\\) of responsiveness at prior adjustments Premium responsiveness depends on the maturity, not the loss ratio or beginnging reto premium ratio Enhancement \\(PDLD_1\\) should separate the basic premium component from the component \\(\\propto\\) losses Simple adjustment, just estimate the basic premium as a ratio to \\(SP\\) and subtract from \\(PDLD_1\\) \\(\\dfrac{P_{fixed}}{\\operatorname{E}[L]} = \\underbrace{\\dfrac{BP}{SP} \\cdot TM}_{\\text{Basic Prem Charge}} \\Big / ELR\\) Based on \\(P_{fixed} = BP \\cdot TM\\) Check this how to apply to CPDLD "],
["past-exam-questions-13.html", "14.7 Past Exam Questions", " 14.7 Past Exam Questions 1999 - #55 2009 - #6 2012 - #7 2014 - #12 2015 - #14 "],
["b1-pc-insurance-company-valuation-r-goldfarb.html", "Chapter 15 B1 P&amp;C Insurance Company Valuation - R. Goldfarb ", " Chapter 15 B1 P&amp;C Insurance Company Valuation - R. Goldfarb "],
["cliffs-summary-9.html", "15.1 Cliff’s Summary", " 15.1 Cliff’s Summary Know how to get the discount rate and growth rate for each of the 3 method DDM: \\(V_0 = \\dfrac{\\mathrm{E}[Div_1]}{k - g}\\) Knows how this can transform to the P:E formula Remembers the terminal value formula uses CF at 1 but get you to time 0 FCFE: \\(FCFE = NI + (Non \\:Cash\\:Charges) - \\Delta Working \\:Capital - \\Delta Capital + \\Delta Debt\\) Discount all the FCFE for \\(V_0\\) Advantages FCFE vs FCFF AE: \\(\\begin{align} V_0 = BV_0 + \\sum_{t=1} \\frac{\\overbrace{(ROE_t - k)BV_{t-1}}^{AE_t}}{(1+k)^t}\\end{align}\\) Remember to add the \\(BV_0\\) Advantages Considerations Relative multiples: \\(\\dfrac{P_0}{E_1} = \\dfrac{1 - \\rho}{k - \\rho \\times ROE}\\); Based on DDM \\(\\dfrac{P_0}{BV_0} = 1 + \\dfrac{ROE - k}{k - g}\\); Based on AE 15.1.1 Types of Exam Questions Haven’t done TIA practice questions Concepts \\(\\star\\) 2011 - #12 a: higher \\(g\\) should be matched with higher \\(\\beta\\) for being more risky 2011 - #12 c: relationship between \\(g\\) and \\(k\\) 2015 #17 b-c: \\(\\beta\\) discussion DDM 2008 #43: DDM calc and convert to P:E \\(\\star\\) 2008 #44: DDM calc and interpretation 2008 #45: Comparison on P:E 2009 #34 2011 - #12 b: DDM calc (Get \\(r_f\\) as t-bond less liquidity premium) \\(\\star\\) 2012 #12: Full calc with DDM 2014 #16: DDM calc 2015 #17 a: DDM calc FCFE \\(\\star\\) 2009 #35: Full FCFE calc \\(\\star\\) 2013 #11: Full FCFE calc with discussion FCFE vs FCFF 2014 #17: FCFE Calc; Not sure bout the reserve increase impact on required capital \\(\\star\\) 2015 #16: Discussion on DCF AE \\(\\star\\) 2010 #31: Full AE calc 2011 - #13: Price to Book value calc + discussion on assumption 2013 #12: Full AE Calc + discussion 2015 #15: AE Calc + discussion Relative Valuation \\(\\star\\) 2014 #15: Price to Book calc using all the ratios given by LoB "],
["assumptions.html", "15.2 Assumptions", " 15.2 Assumptions Key assumptions are cost of captial \\(k\\) and growth rate \\(g\\) 15.2.1 Risk Adjusted Discount Rate Recognize the risky cashflow by discounting them at a rate higher than the risk-free rate based on CAPM \\(k = r_f + \\beta \\left [ \\mathrm{E}(r_m) - r_f \\right ]\\) Risk of an investment depends on the rest of an investor’s portfolio. We focus instead on equilibrium rates of return Different BU has different risk profile \\(\\Rightarrow\\) Different discount rates Discount rates can vary by period if business mix change Not all cash flow have the same risk profile (premium, investment income, paid losses) Simplification is to use average discount rate for the portfolio One alternative way to account for the risky cash flow is to convert the cash flow to certainty equivalent cash flows and discount with risk free instead of the cost of capital Reflect the risk in the cashflow directly Risk free Rate: \\(r_f\\) 90 days t-bill Maturity matched t-notes T-bonds less liquidity &amp; term premium (~1.2%) Market Risk Premium: \\(\\mathrm{E}(r_m) - r_f\\) 6-8% historically \\(r_f\\) here should be consistent Need to sensitivity test Systematic Market Risk: \\(\\beta\\) Based on regression on stock return vs market return Can use industry \\(\\beta\\) Mix of business needs to be similar to industry Industry \\(\\beta\\) should be adjusted for differences in the industry leverage and company leverage \\(\\beta\\) will be higher for firms with more leverage, riskier business units Alternative is to use all equity \\(\\beta\\) to remove bias from leverage Higher growth should have higher \\(\\beta\\) Insurance company has additional leverage from policyholder liabilities Can assume total leverage of insurance companies is similar 15.2.2 Growth Rate Used for the period after the forecast horizon Method Growth Rate: \\(g\\) DDM \\(ROE \\times \\rho\\) FCFE \\(ROE \\times\\) [Reinvestment Rate] AE At most the growth in book value Return on Equity: \\(ROE\\) \\(\\dfrac{NI}{BE} = \\dfrac{\\text{Net Income after Tax}}{\\text{Beginning Equity}}\\) Plowback Ratio: \\(\\rho\\) % of \\(NI\\) that is reinvested in the firm Company that have high growth should retain more earnings Reinvestment Rate \\(\\dfrac{\\Delta Capital}{NI}\\) "],
["dividend-discount-model-ddm.html", "15.3 Dividend Discount Model (DDM)", " 15.3 Dividend Discount Model (DDM) \\(V_0 = \\dfrac{\\mathrm{E}[Div_1]}{k - g}\\) \\(Div_1\\) is paid at the end of year 1 Constant growth assumption \\(\\mathrm{E}[Div_1] = (1 - \\rho) NI\\) Typically forecast a few years and use the above formula for the terminal value Need to use \\(NI\\) after tax When calculating \\(g\\), calculate \\(ROE\\) and \\(g\\) for all years and make selection Firm with high expected growth tend to be riskier \\(\\Rightarrow\\) Higher discount rate Forecast is more susceptible to being wrong, so should be discount more? DDM Assumptions Expected dividends Dividend growth rate Risk-adjusted discount rate (From CAPM) "],
["free-cash-flow-to-equity-fcfe.html", "15.4 Free Cash Flow to Equity (FCFE)", " 15.4 Free Cash Flow to Equity (FCFE) Free cash flow available to pay shareholders: \\(FCFE = NI + (Non \\:Cash\\:Charges) - \\Delta Working \\:Capital - \\Delta Capital + \\Delta Debt\\) Memorize Formula \\(\\Delta\\) loss reserve reflected in \\(NI\\) only; it gets netted out as non-cash charges and capital expenditures \\(NI\\) is net of interest payments to shareholders, after tax \\(g = ROE \\times\\) [Reinvestment Rate] \\(= ROE \\times \\dfrac{\\Delta Capital}{NI}\\) Assume portion of FCF not paid out is invested at \\(k\\) Free Cash Flow Cash flow available to pay out to the firm’s source of capital (for FCFE this is only to equity) net of amounts required to be reinvested to the firm for growth Weakness: require forecasting financial statements, use adjusted accounting measure, large terminal value Similar to DDM, calculate the \\(ROE\\) and reinvestment rate for all years and make a pick FCFE vs FCFF We don’t use the free cash flow to firm because there’s additional leverage for p/h liabilities from reserves held Not clear how to calculate cost of capital due to leverage from 2 different source and it complicates the calculation Using FCFE removes the source of leverage Advantages over DDM Dividend are discretionary Firms also return funds via stock buybacks Focus on free cash flow "],
["abnormal-earnings-ae.html", "15.5 Abnormal Earnings (AE)", " 15.5 Abnormal Earnings (AE) Works with accounting measures of income Need to remove distortions More accurate some say Clean surplus assumption Requires all changes to book value (on the b/s) flow through the I/S Flow through as earnings, dividends or capital contributions \\(V_0 = BV_0 + \\begin{align}\\sum_{t=1} \\frac{AE_t}{(1+k)^t}\\end{align}\\) \\(AE_t = NI_t - k \\cdot BV_{t-1} = (ROE_t - k)BV_{t-1}\\) Important Formula Earnings (net income) XS of cost of capital Assume AE will trend to zero overtime since it’s difficult to maintain AE is difficult to maintain as competitors will see the AE and move into the market Parameters Considerations \\(BV_0\\) Reported book value Focus on tangible book value (e.g. take out goodwill) Remove any systematic bias such as over or understated reserve \\(NI\\) is net of interest payments to shareholders, after tax; Same as DCF model Make complement of the book value adjustments here e.g. any direct adjustment to the B/S that doesn’t flow from the I/S you have to adjust here If reserve is discounted in the \\(BV_0\\), need to change (lower) the \\(ROE\\) as the income will be generated from a larger capital base \\(g\\) Should be negative as AE tend to 0 Does not require additional capital as the growth from that extra capital will not accrue to today’s shareholders Advantages Focus on value creation Earnings above the required return on capital Dividends and CF are just consequence of value creation Small terminal value as it focus on any added value so less leverage Directly using accounting measures so does not need to adjust into a cash flow measure "],
["relative-multiples.html", "15.6 Relative Multiples", " 15.6 Relative Multiples We don’t compare to sales because of leverage from p/h’s liability Stock price can fluctuate so use an average price Multiples can vary significantly even over short periods of time Assumptions Constant \\(ROE\\), \\(\\rho\\), and \\(k\\) 15.6.1 Price to Earnings Based on DDM \\(\\dfrac{P_0}{E_1} = \\dfrac{1 - \\rho}{k - \\underbrace{\\rho \\times ROE}_{g}}\\) Memorize Formula If \\(ROE &gt; k\\) \\(\\Rightarrow\\) Keep a high \\(\\rho\\) P:E Ratio Forward or leading P/E = consensus forecast earnings for next year Trailing P/E = last year’s actual; Can be distorted by unusual events Price = value of the firm derived from any of the methods Earnings = \\(NI\\); Either forward or trailing By default, apply the ratio to next year’s earnings per formula Alternative use of P/E Validating assumptions: reasonability check on the forecast Shortcut to valuation: if you think company will grow similar to the industry Terminal value: use the other method for the forecast horizon then P/E for the terminal value 15.6.2 Price to Book Based on AE method \\(\\dfrac{P_0}{BV_0} = 1 + \\dfrac{ROE - k}{k - g}\\) Memorize Formula \\(BV_0 =\\) equity @ t = 0 Useful for firms with substantial holdings in marketable securities 15.6.3 Transaction Multiples Can use multiples from transaction, however caveat: Companies tend to overpay Control premium, and other reason M&amp;A overpay IPO’s are under priced Financials used to value the transaction could be different from the information being used now (forecast is different) Economic conditions @ transaction \\(\\neq\\) economic conditions now 15.6.4 Relative Valuation for Multi-line firms Use multiples from pure play peers (monoline firms) to estimate by division Or compare multiples from diversified insurers Choose firms with similar business, \\(ROE\\), claims paying rating, \\(\\beta\\) "],
["past-exam-questions-14.html", "15.7 Past Exam Questions", " 15.7 Past Exam Questions 2008 #43 2009 #35 2010 #31 2011 #13 2012 #12 2013 #11 "],
["c1-era-1-1-1-3-introduction-brehm-et-al-.html", "Chapter 16 C1 ERA 1.1 - 1.3 Introduction - Brehm, et al. ", " Chapter 16 C1 ERA 1.1 - 1.3 Introduction - Brehm, et al. "],
["cliffs-summary-10.html", "16.1 Cliff’s Summary", " 16.1 Cliff’s Summary Requirements to promote ERM Definition of ERM 7 key aspect of ERM 4 categories of risk Enterprise risk model process Diagnose: Assess all risk and considergeneral environment, industry, and firm specific items Analyze what risk are critical and consider correlations; Select measures consistent with management’s veiw of risk and prioritize Implement ways to manage the risk: avoid, reduce frequency, mitigate severity, transfer, or retain Monitor the process, update when there’s new risk, new ways to control or new options to transfer/treating the risk Goal of enterprise risk model Model quality 4 key elements of enterprise risk model: underwriting risk, reserving risk, asset risk and dependencies Know the different component of u/w risk Different asset class modeling Source of dependencies Approach to setting capital based on default avoidance or threshold lower than probability of ruin Risk measures do not tell us how much capital to hold; They tell us for a given level of capital, what the risk to the company is Risk model can also be used to allocate capital to BUs and investment function to calculate risk-adjusted performance "],
["historical-context.html", "16.2 1.1 Historical Context", " 16.2 1.1 Historical Context RBC (from 1990s) was the precursor to scenario testing and Dynamic Financial Analysis (DFA) but DFA didn’t pan out DFA requires support from many functions and was not widely used due to lack of natural champion ERM as of 2007: IAIS began development of an approach of rating insurer based on Basel II. See 2004 IAA document CA and AUS require use of internal risk models UK implemented Individual Capital Adequacy Standards (ICAS): A firm is required to undertake regular assessments of the amount and quality of capital which in their view is adequate for the size and nature of their business NAIC is moving towards a new audit paradigm: Capital adequacy, Asset quality, Reserves, Reinsurance, Management, Earnings and Liquidity (CARRMEL) 2005 S&amp;P stated that a firm’s EMR program will become a critical component in its rating methodology ERM needs an internal champion with quantitative skills, operational experience and political skills to work across organizational silos "],
["overview-of-erm.html", "16.3 1.2 Overview of ERM", " 16.3 1.2 Overview of ERM Definition: ERM is the process of systematically and comprehensively identifying critical risk, quantifying their impacts and implementing integrated strategies to maximize enterprise value 16.3.1 Key aspects of ERM ERM is: A process, not a one time analysis Enterprise wide basis Focus on critical or material risk Risk: Has upside and downside, it’s when actual outcome \\(\\neq\\) expected Must be quantified; Dependencies must be evaluated and quantified Strategies: Are developed to Avoid, mitigate, or exploit risk factors Are evaluated as a tradeoff of risk and return, to maximize firm value 16.3.2 4 Categories of Risk Insurance Hazard: Unique to insurers that is intentionally taken on to make a profit In-force exposures: accumulation (CAT); underwriting (non-CAT) Past exposures (reserve) Financial Risk: Assets of insurance company Insurers tend to have high \\(\\frac{Asset}{Equity}\\) ratio Operational Risk: Execution risks of the company Not as responsive, amenable to quantitative modeling Strategic Risk: About choices, making the right or wrong or no strategic choices X* Not as responsive, amenable to quantitative modeling 16.3.3 ERModel Process Diagnose \\(\\Rightarrow\\) Analyze \\(\\Rightarrow\\) Implement \\(\\Rightarrow\\) Monitor 16.3.3.1 1) Diagnose High level risk assessment All risks are considered but start to develop threshold for risk that will be considered material of significant (Key aspect 3) General Environment Political uncertainties (democratic changes, war, revolution) Government policy (fiscal, monetary changes, regulation) Marcoeconomic environment (inflation, interest rates) Social (terrorism) Natural (CAT) Industry Input market (supply, quality) Product market (demand) Competitive uncertainties (new entrants, rivalry) Firm Specific Operating (labor, supply) Liability (products, employment) R&amp;D Credit Behavioral 16.3.3.2 2) Analyze Analyze risk identified in step 1 for materiality and prioritize Critical Risk: Risk with potential to exceed the company’s threshold are modeled Preferably quantified with a probability distribution of outcomes Correlations must be recognized Selected measures must be consistent with management’s view toward risk Prioritize risk factors that contribute to adverse scenarios above the critical threshold 16.3.3.3 3) Implement Focus on managing risks identified and analyzed as material 5 potential actions to manage risk: Avoidance Reduction in chance of occurrence Mitigate effect given occurrence Eliminate or transfer of the consequences Retention Speculative risk factor may provide an opportunity to capitalize on the risk, rather than manage it away Perform a risk/return trade-off analysis 16.3.3.4 4) Monitor Management need to monitor the process, compare to expectations Need to update plans and model as company or market or competitors change New risk to address New ways to control them New options for treating or transferring risk "],
["ermodel-overview.html", "16.4 1.3 ERModel Overview", " 16.4 1.3 ERModel Overview Goal: understand and quantify the relationships among the risks to the business that arise from: asset, liabilities and underwriting All are affected by internal decisions &amp; external factors Modeling will help with management functions and strategic decisions: Planning growth Valuing companies for M&amp;A Determining capital need Setting reinsurance strategies Identifying sources of significant risk Managing asset mix 16.4.1 Quality of Models Good Model: Show risk and reward for a range of different strategies Recognizes its own imperfection Weak Model: Over(under)state risks \\(\\leadsto\\) Overly cautious (aggressive) choices Data quality will affect suitability of the model; require assumptions from the modeler Elements that differentiate model quality: Model reflects relative importance of various risks Model includes dependencies Modelers have a deep knowledge of the fundamentals of those risks Modelers have a trusted relationship with senior management Take into account the uncertainty from other models (CAT, ESG, credit risk) that are used as inputs Operational risks have to be modeled in bulk using informed judgement as we might not be able to model them in a detailed fashion 16.4.2 Key Elements of ERM Underwriting Risk Loss Frequency and Severity Distn Pricing Risk Parameter Risk Estimation Risk Projection Risk Event Risk Systematic Risk CAT Model Uncertainty Reserving Risk Asset Risk Dependencies (Correlation) 16.4.2.1 Underwriting Risk Focus on day to day losses and CAT losses Loss Frequency and Severity Distribution Use statistical methods to: Estimate parameters Test quality of fit Understand uncertainties that remain Pricing Risk Risk from not charging the right price Deficiency may exist for many years for long tailed lines \\(\\Rightarrow\\) Accumulation of reserve deficit Model should include u/w cycle Parameter Risk: Risk of parameters used being incorrect Estimation Risk: Error in estimate since it’s based on available data Projection Risk: Error on forecast of future value even we fit the historical data accurately Error on ultimate losses, future trends Unexpected change in external risk conditions (Linked to event risk) Event Risk: Causal link between a large unpredictable event and losses to an insurer E.g. Asbestos where exposure existed unknown for many years comes to light New cause of loss emerges that was previously regarded as not covered (environmental, CD) Regulator or court expands coverage by barring important exclusions New entrant into market reduces rates Systematic Risk: Risk operate simultaneously on a large # of policies, undiversifiable e.g. inflation Catastrophe Modeling Uncertainty Differences between commercial models and different versions of the same model Company not able to provide data required by the model Parameter risk and correlations 16.4.2.2 Reserving Risk Adverse development can be significant specially for long tailed lines \\(\\uparrow\\) reserve uncertainty \\(\\Rightarrow\\) \\(\\uparrow\\) capital requirement UPR should be part of u/w risk Likely the most important part of the model, including the uncertainty in the parameters Model should be able to be test for quality of fit measures 16.4.2.3 Asset Risk Focus on issues the specific market the company operates in Main asset classes: equities, fixed income, real estate FX and inflation are closely related to asset modeling as well Asset liability matching Can offset insurance risk and investment risk Doesn’t work well for P&amp;C to match the short duration of liabilities P&amp;C liabilities are inflation sensitive as well Efficient Frontier: \\(\\sigma\\) vs \\(\\operatorname{E}[Return]\\) See how it changes by modifying reinsurance structure or asset mix Modeling Considerations Bonds: Model with arbitrage free models Should capture historical features of bond markets (high auto-correlations and distn of yield spreads) Equities: Incorporate correlations with bonds Geometric Brownian motion model with additional volatility is more realistic FX: First model interest rates of the currencies then model the FX rates 16.4.2.4 Dependencies Important to capture dependencies or else the model will be unrealistically stable Sources of Dependencies: Macro econ conditions, affect many risks. Use ESG U/w cycle, loss trends and reserve development can impact across lines CAT and other extreme events like the 2008 financial crisis Difficult to quantify Use copulas to capture different forms of dependency Correlation is too simplistic and does not capture the dependencies in the tail 16.4.3 Setting Capital Requirements Policyholders wants more capital \\(\\Leftrightarrow\\) Shareholders want higher return (less capital) Set capital to maximize insurer’s value \\(\\Rightarrow\\) Unifies the competing requirements of prudence and efficiency Approaches to set capital requirements: 1. Default Avoidance: Focused on the tail and protecting policyholders Caveat: S/h will be hurt at much lower loss amount We have the least confidence in the tail of the model \\(\\hookrightarrow\\) More feasible and relevant to focus on thresholds less extreme than default 2. Lower Threshold than Probability of Ruin: Probability of downgrade Significant loss \\(\\leadsto\\) \\(\\downarrow\\) financial rating \\(\\leadsto\\) \\(\\downarrow\\) franchise value &gt; loss itself Sufficient capital to service renewals Survive and thrive after major CAT 16.4.4 Convert Probability to Loss Converting Probability Levels to Financial Loss Amounts Smallest amount of loss, \\(\\alpha \\%\\) of the time \\(\\Rightarrow\\) \\(VaR_{100 - \\alpha \\%}\\) Average loss, \\(\\alpha \\%\\) of the time \\(\\Rightarrow\\) \\(TVaR_{100 - \\alpha \\%}\\) 1-in-\\(x\\) years event loss no more than \\(y \\%\\) of capital \\(\\Rightarrow\\) \\(\\dfrac{100}{y} \\times TVaR_{1 - \\frac{1}{x}}\\) Important: Risk measures do not tell us how much capital to hold; They tell us for a given level of capital, what the risk to the company is 16.4.5 Risk Adjusted Performance Allocate capital to business units \\(\\Rightarrow\\) Calculate the rate of return \\(\\dfrac{\\text{Profit}}{\\text{Capital}}\\) for that unit \\(\\Rightarrow\\) Compare RoR across BUs Can also allocate risk capital to investments \\(\\Rightarrow\\) Compare riskiness of investments and u/w Need to make sure risk measures are consistent -->"],
["c2-era-2-1-corporate-decision-making-using-an-enterprise-risk-model-don-mango.html", "Chapter 17 C2 ERA 2.1 Corporate Decision Making Using an Enterprise Risk Model - Don Mango ", " Chapter 17 C2 ERA 2.1 Corporate Decision Making Using an Enterprise Risk Model - Don Mango "],
["cliffs-summary-11.html", "17.1 Cliff’s Summary", " 17.1 Cliff’s Summary 3 evolutionary steps in decision analysis: Deterministic \\(\\Rightarrow\\) Risk Analysis \\(\\Rightarrow\\) Cetainty Equivalent Argument that 2nd step is sufficient and reasons to move to the 3rd step Attribute cost back to source of risk for decision making Quantify corporate risk tolerance and what it is dependent on Spetzler: Benefits of utility curve Walla: Calculates efficient frontier \\(\\Rightarrow\\) Estimate risk tolerance \\(\\Rightarrow\\) Where on the frontier to select the best portfolio based on \\(R\\), utility curve and CE Allocating risk captial Cost benefit analysis with EVA or capital allocation Use a suite of decision metrics that are distinct and independent (to give different perspectives) and are responsive to different dynamics "],
["evoluation-of-corporate-decision-making-under-uncertainty.html", "17.2 Evoluation of Corporate Decision Making Under Uncertainty", " 17.2 Evoluation of Corporate Decision Making Under Uncertainty Decision making under uncertainty 3 evolutionary steps: Deterministic Project Analysis: Forecast CF, return metric (e.g. NPV) and sensitivity Risk Analysis: Disbtribution of inputs (e.g. Revenue) and distribution of outcome Certainty Equivalent: From 2) run the outcomes through a risk preference function of the company 17.2.1 Deterministic Project Analysis Single deterministic forecast of cash flow Return metric using NPV or IRR Sensitivity on inputs (Rev, exp, CoC) Sensitivity doesn’t have any probabilities for each scenarios Uncertainty is handled judgmentally 17.2.2 Risk Analysis Critical inputs have a distribution (w/ correlations) \\(\\hookrightarrow\\) Output also a distribution of NPV or IRR Uncertainty is handled judgmentally; Good portion has been moved into the distn Argument that step 2 is sufficient (Current best practice 2007): Since investors are only compensated for systemic risk and not firm specific risk, no need to focus on firm specific risk Based on portfolio theory where investors holds many different firm so the firm specific risk are diversified away \\(\\hookrightarrow\\) Only need to manage systemic risk Counter argument: Management can’t distinguish between systemic and firm risk Instantaneous risk (jump risk) is still present as information have a time lag I.e. some risk has no time lage Portfolio theory manages risk through discounting Market based information is too noisy for management to be able to do proper cost-benefit analysis and make trade off decisions 17.2.3 Certainty Equivalent Distribution output from risk analysis is input to a utility function for the firm Utility function is based on corporate risk policy Purpose is to express preference in a transparent and consistent manner \\(\\hookrightarrow\\) Automate and formalizes some of the risk assessment, while risk judgement is still required Reasons to move to step 3: Risk managers and owners are both interested in preserving franchise value \\(\\hookrightarrow\\) Want RM in place to protect FV \\(\\hookrightarrow\\) Both are interested in policy to help make risk management more objective, consistent, repeatable, and transaparent Market Value = Book Value + Franchise Value Franchise Value = Value of future earnings growth "],
["decision-making-with-an-irm.html", "17.3 Decision Making with an IRM", " 17.3 Decision Making with an IRM Focus on Element 5 below, 1-4 discussed else where Attribute Cost Back to Source of Risk Estimate aggregate loss distribution (4 Above) Distribution of outcomes for each LoB \\(\\Rightarrow\\) Correlate outcomes \\(\\Rightarrow\\) Correlate on external sources Quantify the impact of the loss outcomes on the organization Amount of P&amp;L or level of PHS; Can be a distribution Assign a cost to each amount of impact Utility function will be non linear \\(\\Rightarrow\\) Higher cost to events further out in the tail Attribute cost to source (e.g. BUs) 17.3.1 5a - Corporate Risk Tolerance Corporate risk tolerance How much risk a company is willing to take How much fluctuation in annual results it is willing to bear Needed in Steps 2 and 3 above Corporate risk tolerance is dependent on: Organization Size: Depending how much capital a firm have to deploy to invest in project each year changes how big an investment is Capital: The % impact to the firm’s total capital is more telling than the nominal value \\(\\Rightarrow\\) Firm with higher capital can tolerate bigger nominal value volatility Volatility: Investors might want steady stream of dividend paid \\(\\Rightarrow\\) Lower risk tolerance Public firms with quarterly earnings are rewarded for having consistent predictable profits \\(\\Rightarrow\\) Lower risk tolerance Private firm can afford to have more volatile results Risk Tolerance Important to define firm’s risk tolerance Can be defined by Risk Preference Function aka Utility Function Non linear e.g. \\(U(x) = A + B \\cdot \\operatorname{ln}[x+c]\\) or \\(U(x) = A - e^{ -x / R }\\) Slope decrease further into profit while increases further into losses 17.3.1.1 Spetzler “The Development of Corporate Risk Policy for Capital Investment Decisions” - Spetzler 1968 Paper found that managers have very different risk tolerate and found that they are likely to be too conservative for small projects (damages not meaningful to the firm as a whole) Useful for management to 1) see the different utility curves for different managers 2) make decisions on where the company utility curve should be and communicate that to the day to day decision makers Benefits of Utility Curve: Transparent, objective mathematical expression of the firm’s acceptable risk/reward trade offs Without, risk/reward decisions criteria will be inconsistent and opaque and driven by individuals 17.3.1.2 Walls “Combining decision analysis and portfolio management to improve project selection in the exploration and production firm” - Walls 2004 Calculates efficient frontier \\(\\Rightarrow\\) Estimate risk tolerance \\(\\Rightarrow\\) Where on the frontier to select the best portfolio Efficient Frontier Given \\(n\\) projects with \\(\\operatorname{E}[NPV_i]\\) and \\(\\sigma_i\\), the firm can choose to participate on each project with % \\(x_i\\) Given the budget based on current portfolio, an efficient frontier is built based on the lowest portfolio \\(\\sigma\\) given different expected NPVs They note that the current portfolio is not optimal The next 3 point is to decide where on the efficient frontier to be on Risk Tolerance Risk tolerance level \\(R\\) and utility function tells you where the firm choose to be on the efficient frontier \\(R\\) is estimated based on where the decision maker is indifferent from a gamble of 50% of \\(R\\) and 50% of \\(-R/2\\) and not taking it Utility Function \\(U(x) = 1 - e^{ - x / R}\\) Tell us how much risk is the firm willing to tolerate; How much reward are we willing to give up for a given reduction in risk and vice versa Certainty Equivalent With the above, calculate Certainty Equivalent (CE) of a given portfolio: CE = The fixed amount that the firm is indifferent between taking the risky portfolio or the fixed amount CE changes based on the \\(R\\) selected Pick the portfolio with the highest CE Negative CE means the firm would be better of not investing in it Firm must answer these questions: How much risk are we willing to tolerate (Picking \\(R\\)) How much reward are we willing to give up for a given reduction of risk and vice versa (Selection of utility curve) Are the risk-reward trade offs available along the efficient frontier acceptable to us (Answer by the first 2 points) 17.3.2 5b - Cost of Capital Allocated Still an open question *Risk Capital** Risk capital is a measure of firm’s total risk bearing capacity and is only an aggregate measure. It gives counterparty confidence that the financial firm can perform Allocation Cost of risk capital is being allocated, not the capital itself Allocation of risk capital is theoretical since no capital is actually transferred to the policy Useful to allocate risk capital to risk-taking units (and non risk-taking units too maybe) Total risk capital required is reduced by diversification benefits and the contributions to risk are not linear (See ERA 2.2) Difficult to allocate risk measure to different units, one method below: Return on Risk Adjusted Capital (RORAC) Allocate capital in a risk adjusted way \\(\\Rightarrow\\) Riskier sources require more capital Apply firm wide hurdle rate to determine cost of capital for each BU \\(\\neq\\) RAROC as RAROC adjusts the hurdle rate and does not risk adjust capital Bypassing Allocation Mango argues that concept of allocating capital is meaningless as each risk source has access to the capital of the firm Focus on cost of capital the risk source uses we get a direct answer Bypass allocation and goes straight to the cost How to determine the cost is the difficult part EVA EVA = Economic Value Added = NPV - Cost of Capital EVA &gt; 0 means BU is adding value 17.3.3 5c - CBA for Mitigation Cost-benefit analysis (CBA) EVA Mitigation effort that has positive EVA should be done Capital Allocation Projects that reduce capital cost by more than the cost of the project should be undertaken "],
["conclusion-1.html", "17.4 Conclusion", " 17.4 Conclusion Difficult to perform risk management for a firm based on a single metric Recommend a suite of decision metrics that are distinct and independent (to give different perspectives) and are responsive to different dynamics Parsimony dictates that reduce complexity as much as possible, but not more so \\(\\therefore\\) using a handful of risk metrics is acceptable -->"],
["c2-era-2-2-risk-measures-and-capital-allocation-g-venter.html", "Chapter 18 C2 ERA 2.2 Risk Measures and Capital Allocation - G. Venter ", " Chapter 18 C2 ERA 2.2 Risk Measures and Capital Allocation - G. Venter "],
["cliffs-summary-12.html", "18.1 Cliff’s Summary", " 18.1 Cliff’s Summary Know the risk measures Moment based Tail based (\\(VaR\\), \\(TVaR\\), \\(XTVaR\\), \\(EPD\\)) Probability transform Generalized moments Capital a company should hold depends on practical considerations, not simply derived from a risk measure Allocation of risk measures Proportional Co-measures Allocation is marginal if the change to the company’s risk measure from a small change in a single BU is attributed to that business unit Scalable risk measures Suitable alloaction A risk measure can have many co-measures but only 1 will be marginal Can bypass the above if we can just allocate the cost of capital "],
["introduction-6.html", "18.2 Introduction", " 18.2 Introduction VaR is too simplistic for insurance companies due to risk from reserves and u/w losses Advantages attributed to Economic Capital Unifying measure for all risks in an organization More meaningful than RBC or Capital Adequacy Ratios (Prem/Surplus, Res/Surplus) Forces firm to quantify risks it faces and combine them into a probability distn Provides a framework for setting acceptable risk levels for the company and its business units (One of the risk measure discuss in the paper will be econ capital (?) Not all are econ capital) Should consider multiple risk measures to see if the results are consistent Many other risk measures will also have the properties above Calling one of the risk measures economic capital is likely to confuse the process (I guess econ capital is just a generic term?) Current modeling approaches are not able to accurately estimate losses deep in the tail like 1-in-3000 (99.97%) event Bond ratings are discusses at this level but the ratings are not tied to the probabilities; Probabilities were simply observed Issue can be circumvented by focusing on events that can lead to impairment of the company, not just insolvency Choosing probability level is an artificial exercise \\(\\because\\) There are no theoretical support to choose one level over another The company sometimes compare held capital to the loss distn stating that it’s holding capital to cover a 1-in-x year event. But not the other way around like setting the capital based on it Company is better off expressing the capital under various risk measures "],
["risk-measures.html", "18.3 Risk Measures", " 18.3 Risk Measures 3 major class of risk measures Moment-based (e.g. mean, variance) Tail-based (VaR, TVaR) Probability Transformation (WTVaR, Mean of Transformed distn) Risk measure can measure losses (like from insurance) but mostly mean negative profit (including premium charged) here \\(Y =\\) Losses \\(\\rho(Y) =\\) Risk Measure 18.3.1 Moment-Based Measures \\(\\operatorname{E}[Y^k]\\) is the kth moment of \\(Y\\) which represent losses 1st moment = Mean 2nd central moment = Variance = \\(\\operatorname{E}[(Y-\\operatorname{E}[Y])^2]\\) S.d. is preferred as it has the same units as the loss 3rd moment = Skewness \\(\\operatorname{E}[Y e^{cY/\\operatorname{E}[Y]}]\\) captures all the moment using taylor expansion Does not exist unless there is a maximum possible loss 18.3.2 Tail-Based Measures at \\(\\alpha\\) Percentile Risk Measure Name Description \\(VaR\\) Value at Risk \\(\\alpha\\)th Percentile \\(TVaR\\) Tail Value at Risk Avg loss above \\(\\alpha\\)th percentile \\(XTVaR\\) XS Tail Value at Risk \\(TVaR - Mean\\) \\(EPD\\) Expected Policyholder Deficit \\((TVaR - VaR) \\cdot (1 - \\alpha)\\) Value of Default Put Option Market cost of insuring for losses over \\(VaR_{\\alpha}\\) \\(XTVaR\\) focuses on funding losses above the mean \\(EPD\\) = unconditional expected value of defaulted losses if Capital = \\(VaR_{\\alpha}\\) If there was no risk premium required, this is the cost of insuring for losses over \\(VaR_{\\alpha}\\) Value of default put is great than EPD, it includes an additional risk premium (market cost of insuring for losses over \\(VaR_{\\alpha}\\)) 18.3.3 Probability Transforms To recognize that large losses are worse for the firm in more than a linear way (e.g. losses twice as big is &gt; twice as worst) Change the loss distribution function by putting more weight (add probability) to the worst losses e.g. Esscher transform \\(f^*(y) = k \\cdot e ^{y/c} \\: f(y)\\) Lower \\(c\\) \\(\\Rightarrow\\) Higher losses from the transformed distn Can get non sensical results for \\(c\\) that’s too low \\(EPD\\) on a transformed distn can give us the value of the default put Comments Most asset pricing formulas like CAPM and Black-Scholes can be expressed as transformed means \\(\\therefore\\) transformed means are a promising risk measure for determining the market value of risk as it can also be the market value of the risk Complete market is where any risk can be sold, but we work in incomplete markets Theory of pricing in incomplete markets favors: Minimum Martingale Transform (MMT) Minimum Entropy Martingale Transform (MET) These give reasonable approximations of reinsurance prices Mean under then Wang transform closely approximates market prices for bonds and CAT bonds \\(WTVaR\\) is \\(TVaR\\) on a transformed distribution 18.3.4 Generalized Moments Include more than just \\(\\operatorname{E}[Y^k]\\) \\(TVaR_{\\alpha} = \\operatorname{E} \\left[ Y \\cdot \\left( Y &gt; F^{ -1}(\\alpha) \\right) \\mid Y &gt; F^{ -1}(\\alpha) \\right]\\) \\((Y &gt; b) = \\begin{cases} 1 &amp; : Y &gt; b\\\\ 0 &amp; : \\text{otherwise} \\end{cases}\\) Average of \\(Y\\) when \\(Y\\) &gt; than the \\(\\alpha^{th}\\) percentile: \\(F^{ -1}(\\alpha)\\) Spectral Measures \\(\\rho(Y) = \\operatorname{E} \\left[ Y \\cdot \\eta \\left( F(Y) \\right) \\right]\\) \\(\\eta \\geq 0\\) Multiply the loss by a non-negative scalar \\(\\eta(Y)\\) when taking the expectations Blurred VaR \\(\\eta(p) = \\operatorname{exp} \\left \\{ - \\theta (p-\\alpha)^2 \\right \\}\\) Give more weight to losses near \\(\\alpha^{th}\\) percentile Still using the whole distribution \\(\\theta\\) controls how quickly the weight drops of as we get further from \\(\\alpha\\) Which risk measures to use \\(TVaR\\) at a low percentile is fine as it captures all risks above this level; Set at level where we begin to loss capital Try to get close to market value of risk \\(WTVaR\\) with the MET is promising, or Exponential moment \\(\\operatorname{E}[Y e^{cY/\\operatorname{E}[Y]}]\\) "],
["required-capital.html", "18.4 Required Capital", " 18.4 Required Capital Capital a company should hold depends on practical considerations, not simply derived from a risk measure Risk measures should be used to measure the safety of the capital Considerations that drive capital requirements: Customers reactions: Well capitalized insurer \\(\\Leftarrow\\) vs \\(\\Rightarrow\\) better price \\(\\downarrow\\) rating \\(\\Rightarrow\\) \\(\\downdownarrows\\) of cumtomers since customers that want a higher level can quickly leave \\(\\uparrow\\) rating \\(\\Rightarrow\\) \\(\\upharpoonleft\\) of growth since higher rating just provides an opportunity to compete with other insurers that already have the business Keeping renewal business: Have enough capital so that in adverse scenario there is still enough capital to service renewals that is x% of the business Once capital level has been established, compare it against different risk measures \\(TVaR\\) is a better risk measure than \\(VaR\\) for a given percentile since \\(VaR\\) is just the smallest loss XS of a percentile, \\(TVaR\\) is the average "],
["capital-allocation.html", "18.5 Capital Allocation", " 18.5 Capital Allocation Allocate the company risk measure to each BU for the following purposes: Measure the amount of risk each BU contributes Set capacity and policy limits for each BU Calculate risk-adjusted profitability e.g. \\(\\dfrac{\\text{Profit}}{\\text{Allocated Capital}}\\) or EVA = Profit - Cost of Capital Definition: \\(Y\\): Losses of a company \\(X_j\\): Losses for each BU, \\(\\sum_j X_j = Y\\) \\(\\rho(Y)\\): Risk measure of \\(Y\\), \\(\\mapsto\\) a real number \\(r(X_j)\\): Allocation of risk measure to the individual BUs Usually different from \\(\\rho(X_j)\\), which is the risk measure applied to BU \\(j\\) 18.5.1 Proportional Method Allocate risk measure in proportion to the risk measure applied to each unit \\(r(X_j) = \\dfrac{\\rho(X_j)}{\\sum_i \\rho(X_i)} \\rho(Y)\\) \\(TVaR\\): \\(r(X_j) = \\operatorname{E}[X_j \\mid F(Y) &gt; \\alpha]\\); Condition on when the firm has losses XS the \\(\\alpha^{th}\\) percentile \\(VaR\\): \\(r(X_j) = \\operatorname{E}[X_j \\mid F(Y) = \\alpha]\\); Condition on when the firm has losses at the \\(\\alpha^{th}\\) percentile Difficult to calculate because it is estimating a single point in the distn, results from sims can be quite variable Typical you’re conditioning on the BU’s \\(\\alpha^{th}\\) percentile Not marginal if the condition is not on the firm’s \\(\\alpha\\) Marginal allocation means as unit grows, it is charged with the additional capital it requires Not marginal means the individuals don’t sum to the total (?) 18.5.2 Co-Measures \\(r(X_j) = \\operatorname{E}[h(X_j) \\cdot L(Y) \\mid g(Y)]\\) Given that: \\(\\rho(Y) = \\operatorname{E}[h(Y) \\cdot L(Y) \\mid g(Y)]\\) \\(h(\\cdot)\\) is additive: \\(h(u+v) = h(u) + h(v)\\) \\(\\hookrightarrow\\) \\(\\rho(Y) = \\sum_j r(X_j)\\) This is a marginal allocation that sums to the total risk measure Risk Measure \\(h(Y)\\) \\(L(Y)\\) Condition Co-Measure \\(TVaR\\) \\(Y\\) \\(1\\) \\(F(Y) &gt; \\alpha\\) \\(\\operatorname{E}[X_j \\mid F(Y) &gt; \\alpha]\\) \\(VaR\\) \\(Y\\) \\(1\\) \\(F(Y) = \\alpha\\) \\(\\operatorname{E}[X_j \\mid F(Y) = \\alpha]\\) Standard Deviation \\(Y - \\operatorname{E}[Y]\\) \\(\\dfrac{Y - \\operatorname{E}[Y]}{\\sigma_Y}\\) none \\(\\dfrac{\\operatorname{Cov}(X_j,Y)}{\\sigma_Y}\\) \\(XTVaR\\) \\(\\operatorname{E}[X_j \\mid Y &gt; b] - \\operatorname{E}[X_j]\\) Note for S.d.: \\(\\rho(X_j) = \\dfrac{\\operatorname{Cov}(X_j,Y)}{\\sigma_Y} = \\operatorname{E} \\left [ (X_j - \\operatorname{E}[X_j]) \\cdot \\dfrac{Y - \\operatorname{E}[Y]}{\\sigma_Y} \\right ]\\) For \\(XTVaR\\): \\(\\rho(Y) = \\operatorname{E}[Y - \\operatorname{E}[Y ]\\mid Y &gt; b]\\) Note the condition is on loss amount \\(Y\\) not the percentile Expected losses in unit \\(j\\) when the company has a loss above \\(b\\) less the expected losses in unit \\(j\\) At low threshold, unit with higher mean have higher allocation At higher threshold, unite with higher variance have higher allocation 18.5.3 Having a Marginal Method Marginal Allocation: Allocation is marginal if the change to the company’s risk measure from a small change in a single BU is attributed to that business unit Consistent with concept that price should be proportional to marginal cost Leads to suitable allocation Scalable Risk Measures: A small proportional change in the business change the risk measure by the same proportion Require that \\(\\rho(aY) = a \\cdot \\rho(Y)\\) \\(\\Rightarrow\\) Homogenous of degree 1 Most measures in currency units have this property (e.g. S.D. TVaR) but not variance and probability of default Scalability gives us that risk measure is both marginal and additive Proportional change can be achieve from proportional reinsurance and normal growth Except for companies that write large policies compared to their volume; Transformed risk measure might still work Suitable Allocation: Growing a unit that have above average profit/risk will increase the profit/risk for the company Given \\(\\dfrac{P_j}{r(X_j)} &gt; \\dfrac{P}{\\rho(Y)}\\) Show \\(\\dfrac{P + \\epsilon P_j}{\\rho(Y + \\epsilon X_j)} &gt; \\dfrac{P}{\\rho(Y)}\\) 18.5.4 Marginal Impact A risk measure can have many co-measures but only 1 will be marginal It is the one that is based on derivative of the firm risk measure w.r.t. growth in BU \\(j\\) \\(r(X_j) = \\lim \\limits_{\\epsilon \\rightarrow 0} \\dfrac{\\rho(Y + \\epsilon X_j) - \\rho(Y)}{\\epsilon} = \\dfrac{\\partial \\rho(Y)}{\\partial X_j}\\) Even under non-homogeneous growth, the risk measure is still decompositional (sum to the total) and is often close to marginal \\(XTVaR\\) Scalable when done XS of a percentile \\(\\alpha\\) \\(\\operatorname{E}[X_j \\mid F(Y) &gt; \\alpha] - \\operatorname{E}[X_j]\\) Variance \\(r(X_j) = \\operatorname{Cov}(X_j,Y)\\) Not scalable, so not marginal \\(VaR\\) \\(r(X_j) = \\operatorname{E}[X_j \\mid F(Y) = \\alpha]\\) Standard Deviation Spreading the s.d. \\(\\propto\\) mean is not marginal: \\(r(X_j) = \\dfrac{\\operatorname{E}[X_j]}{\\operatorname{E}[Y]} \\cdot \\sigma_Y\\) This is marginal: \\(r(X_j) = \\dfrac{\\operatorname{Cov}(X_j,Y)}{\\sigma_Y}\\) Exponential Moment \\(r(X_j) = r_1(X_j) + \\dfrac{\\operatorname{E}[X_j]}{\\operatorname{E}[Y]} \\cdot c \\cdot \\operatorname{E} \\left [ Y \\cdot e^{cY/\\operatorname{E}[Y]} \\cdot \\left \\{ \\dfrac{X_j}{\\operatorname{E}[X_j]} - \\dfrac{Y}{\\operatorname{E}[Y]} \\right \\} \\right ]\\) Without the curly bracket it is just allocating \\(\\propto\\) the mean Curly bracket (XS ratio factor) adjust for correlation, where BU that have large \\(X_j\\) when Y is also large will have additional allocation; UBs that don’t contribute to large losses will be negative in the curly bracket, receive reduced allocation EPD \\(r(X_j) = (CoTVaR - CoVaR) \\cdot (1-\\alpha)\\) Only works when XS of a percentile Table of Co-Measures Risk Measure \\(h(Y)\\) \\(L(Y)\\) Condition Co-Measure \\(TVaR\\) \\(Y\\) \\(1\\) \\(F(Y) &gt; \\alpha\\) \\(\\operatorname{E}[X_j \\mid F(Y) &gt; \\alpha]\\) \\(VaR\\) \\(Y\\) \\(1\\) \\(F(Y) = \\alpha\\) \\(\\operatorname{E}[X_j \\mid F(Y) = \\alpha]\\) Standard Deviation \\(Y - \\operatorname{E}[Y]\\) \\(\\dfrac{Y - \\operatorname{E}[Y]}{\\sigma_Y}\\) none \\(\\dfrac{\\operatorname{Cov}(X_j,Y)}{\\sigma_Y}\\) \\(EPD\\) \\(X - \\operatorname{E}[X \\mid F(Y) = \\alpha]\\) \\(1-\\alpha\\) \\(F(Y) &gt; \\alpha\\) \\((CoTVaR - CoVaR) \\cdot (1-\\alpha)\\) 18.5.5 Using Decomposition If the allocated risk measure \\(r(X_j)\\) is a decomposition of the company risk measure \\(\\rho(Y)\\) then we can use the measure to measure risk-adjusted profitability of a BU \\(\\dfrac{P_j}{r(X_j)}\\) We also need \\(r(X_j) \\propto\\) [Market Value of Risk] for this to be an appropriate metric BU with higher ratio have more profit relative to the value of risk they are taking However, we don’t know how to determine the market value of risk \\(\\Rightarrow\\) Use several risk measures and hope that one is close and that the indicated strategic directions are consistent with each other Question on how to determine the market value of risk have not been settle yet, likely a risk measure on a transformed probability but we don’t know yet "],
["allocating-the-cost-of-capital.html", "18.6 Allocating the Cost of Capital", " 18.6 Allocating the Cost of Capital Compare profitability among the BU by allocate a cost of capital for each BU 2 possible definitions for cost of capital \\(C_j\\) Option pricing: Value of the right to call upon the capital of the firm Difficult to value as the timing is not fixed in advanced Market value of a stop loss that attach at zero profits for the BU Could use the expected value of the stop loss on transformed probabilities MET is recommended Practical estimate: Mean + 30% of s.d. would apply a consistent risk load to the BU "],
["summary.html", "18.7 Summary", " 18.7 Summary Allocating capital, even using marginal decomposition is arbitrary and artificial Arbitrary because different risk measures give us different allocations and therefore different answers Artificial because the business unit has access to the entire capital of the firm, not just a portion of it Using a value added approach, by allocating cost of capital is more economically realistic -->"],
["c2-era-2-3-regulatory-and-rating-agency-capital-adequacy-models-witcraft.html", "Chapter 19 C2 ERA 2.3 Regulatory and Rating Agency Capital Adequacy Models - Witcraft ", " Chapter 19 C2 ERA 2.3 Regulatory and Rating Agency Capital Adequacy Models - Witcraft "],
["cliffs-summary-13.html", "19.1 Cliff’s Summary", " 19.1 Cliff’s Summary Talks about different ways regulator regulates company’s capitla adequacy Leverage ratios were used in the early days In the 1990s RBC was introduced that combie Credit Reserve Accumulation Scenario testing Against scenarios from regulators Can be stochastics 2 options to meet capital requirements: Purchase reinsurance Issue surplus notes "],
["introduction-7.html", "19.2 Introduction", " 19.2 Introduction Prior to 1990: Leverage Ratios After 1990: Risk Based Capital Model (RBC) Soon after RBC: Scenario Testing More recently: Stochastic Scenario Testing "],
["leverage-ratios.html", "19.3 Leverage Ratios", " 19.3 Leverage Ratios In early 1970s in US if companies fail 4 of 12 IRIS tests will get regulatory scrutiny In US prior to the 1990s, 2 leverage ratio tests were used: \\(\\dfrac{\\text{Premium}}{\\text{Surplus}} &lt; 3.0\\) \\(\\dfrac{\\text{Reserves}}{\\text{Surplus}} &lt; a\\) where \\(a\\) is fixed In EU Solvency I are equivalent to the higher of \\(\\dfrac{\\text{Premium}}{\\text{Surplus}}\\) and \\(\\dfrac{\\text{Incurred Claims}}{\\text{Surplus}}\\) And also net leverage \\(\\dfrac{\\text{Premium} + \\text{Reserves}}{\\text{Surplus}}\\) Does not distinguish between LoBs "],
["risk-based-capital-models.html", "19.4 Risk-Based Capital Models", " 19.4 Risk-Based Capital Models Advantagtes Combining several risk include asset, credit, premium, reserve Factor models: Factors vary with the quality and type of asset or LoB Factors applied to accounting values Used in UK, AUS, US, CA, JAP, AM Best and S&amp;P Models recognize accumulation risk (CATs) and multiple events instead of just occurrence amounts (most of them 1-in-100 or 1-in-250) AM Best has much higher factors than the rest: Rating agency focus on long term viability vs regulatory focus on one year survival Correlation adjustment reduces the combined risk charge especially when the factors are of similar size 19.4.1 Credit Risk Largest component is reinsurance recoverable Many models vary the factors with the credit quality of the reinsurers AM Best increases credit charge for companies with high Reinsurance Recoverable / Surplus ratio UK: premium ceded to one reinsurer can not &gt; 20% of gross premium; Recoverable from an insurance group cannot &gt; 100% of surplus 19.4.2 Reserve Risk Similar to premium factors, vary by LoB and applied to net reserves Japan: reserve levels are low as payments are made quickly, factor is applied to net loss payment 19.4.3 Accumulation Risk Some models have started to use accumulation risk but many do not use it yet Focus is on 1-in-100 or 1-in-250 events "],
["scenario-testing.html", "19.5 Scenario Testing", " 19.5 Scenario Testing Some regulators require test of capital against scenarios Static scenarios in Canada (DCAT) Stochastic in UK and Australia with 99.5% of 1 year survival rate UK also have 3 years 98.5% and 5 year 97.5% survival Stochastic modeling requires: Forecasting financials for 1-5 years Probability distn where they can be developed Dependencies among risks Reflection of Management Responses "],
["evaluating-capital-structure-strategies.html", "19.6 Evaluating Capital Structure Strategies", " 19.6 Evaluating Capital Structure Strategies 2 options to meet capital requirements: Purchase reinsurance Impact: Premium \\(\\downarrow\\); Capital Requirement \\(\\downarrow\\) Reserves \\(\\downarrow\\); Capital Requirement \\(\\downarrow\\) Reinsurance Recoverable \\(\\uparrow\\); Capital Requirement \\(\\uparrow\\) Recoverables change is about 25% of the premium and loss reserve charge Change in premium or reserve charge have a larger impact then to the recoverable charge due to the covariance adjustment Changes to small risk charges have a small impact on the total risk charge Annual cost: Ceded Premium \\(\\times\\) Profit Margin Issue Surplus Notes Annual cost: (Surplus note yield - Bond yields) \\(\\times\\) Face Value of Surplus Note However surplus note is a longer commitment as they can not be repaid for a certain time period Reinsurance might be cheaper and the company can exit quickly -->"],
["c2-era-2-4-asset-liability-management-brehm.html", "Chapter 20 C2 ERA 2.4 Asset-Liability Management - Brehm ", " Chapter 20 C2 ERA 2.4 Asset-Liability Management - Brehm "],
["cliffs-summary-14.html", "20.1 Cliff’s Summary", " 20.1 Cliff’s Summary Asset liability Management: Looks at current assets and liabilities + flow from future premiums of a going concern company Identify and exploit hedges of any sort (e.g. use equity as inflation hedge) Optimal portfolio under 4 different scenarios Going concern is most complicated as it includes cash flow coming in Additional considerations: Tax: tax exempt bonds is preferred during profitable times Equity: risky but useful to hedge inflation If we are not matching the duration, best to just invest in long bonds 9 steps of asset liability modeling Model Asset Classes, Liabilities, and Current Business Operations Define Risk Metrics Return Metrics Time Horizon Consider Relevant Constraints Simulation Model Efficient Frontier Graph Liabilities Review Results Areas for future research are correlations and reserve models "],
["introduction-8.html", "20.2 Introduction", " 20.2 Introduction Asset-Liability Management \\(\\neq\\) Matching ALManagement: Looks at current assets and liabilities + flow from future premiums of a going concern company Identify and exploit hedges of any sort (e.g. use equity as inflation hedge) ALMatching: focus on hedging interest rate risk by matching duration or cashflow matching Insurance liabilities are less liquid than investments \\(\\leadsto\\) Focus mostly on investment With that said, still consider u/w and other hedges like reinsurance on the liabilities "],
["optimal-porfolio-under-4-scenarios.html", "20.3 Optimal Porfolio Under 4 Scenarios", " 20.3 Optimal Porfolio Under 4 Scenarios 1) Assets (No Liabilities) Porfolio Short term treasures: risk free and maintain value well Long bond and stocks are risky Use modern portfolio theory to find alternatives on the efficient frontier 2) Known Liabilities and Known Cash Flows (Fixed in Time and Amount) Short duration \\(\\Rightarrow\\) Reinvestment risk Long duration \\(\\Rightarrow\\) Interest rate risk Risk and conclusions are different if liabilities are discounted at current rates (Such as using Economic balance sheet) 3) Liabilities are Variable and Timing is Variable Precise matching is impossible Requires model that incorporate asset and liability fluctuations Inflation sensitive liabilities further complicates things 4) Going Concern Company, with Cash Flow from Current Operations Can have positive or negative cash flow If asset prices are too low, can pay claims from premium cash flows and use depreciated assets to support loss reserves Need to model: premium, income, losses, cat losses, expenses Need enterprise wide model with holistic view to handle the complexity 20.3.1 Additional Considerations Taxes Investment strategy change in responses to u/w cycle Profitable period: tax exempt bonds are better Unprofitable period: taxable bonds are better as the investment income is reduced by u/w losses Assets is also reallocated to maximize income while avoiding Alternative Minimum Tax Equities Generally considered risky and imply a potentially worst downside risk to capital However, they can be useful hedge against inflation Can be tested with an ERModel, but conclusions will be sensitive to input assumptions of the macroeconomic model VFIC 2002 Published by CAS on the optimality of duration matching assets &amp; liabilities Simulation against: Long tailed business vs short tailed business (w/ CAT) Profitable vs unprofitable (Generate cash and consumes cash) Growing vs shrinking companies Reviewed on both GAAP, Stat, Economic B/S basis and several risk measures from each accounting basis, findings: Duration matching was one of the optimal strategies Investment choice depends on: Risk metrics selected Return metrics (Venter used US GAAP pretax \\(\\Delta\\) in surplus) Risk tolerance or preference Results: Economic Balance Sheet: Assets are mark to market, liabilities are discounted at current rates Duration match results in low interest rate risk Longer duration is better if not matched: Long duration investments: \\(\\uparrow\\) interest rate risk \\(\\uparrow\\) returns Short duration investments: \\(\\uparrow\\) reinvestment rate risk and \\(\\downarrow\\) return Duration match is irrelevant for Stat and GAAP accounting as they are not responsive to interest rate movement Adding cash flows from continuing operations complicates this analysis "],
["asset-liability-modeling-approach.html", "20.4 Asset-Liability Modeling Approach", " 20.4 Asset-Liability Modeling Approach 9 steps for enterprise-wide model for ALManagement 1) Model Asset Classes, Liabilities, and Current Business Operations Asset classes: interest rate model, stock price model, FX model Liabilities: response to inflation/interest rate or economic variables like GDP growth Business operations: premium, expense, response to u/w cycle and economic environment 2) Define Risk Metrics Using different accounting basis: Stat, GAAP, Economic Income based metrics \\(\\sigma\\) of income QtQ Probability of not meeting earning target \\(\\Delta\\) in surplus or equity B/S based metrics Focus on level of surplus or equity of firm \\(\\sigma\\) and probability of not meeting target VaR, TVaR, WTVaR Probability of ruin or impairment Time Frames Typically 1-5 years 3) Return Metrics Use consistent accounting basis as the risk metrics Income based: Quarterly earnings B/S based: RoE, Terminal value of equity at period end 4) Time Horizon Single period is simpler Multiperiod is more accurate More difficult Include serial correlations of variables: Interst rates Level of prices in market 5) Consider Relevant Constraints Asset limits imposed by regulators Cost of regulatory capital of asset class RBC or BCAR capital scores Company’s own investment policy 6) Simulation Model Consider and varies: U/w strategies Reinsurance options Investment strategies Risk and return metrics are calculated over these simulations 7) Efficient Frontier Graph Construction from various portfolio options Based on current portfolio, options with same return but less risk or higher return with same risk should be consider (in between current and the frontier is fine as well) 8) Liabilities Liabilities can be modify in particular future loss reserves \\(\\Delta\\) u/w strategies Reinsurance Should analyze various reinsurance structure and compare results Important in multiperiod model 9) Review Results Identify situations that the preferred portfolio performed poorly Develop hedging strategy for those situations Review may highlight prevailing conditions that lead to substandard performance e.g. large CAT loss that forces liquidation of assets in soft market Can establish monitoring mechanisms to identify the likelihood of such conditions and make adjustments when they are noticed "],
["future-research.html", "20.5 Future Research", " 20.5 Future Research Correlations: Between LoBs; asset &amp; liabilities; over time (low most of the time and high in a crisis) Can materially alter the optimal portfolio Models of unpaid losses are not explanatory model Forecast mean and distribution but, Do not predict loss payments based on economic variables (interest rates, inflation, GDP) Sometimes inflation is hypothesized but rarely explicitly developed from the historic economic data \\(\\hookrightarrow\\) Model is unable to use the results of ESG to make meaningful predictions Asset model do this better (equity, bond pricing) Alternative is to treat the correlation parameters and inflation sensitivity as random variables, then models can be created with parameter estimates -->"],
["c2-era-2-5-measuring-value-in-reinsurance-venter-gluck-brehm.html", "Chapter 21 C2 ERA 2.5 Measuring Value in Reinsurance - Venter, Gluck, Brehm ", " Chapter 21 C2 ERA 2.5 Measuring Value in Reinsurance - Venter, Gluck, Brehm "],
["cliffs-summary-15.html", "21.1 Cliff’s Summary", " 21.1 Cliff’s Summary Value of reinsurance Provides stability Frees up capital Adds to the value of the firm Quantify stability metrics Based on reinsurance premium and expected recoveries Amount of protection Premium - loss Distributions Efficient frontier Reinsurance as capital, 2 metrics: Net Benefit = |\\(\\Delta\\)Cost of Capital| - |Net Cost of Reinsurance| Requires &gt; 0 \\(ROE = \\dfrac{\\text{Net Cost of Reinsurance}}{\\Delta \\text{Capital}}\\) Requires &lt; cost of capital Capital can be based on theoretical or practical model Accumulation risk: as-if reserve Measure the impact on the value of the firm Difficult to do but some recent study results "],
["introduction-9.html", "21.2 Introduction", " 21.2 Introduction Value of reinsurance Provides stability Frees up capital Adds to the value of the firm Naive calculation: Sum(cash flow over contract length) (-) Premium and reinstatement (+) Ceding comm and reinsurance loss \\(\\hookrightarrow\\) Will always show reinsurance is a bad deal especially after discounting the cashflow Make sense since reinsurers need to make a profit Cases where it’s “profitable” to the cedant, it’ll either be repriced or the cedants results are really poor "],
["quantifying-stability-and-its-value.html", "21.3 1. Quantifying Stability and Its Value", " 21.3 1. Quantifying Stability and Its Value Measuring the value of reinsurance is more than calculating the expected cashflow from the reinsurance program Significant judgement is required to evaluate the cost-benefit tradeoff Metrics to measure and value stability: Ratio and difference of \\(\\text{Reinsurance Premium}\\) and \\(\\operatorname{E}[Recovery]\\) Expected loss under different programs \\(Premium - \\operatorname{E}[Loss]\\) under different programs \\(\\sigma\\), percentiles, whole distribution Space needle view, Distribution of other metrics cost benefit diagram, pre tax net income, combined ratio Efficient frontier charts 21.3.1 Reinsurance Premium and Expected Recoveries Metrics 1: Ratio = \\(\\dfrac{\\operatorname{E}[Recovery]}{\\text{Reinsurance Premium}}\\) Metrics 2: Net Cost of Reinsurance = \\(\\text{Reinsurance Premium} - \\operatorname{E}[Recovery]\\) On a PV basis? Notes: \\(\\operatorname{E}[Recovery]\\) is net of reinstatement premiums, typically based on simulation results Should gauge how significant the net cost of reinsurance is by comparing to the firm’s expected earnings for the year 21.3.2 Amount of Protection Metrics 3: Compare expected net loss \\(\\forall\\) programs 21.3.3 Premium Less Expected Loss This is Net Premium - Net Expected Loss Metrics 4: Compare \\(Premium - \\operatorname{E}[Loss]\\) \\(\\forall\\) programs Does not include expenses or investment income Metrics 5: \\(\\sigma_{Premium - \\operatorname{E}[Loss]}\\) Caveat: Can be lowered by removing favorable outcomes Metrics 6: 1st percentile (most favorable) \\(Premium - \\operatorname{E}[Loss]\\) Metrics 7: Worst simulated outcome \\(Premium - \\operatorname{E}[Loss]\\) Might be too extreme to look at 1-in-25K (99.996%) Notes: Here we are sticking to high percentile means bad, similar to how we used to look at loss, but flip it for looking at earnings 21.3.4 Distribution Based Density of \\(Premium - \\operatorname{E}[Loss]\\) Compare shapes for different programs See if it’s giving up the upside and like how does the tail look CDF of \\(Premium - \\operatorname{E}[Loss]\\) Pick a percentile and read it across the graph to find the value at that percentile for each curve Look at the difference between different programs at each percentile Note on difference of distribution: Event that generate a given percentile is different across programs Reinsurance changes the order of the outcomes We are interested in the difference of the distributions NOT interested in the distribution of differences (as it doesn’t make sense?) Goal here is the choose a reinsurance program and its associated distribution \\(\\Rightarrow\\) More useful to look at the distn themselves Interested in the cost-benefit trade off, cost is the net cost of reinsurance and benefit is the protection against adverse deviation Space Needle View on \\(Premium - \\operatorname{E}[Loss]\\) Shows different percentiles Colored section is proportional to the probability that the result is in that range Easy to compare each quantile across programs Cost Benefit Diagram Cost of reinsurance (for each program) vs loss @ each percentile Pre-Tax Net Income Include expenses and investment income Look at value at each iterations of the sim (percentile) Get a perspective on overall profitability e.g. probability of negative earnings CDF on Combined Ratio Caveat: Can give distorted results when the net premium is reduced due to significant ceded premium Expense ratio will be higher due to lower net premium (denominator) Can make results look worst for program that have big cession 21.3.5 Efficient Frontier Charts Plot the risk metric vs return metric for different reinsurance structure options and at different percentile and then graph out the efficient frontier as well Plot increasing risk to the right so it looks more familiar Look for programs that are up and left along the efficient frontier with the lowest risk metric for a given return metric Return metrics: Earnings Risk metrics: Probability of making plan Probability of: surplus &lt; 2 \\(\\times RBC\\); surplus &lt; BCAR score supporting a target rating; Expected loss in a 10 year period, \\(TVaR_{90\\%}\\), exceeds a threshold level or surplus (\\(TVaR_{90\\%}\\) should not exceed 20% of surplus); x% drop in quarterly EPS "],
["reinsurance-as-capital.html", "21.4 2. Reinsurance as Capital", " 21.4 2. Reinsurance as Capital Capital is held to pay for claims when the premium is not sufficient to do so Highly variable results will require more capital \\(\\Downarrow\\) Reinsurance reduce volatility of results \\(\\Rightarrow\\) reduce capital required \\(\\Downarrow\\) Reinsurance can be thought of as borrowed capital We value the reinsurance by the impact it has in reducing Capital, or Cost of Capital. We compare these reductions to the cost of the reinsurance Measure the amount of capital reduction that the insurer receives from the reinsurance program Compare the cost of capital reduction from reinsurance program Metrics 1 Net Benefit = |\\(\\Delta\\)Cost of Capital| - |Net Cost of Reinsurance| Net benefit &gt; 0 then accept Net cost of reinsurance = \\(\\text{Reinsurance Premium} - \\operatorname{E}[Recovery]\\) on a PV basis Alt formula: Net benefit = Net Cost of Reinsurance - \\(\\Delta\\)Cost of Capital Review other criteria when the dollar benefit is the same Metrics 2 \\(ROE = \\dfrac{\\text{Net Cost of Reinsurance}}{\\Delta \\text{Capital}}\\) Compare return on equity We want \\(ROE\\) below the cost of capital here and that lower is better since we are reducing capital Think about it if you’re going from the plan interested to gross, if the \\(ROE\\) is lower you don’t want to change so the current plan is better 21.4.1 Change in Capital 2 different methods to determine this change 21.4.1.1 Theoretical Models A risk metric is selected as a proxy to represent the economic capital required e.g. \\(VaR\\), \\(TVaR\\), multiples of \\(VaR\\), \\(XTVaR\\), etc The risk metric is measured for each reinsurance program and the \\(\\Delta\\) in capital can be measured against a base (bare or current) Set the economic capital = Net Premium - Expenses - risk metric ROE here is calculated as the \\(\\dfrac{\\Delta \\text{Expected Income}}{\\Delta \\text{Capital}}\\) Check this might be same, again we want it to be low 21.4.1.2 Practical Models Use capital requirements from rating agencies like BCAR, S&amp;P, CAR, RBC, ICAR, etc Typically use a multiple of a regulatory metric Reinsurance reduces the capital required due to lower net premium and net reserves with a slight offset from increase counterparty risk Pros: Easier to implement (no need to model loss distn and correlation) Cons: Not as accurate because the measurement of risk is based on proxies and not the actual risk itself e.g. stop loss only have small impact on premium but can have large impact on a risk (like the tail) that the method might not pick up Alternatively, model a year out and predict the probability of falling below certain thresholds \\(\\Rightarrow\\) Capital could be set so that the probability is lower But then we’ll have to model the I/S and reinsurance structure and everything 21.4.2 Marginal ROE Compare the marginal cost of reinsurance to the marginal capital to judge the value of reinsurance Pick a risk metric as proxy for capital required Calculate marginal capital relative the current program Calculate \\(\\Delta\\) NPV Net Benefit = \\(\\Delta\\) NPV Ceded Loss - \\(\\Delta\\) NPV Ceded Premium Calculate \\(ROE = \\dfrac{\\Delta \\text{NPV Net Benefit}}{\\text{Marginal Capital}}\\) Calculate after tax \\(ROE\\) Compare the \\(ROE\\) Not sure high or low we want Bare in mind that a high ROE is meaningless if the amount of capital you can invest is small 21.4.2.1 XTVaR Might be better to look at a multiple of XTVaR at a lower percentile XTVaR is the unfunded part of TVaR VaR is a single point and can be volatile at high percentiles 21.4.3 Accumulation Risk So far we implicitly assumed that the capita is held for only 1 year Capital calculated was static without regard to how long it takes to run it down as claims are paid Capital needs to be held until all claims are paid Consider length of time that capital is held Look at “lifetime” amount of capital = capital for new business + capital to support all unpaid claims from prior years Set the capital requirements based on premium and reserves As-if Reserves Reserves on prior AYs maybe based on business that is different from this year’s book \\(\\hookrightarrow\\) Use “as-if” reserves for the capital calculation Calculate the reserves as if the company had been writing the same book the last several years (and only differ by inflation?) Important for long tailed lines where capital requirement for the accumulated o/s reserve can be much large than the requirement for new business 2 advantages for the “as-if” reserves Measure the impact of accumulated risk caused by correlated factors Likely means correlated factors that can influence all those accident years e.g. inflation, change in law Alternative reinsurance programs can be applied to the premium and as‐if reserves, providing a valid measure of the accumulated risk and capital used over the full life of the AY Capital calculated for both new premium and the loss reserves is a surrogate for the PV of the capital required for the current book over its lifetime When aggregating AYs, we get temporal risk (risk based on time) that affect all AYs at the same time e.g. severity trend and auto correlation of severity trend Including all AYs create a bigger spread of possible outcomes 21.4.3.1 Capital Consumed How much capital is needed to pay deficiencies Plot PDF and CDF of capital consumed: Capital Consumed = - PV[Premium + Reserves - Losses - Expenses] When modeling XoL we need to model severity trend As it impacts all open claims Payment pattern is important as claims that are open longer will have more severity trend applied We can base the capital consumed on different risk metrics like VaR and TVaR ROE = Expected Profit / Capital Consumed We want to see increase in ROE Make sure understand why Comment from notes that author is not sure about "],
["reinsurance-and-market-value.html", "21.5 3. Reinsurance and Market Value", " 21.5 3. Reinsurance and Market Value Measure the impact on the value of the firm instead of the impact on capital Don’t know how this can be done Currently: we measure value of the stability that reinsurance provides by measuring the reduction in capital Next step: Measure the value of reinsurance by measuring the \\(\\Delta\\) in the value of the company Some conclusion from recent studies Insureds demand price discounts of 10-20 times the expected cost of an insurer default (EPD) e.g. 0.5% default, expected cost = 100K if default \\(\\Rightarrow\\) EPD = 500 \\(\\Rightarrow\\) Discount demand = 500 $10 or 20 = 5-10K 1% decrease in capital \\(\\Rightarrow\\) 1% loss in pricing level 1% increase in \\(\\sigma_{earnings}\\) \\(\\Rightarrow\\) 0.33% decrease in pricing level Rating upgrades is worth 3% of business growth Rating downgrades can drop business by 5-20% "],
["conclusion-2.html", "21.6 Conclusion", " 21.6 Conclusion Use cost-benefit analysis to compare reinsurance programs Select metrics to determine the benefit of the reinsurance and compare to cost for each reinsurance programs Net cost of reinsurance = NPV of expected decrease in earnings from purchasing reinsurance = PV[Expected loss - Premium - Reinstatement + Commission] Combined ratio can be distorted due to expense ratio Stability is the simplest measure of benefit; but \\(\\sigma\\) can be misleading as they can be lowered by eliminating favorable results Useful to look at the differences of distributions from the reinsurance options Efficient frontier is useful If we can measure the increase of value of the firm that would be great Increased earnings from reduced financing costs Higher premium from better claims paying ratings Can’t do this right now so the substitute is to measure the reduction in capital requirements Value of reinsurance can be measures vs a cost of capital or vs capital (ROE) -->"],
["c3-era-3-1-considerations-on-implementing-internal-risk-model-mango.html", "Chapter 22 C3 ERA 3.1 Considerations on Implementing Internal Risk Model - Mango ", " Chapter 22 C3 ERA 3.1 Considerations on Implementing Internal Risk Model - Mango "],
["cliffs-summary-16.html", "22.1 Cliff’s Summary", " 22.1 Cliff’s Summary Just know the 4 sections and have an idea of what it covers "],
["introduction-10.html", "22.2 Introduction", " 22.2 Introduction Creation of internal risk model Decisions at the beginning of the process How the input parameters should be determined How to overcome political hurdles How to integrate model into the company’s business planning "],
["startup-staffing-and-scope.html", "22.3 Startup: Staffing and Scope", " 22.3 Startup: Staffing and Scope Important to define responsibilities of those involved and scope of the project Organization Chart Reporting line of team Leader needs reputation for fairness and balance Functions Represented U/w, planning, actuarial, risk Resource Commitment Full time or part time Critical Roles and Responsibilities Control of inputs and outputs Analyses and uses of output Purpose Quantify variation around? Objective view of distn of results? Scope Prospective u/w year only? Reserves, assets, op risk? Low detail on whole company? or, High detail on a pilot segment "],
["irm-parameter-development.html", "22.4 IRM Parameter Development", " 22.4 IRM Parameter Development Important input parameters to model: ELR LR distn Expected premium Correlation amongst risk ESG Other considerations: Modeling software: should match the capabilities of the team Parameter development: should include expertise from u/w, planning, claims and actuarial; have a systematic way to capture expert opinion Correlations: Have to be owned at a high level (C-suites) as it crosses LoBs and have significant impact on the allocated capital; Should have inputs from IRM team Validation: test and validate over extended period; provide training so that interested parties all have a basic understanding of the statistics "],
["implementation.html", "22.5 Implementation", " 22.5 Implementation Have senior management set priority and timeline for IRM analysis and rollout to prevent ambushes Give opinion leaders reasonable time frames to raise their concerns and have them addressed and not the power to veto or delay the rollout Don’t attempt to sell the concept to various opinion leaders before moving forward There will be a large appetite for the results of the IRM Recommendations Priority Setting: Have top management set the priority for implementation Communications: Regular communication and to a broad audience Pilot Testing: Allows effective preparation of the company for the magnitude of the change Education: Training so leadership has a similar base level of understanding "],
["integration-and-mainteneance.html", "22.6 Integration and Mainteneance", " 22.6 Integration and Mainteneance Make it part of the corporate calendar Cycle: Integrate into annual planning process Updating: Major updates no more than once per year Controls: Maintain centralized controls of inputs, outputs and even application templates -->"],
["c3-era-3-2-modeling-parameter-uncertainty-venter-gluck.html", "Chapter 23 C3 ERA 3.2 Modeling Parameter Uncertainty - Venter, Gluck ", " Chapter 23 C3 ERA 3.2 Modeling Parameter Uncertainty - Venter, Gluck "],
["cliffs-summary-17.html", "23.1 Cliff’s Summary", " 23.1 Cliff’s Summary Parameter risk is more important as the process risk is smaller Different ways to model the severity trend Model the trend indepedently or adjust the general inflation then model the superimposed inflation Model with AR(1) Estimation risk in selecting the parameters and the downstream impact Use MLE working with negative log likelihood Slope of the negative log likelihood determines our confidence in the selection Use HQIC to adjust the loglikelihood with a penalty for # of parameters Also use a pool of distribution of reasonable parameters Form of the mdoel we use to do the modeling e.g. expected parameters vs pool of distribution of parameters Should focuse on the spread of potential outcomes instead of best estimate Add flexibility and let the parameters reflect the uncertainty in the assumptions "],
["introduction-11.html", "23.2 Introduction", " 23.2 Introduction Parameter risk is key for large companies since process risk is reduced with a large book Model projection risk, measure estimation risk in parameter selection and discuss model risk "],
["impact-of-parameter-risk.html", "23.3 Impact of Parameter Risk", " 23.3 Impact of Parameter Risk CoV for total losses: \\(CV^2(S) = \\underbrace{CV^2(N)}_{\\text{# of Claims}} + \\underbrace{\\dfrac{CV^2(X)}{\\mu_N}}_{\\text{Severity}}\\) For large company, \\(\\mu_N\\) is large so the 2nd term is small However if we add systemic factor that impacts all claims (e.g. inflation), it doesn’t diversify away as the # of expected claims \\(\\uparrow\\) For small company there is a large amount of process risk so the overall variability does not increase very much "],
["projection-risk.html", "23.4 Projection Risk", " 23.4 Projection Risk Different ways to project trends 23.4.1 Simple Trend Forecast one trend historically and into the future Additional uncertainty due to estimate of ultimate losses is uncertain 23.4.2 Severity Trend and Inflation Claim severity \\(\\neq\\) general inflation [Claim Severity Trend] \\(\\approx\\) [General Inflation] + [Superimposed Inflation] 2 approaches: Model the severity trend independently Adjust the data for general inflation and model the residual superimposed inflation For ERM where general inflation is modeled, severity trend should be dependent on the general inflation 23.4.3 Trend as a Time Series More realistic, allow for trends to change over time AR(1) with mean reverting form: \\(\\begin{array}{ccccc} r_{t+1} &amp;= &amp;m + \\alpha_1(r_t - m) &amp;+ &amp;\\epsilon_{t+1} \\\\ &amp;= &amp;(m - \\alpha_1 m) + \\alpha_1 r_t &amp;+ &amp;\\epsilon_{t+1} \\\\ &amp;= &amp;\\alpha_0 + \\alpha_1 r_t &amp;+ &amp;\\epsilon_{t+1} \\\\ \\end{array}\\) \\(m\\) is the long term mean estimated from data \\(\\epsilon_t \\sim N(0,\\sigma)\\) Simple trend understate the projection risk especially for long tail LoBs Simple trend had a 10 year forecast 99th percentile of 45% above the mean "],
["estimation-risk.html", "23.5 Estimation Risk", " 23.5 Estimation Risk Estimation risk is due to the uncertainty in selecting parameters (freq, sev, trend, variability) and specifically the downstream impact of this Maximum Likelihood Estimate For large data sets it has the lowest estimation error of unbiased estimators Uncertainty depends on the slop of the likelihood surface; Steeper the slope near the MLE, the more confident we are in the estimate 2nd derivative of the likelihood w.r.t. each parameters Negative of the 2nd derivative of the log likelihood = information matrix \\(I = \\dfrac{\\partial [\\overbrace{ -LL }^{\\text{Neg Log Likelihoo}}]}{\\partial \\vec{\\underbrace{\\alpha}_{\\text{Parameters}}}}\\) Inverse of the information matrix = co-variance matrix \\(\\Sigma = I^{ -1 }\\) If slope of \\(-LL\\) is steep near the selection \\(\\Rightarrow\\) More confidence in the selection Model parameter uncertainty by: Assume join lognormal distribution for the parameters and use the correlation from \\(\\Sigma\\) from the MLE A new set of parameters are selection for each iterations in the simulation Joint LogNormal distribution is selected because: Eliminates negative losses Parameters estimates themselves are heavy tailed for heavy tailed distribution like Pareto e.g. \\(\\alpha\\) in simple Pareto \\(F(x) 1 - \\left( \\frac{\\theta}{x} \\right)^{\\alpha}\\) follows inverse gamma, which is similar to LogNormal Results from simulations on small data sets showed that joint lognormal is reasonable "],
["model-risk.html", "23.6 Model Risk", " 23.6 Model Risk When considering different distributions, need to adjust the log likelihood with penalty for the number of parameters Hannan-Quinn Information Criterion (HQIC) is recommended because it is a compromise of other information criteria which add large or smaller penalties for # of parameter Create a pool of distn that are consider reasonable: Randomly select a mean for the parameters and a \\(\\Sigma\\) from the pool of distn Randomly draw the parameters based on the selected distn For each claim that is simulated, draw from the distribution with parameters from (2) "],
["projection-models.html", "23.7 Projection Models", " 23.7 Projection Models The form of the model is very important in measuring variability e.g. simple trend vs time series; expected parameters vs pool of distn for those parameters Use common sense to ensure model is structurally consistent with the underlying process Risk modeling we are more focused on the spread of potential outcomes instead of the best estimate Selecting models using parsimony will lead to unrealistically stable results Add flexibility and let the parameters reflect the uncertainty in the assumptions "],
["conclusion-3.html", "23.8 Conclusion", " 23.8 Conclusion Parameter risk is a key source of variability, especially for large companies Projection risk, estimation risk, model risk can all be quantified and applied i a simulation model -->"],
["c3-era-3-3-modeling-and-dependency-correlations-and-copulas-g-venter.html", "Chapter 24 C3 ERA 3.3 Modeling and Dependency: Correlations and Copulas - G. Venter ", " Chapter 24 C3 ERA 3.3 Modeling and Dependency: Correlations and Copulas - G. Venter "],
["cliffs-summary-18.html", "24.1 Cliff’s Summary", " 24.1 Cliff’s Summary Correlation Pearson’s correlation: Formula and its properties Outliers will have disproportionate weight Kendall’s \\(\\tau\\): depends on the rank Know the formula for discrete and continuous Copulas Know the limitation of joint distribution and advantages for using copulas Different joint distribution plots Best method is to plot the percentile from each marginal distribution; each box should have the same number of points if independent How to use a copula and Sklar’s Theorem Joint density function express with copula: \\(h(x,y) = \\underbrace{c \\left( F(x), G(y) \\right)}_{\\text{Density of copula}} \\cdot \\underbrace{\\left[f(x) \\cdot g(y) \\right]}_{\\text{Joint dist if }\\perp\\!\\!\\!\\perp}\\) Copula Shape Dependency \\(\\tau\\) Frank’s Symmetric Light tails Complicated has an integral Gumbel Asymmetric More weight in the right. Higher tail than Frank \\(1 - \\dfrac{1}{a}\\) HRT Asymmetric Less tail on the left but high on the right \\(\\dfrac{1}{2a + 1}\\) Normal Symmetric Higher tail than Frank \\(\\dfrac{2\\operatorname{arcsin}(a)}{\\pi}\\) There’s also the partial perfect correlation copula Know how the simulation is done for each of the copula Frank, Gumble, HRT is same as Frank, and Normal Tail Concentration Functions Left tail and right tail concentration function Know what each copulat looks like Methods for selecting Copulas Plot the percentile plot Empirical tail concentration function Multivariate Copulas Normal and t-copula and their properties Fitting copulas to data Using the \\(J(z)\\) and \\(\\chi(z)\\) Graph the possible \\(J(z)\\) with empirical \\(J(z)\\) to see which fits best; Similarly for \\(\\chi(z)\\) "],
["introduction-12.html", "24.2 Introduction", " 24.2 Introduction We need loss distribution for the whole group Create distribution for each LoB or Company Combine them into one distribution that reflects all the inter dependencies between BUs Go through 4 copulas that are useful in practice and how to select them for a given dataset "],
["pearsons-correlation.html", "24.3 Pearson’s Correlation", " 24.3 Pearson’s Correlation \\(\\rho = \\dfrac{\\sum_i \\tilde{y}_i \\tilde{z}_i}{\\sqrt{\\sum_i \\tilde{y}_i^2 \\sum_i \\tilde{z}_i^2}} = \\dfrac{\\operatorname{E}[YZ] - \\operatorname{E}[Y] \\cdot \\operatorname{E}[Z]}{\\sigma_{y} \\cdot \\sigma_{z}}\\) \\(\\tilde{y}_i = (y_i - \\bar{y})\\) \\(\\sigma_y^2 = \\operatorname{E}[Y^2] - \\operatorname{E}[Y]^2\\) Only appropriate for distn symmetric and have thin tail Value far from the mean will have a disproportionate weight as it focus on the amount of each \\(\\tilde{y}_i\\) and \\(\\tilde{z}_i\\) Properties: Pearson correlation will stays the same under positive linear transformation on \\(Y\\) or \\(Z\\) Monotone function that is not linear might change the Pearson correlation "],
["kendalls-tau.html", "24.4 Kendall’s Tau", " 24.4 Kendall’s Tau Depends on the order not the value of the data Concordant: When one pair dominates the other given \\((x_1, y_1)\\) and \\((x_2, y_2)\\) \\(x_1 &gt; x_2\\) and \\(y_1 &gt; y_2\\) or, \\(x_2 &gt; x_1\\) and \\(y_2 &gt; y_1\\) Discordant: when the pair is mixed \\(\\tau = \\dfrac{C - D}{\\text{# of pairs}}\\) \\(\\tau = \\dfrac{C - D}{C + D}\\) if there are no ties \\(\\tau \\in [-1, 1]\\) with same interpretation as Pearson’s Focus on the rank \\(\\Rightarrow\\) changes in one extreme value won’t change the indication Continuous \\(\\tau = 4 \\operatorname{E}[C(u,v)] - 1\\) \\(\\tau = -1 + 4 \\int_0^1 \\int_0^1 C(u,v)c(u,v)dudv\\) Defn Correlation = mathematical calculation, a statistic, and measure of dependency Dependency = interaction between random variables (boarder term) e.g. correlation can be 0 while there are still dependency "],
["motivations-for-using-copulas.html", "24.5 Motivations for Using Copulas", " 24.5 Motivations for Using Copulas There are only a few joint distributions that are tractable to work with (normal, lognormal, exponential, etc) We can’t do weibull, Pareto or gamma We can’t mix distributions like joining normal with exponential Modeling all business together is not feasible due to inconsistent of mix of business over time Joint Distribution Plots 3 ways to look at them to try and see if there are dependencies Straight plotting on (x,y) Draw lines at 25%, 50% and 75% and segment the plot into 16 If the 2 marginal distributions are independent \\(\\Rightarrow\\) there should be about \\(\\frac{1}{16}\\) of the points in each rectangles This is useful to show us actual values, but Might be difficult to see points in some of the rectangles Plot on log log scale (ln(x), ln(y)) This alleviate the issue above and shows a clearer picture of the dependencies Plot the percentile from each marginal distn This gives us a clean picture as all the rectangles are the same size since the axis are now the percentile Advantages of using copula to describe dependency with percentiles It is independent of the underlying distributions Can update the marginal distn without changing the dependency structure Can joint distn that is not the same "],
["how-to-use-a-copula.html", "24.6 How to Use a Copula", " 24.6 How to Use a Copula We can fully describe the joint distribution with: \\(H(x,y) = P(X \\leq x \\: \\&amp; \\: Y \\leq y)\\) If we know this then we know the entire distn Marginal distn: \\(F(x) = \\lim \\limits_{y \\rightarrow \\infty} H(x,y)\\) \\(G(y) = \\lim \\limits_{x \\rightarrow \\infty} H(x,y)\\) Density: \\(h(x,y) = \\dfrac{\\partial^2 H(x,y)}{\\partial x \\partial y}\\) Describes where the probability lies A bit more intuitive to looks at rather than the CDF It is easier to work with percentiles, so instead of using \\(x\\) &amp; \\(y\\), we’ll use the percentiles \\(u\\) &amp; \\(v\\) and we’ll compare them to the percentile of the r.v. Sklar’s Theorem For any joint distn \\(H(x,y)\\) \\(\\exists\\) a function \\(C(u,v)\\) such that \\(H(x,y) = C(F(x), G(y))\\) \\(C(u,v)\\) is a copula Input is 2 percentiles and output is the joint percentile Density of a Copula Graphs of actual copula is difficult to interpret, so focus on the density \\(c(u,v) = \\dfrac{\\partial^2 C(u,v)}{\\partial u \\partial v}\\) Relate the density of the copula to the density of the joint distn \\(h(x,y) = \\underbrace{c \\left( F(x), G(y) \\right)}_{\\text{Density of copula}} \\cdot \\underbrace{\\left[f(x) \\cdot g(y) \\right]}_{\\text{Joint dist if }\\perp\\!\\!\\!\\perp}\\) Think of the \\(c(u,v)\\) as a multiplier Scales up and down the independent distribution \\(c(u,v) &gt; 1\\) \\(\\Rightarrow\\) Higher density \\(c(u,v) = 1\\) \\(\\Rightarrow\\) Independence \\(c(u,v) &lt; 1\\) \\(\\Rightarrow\\) Lower density Different copulas have different behavior w.r.t. where the dependencies are located We’ll use Kendall’s \\(\\tau\\) to measure correlation as it won’t be affected by the underlying distribution Note, don’t have to memorize the formulas "],
["summary-of-copulas.html", "24.7 Summary of Copulas", " 24.7 Summary of Copulas Copula Shape Dependency \\(\\tau\\) Frank’s Symmetric Light tails Complicated has an integral Gumbel Asymmetric More weight in the right. Higher tail than Frank \\(1 - \\dfrac{1}{a}\\) HRT Asymmetric Less tail on the left but high on the right \\(\\dfrac{1}{2a + 1}\\) Normal Symmetric Higher tail than Frank \\(\\dfrac{2\\operatorname{arcsin}(a)}{\\pi}\\) 24.7.1 Frank’s Copula Small tail dependencies compared to Gumbel and HRT Density of the copula is the 2nd derivative w.r.t. both \\(u\\) and \\(v\\) \\(\\tau\\) for Frank is not closed form Conditional probability of \\(y\\) is the derivative of the copula w.r.t. \\(u\\) Simulation Need the conditional distribution \\(P(Y \\leq y \\mid X = x)\\) Given \\(tau\\) we can solve for \\(a\\) which is used in the conditional formula Simulate 2 r.v.: \\(u,p \\sim U[0,1]\\) Use \\(u\\) to determine \\(x\\) and use \\(p\\) to find \\(y \\mid x\\) \\(x = F^{-1}(u)\\) \\(p = P(Y \\leq y \\mid X = x) = C_1(u,v)\\) and solve for \\(v\\) We know the formula for \\(p\\) and can do some algebra to get \\(v\\) Use the \\((u,v)\\) we get from above and then find the values in the 2 distn associated with the percentiles 24.7.2 Gumbel Copula \\(\\tau\\) has a simple form \\(= 1 - \\frac{1}{a}\\) \\(v\\) can’t be solve from \\(p=C_1(u,v)\\) \\(\\Rightarrow\\) Need to simulate to some other ways Simulation Simulate 2 r.v.: \\(p,r \\sim U[0,1]\\) Numerically solve for \\(s\\) that satisfies \\(s \\operatorname{ln}(s) = a(s-p)\\) where \\(0&lt;s&lt;1\\) Use \\(s\\) we can get \\((u,v) = \\left( \\operatorname{exp}\\left[ \\operatorname{ln}(s)r^{\\frac{1}{a}} \\right], \\operatorname{exp}\\left[ \\operatorname{ln}(1-r)r^{\\frac{1}{a}} \\right] \\right)\\) 24.7.3 Heavy Right Tail Copula Less correlation in the left tail but high in the right tail \\(\\tau(a) = \\dfrac{1}{2a + 1}\\) We can solve \\(v\\) from \\(p=C_1(u,v)\\) so we can simulate it same way as Frank’s Joint Burr Distribution Joint distn where marginal distn are Burr and the conditionals are also Burr Join the 2 marginal Burr using the HRT Also need the 2 marginal distn to have the same \\(a\\) parameter as the HRT Analogous to the joint normal, in that the marginal and conditional distn are the same \\(a\\) is to control correlation \\(p\\) &amp; \\(q\\) is used to fit the tail \\(b\\) &amp; \\(d\\) are used to set the scales of the distn 24.7.4 Normal Copula Joins 2 distn using correlations from the bi variate normal More tail dependencies than Frank and symmetrical The copula take \\(u\\) and \\(v\\) from any distn and converts them to standard normal variables and calculates the probability under the join normal distn with parameter \\(a\\) (See a whole lot from full manual) Normal copula is simple to simulate from and also easy to generalizes to multiple dimensions Simulation Solve for \\(a\\) with the \\(\\tau\\) Simulate 2 r.v. \\(u\\) and \\(r\\) Get the standard normal value of the 2 percentiles Join the 2 normal value from 2 to get \\(z = ax + y \\sqrt{1 - a^2}\\) Convert \\(z\\) to a percentile with the normal function to get \\(v\\) 24.7.5 Partial Perfect Correlation Copula Krep’s Partial Perfect Correlation Copula Sort of artificial, more useful for simulating evens then for description of actual outcomes Generates dependencies by sometimes simulating \\(u\\) &amp; \\(v\\) where they are independent and sometimes 100% dependent by setting \\(v=u\\) Level of dependency is controlled by \\(q(u,p): [0,1]^2 \\rightarrow [0,1]\\) \\(q(u,p)\\) needs to be symmetric Definition Simulate PPC by drawing \\(u\\), \\(p\\) &amp; \\(w\\) from \\(U[0,1]\\) \\(v = \\begin{cases} p &amp;:q(u,p) &lt; w &amp; Independent \\\\ u &amp;:q(u,p) \\geq w &amp; Dependent \\\\ \\end{cases}\\) Partial Perfect Power \\(h(u,p) = q(u,p) = (up)^a\\) More concentrate at the top right, since if either \\(u\\) or \\(v\\) is large then the other will likely be at the same percentile \\(\\Rightarrow\\) perfectly correlated "],
["tail-concentration-functions.html", "24.8 Tail Concentration Functions", " 24.8 Tail Concentration Functions Measure how much probability is concentrated in the tails Left Tail Concentration Function \\(L(z) = P(U &lt; z \\mid V &lt; z) = P(V &lt; z \\mid U &lt; z)\\) What is the probability that \\(U\\) is small given that \\(V\\) is small Works both ways can do \\(L(z) = P(V &lt; z \\mid U &lt; z)\\) as well \\(L(z) = \\dfrac{C(z,z)}{z}\\) Independent copula: \\(L(z) = z\\) Graph this to 0.5 because all copulas have \\(L(1) = 1\\) See Graph (1) Right Tail Concentration Function \\(R(z) = P(U &gt; z \\mid V &gt; z) = P(V &gt; z \\mid U &gt; z)\\) What is the probability that one variable is large given that the other variable is large \\(R(z) - \\dfrac{1 - 2z + C(z,z)}{1-z}\\) Independent copula: \\(R(z) = 1 - z\\) Graph this from 0.5 to 1.0 because all copulas has \\(R(0) = 0\\) See Graph (2) Combine the 2 above we have the tail concentration graph or the LR graph See Graph (3) The independent copula = product copula Graph (4) it approaches 0 on the left and right Graph (5) asymmetric, right limit is not 0 Graph (6) asymmetric, right limit is not 0, and higher than Gumbel. Has the thinnest left tail Graph (7) it approaches 0 on the left and right but slower, simply because we couldn’t calculate the tail so deep As the \\(\\tau\\) gets higher, we can start the see the difference between them "],
["how-to-select-copula-given-dataset.html", "24.9 How to Select Copula Given Dataset", " 24.9 How to Select Copula Given Dataset Plot the percentile plot to diagnose Calculate the empirical tail concentration function (LR function) and you can see how each copula fit best Figure 3.3.5 and 3.3.6 is not very good as it mix the underlying distn with the copula Multivariate Copulas Many options of copulas for 2 variables but not multivariate Normal and t-copula are the only well known ones Parameters are the full correlation matrix Normal copula has no correlation deep in the tail The right tail concentration is very small, and approaches 0 at the limit Just to be clear, the density deep in the tail (z &gt; 0.999) you get very high values still Properties of the t-copula Can be very strongly correlated in the tails, controlled with \\(n\\) \\(n \\rightarrow \\infty\\), the t-copula approaches the normal copula For small n, it has high density in the tails T-copula also have some density in the other corners "],
["fitting-copulas-to-data.html", "24.10 Fitting Copulas to Data", " 24.10 Fitting Copulas to Data \\(J(z)\\) and \\(\\chi(z)\\) are the empirical statistics we’re interested in that are related to \\(\\tau\\) and \\(R\\) 24.10.1 \\(J(z)\\) \\(J(z) = - z^2 + \\dfrac{4 \\int_0^z \\int_0^z C(u,v) \\cdot c(u,v)dudv}{C(z,z)}\\) \\(C(1,1) =1\\) so \\(\\lim limits_{z \\rightarrow 1} J(z) = \\tau\\) \\(J(z)\\) build up to \\(\\tau\\) from 0 (when no data points are considered), up to \\(\\tau\\) when all the data points are considered Note that 3.3.14 is incorrect Look at 3.3.16 we can tell that t-copula is the best fit and MM2 is the next closest, MM1 is the worst 24.10.2 \\(\\chi(z)\\) Don’t have much on an intuitive explanation for what the graph represents \\(z \\rightarrow 1\\) approaches \\(R\\) \\(\\chi(z) = 2 - \\dfrac{operatorname{ln}C(z,z)}{\\operatorname{ln}(z)}\\) $R = limits_{z 1} R(z) = limits_{z 1} Compare the empirical graph of \\(\\chi(z)\\) with the theoretical for the three copulas to determine which is best fit -->"],
["c4-era-4-1-4-2-operational-and-strategic-risk-d-mango-g-venter.html", "Chapter 25 C4 ERA 4.1 &amp; 4.2 Operational and Strategic Risk - D. Mango, G. Venter ", " Chapter 25 C4 ERA 4.1 &amp; 4.2 Operational and Strategic Risk - D. Mango, G. Venter "],
["cliffs-summary-19.html", "25.1 Cliff’s Summary", " 25.1 Cliff’s Summary 25.1.1 Op risk The risk of loss resulting from inadequate or failed internal process, people, and systems or from external events Types of operational risk Underlying cause of insurer failure can be op risk even though if the surface cause is reserving or under pricing Plan loss ratio example hows how optimistic loss ratio propagates forward 3 different reasons why the planned LR didn’t work and only 1 of them is truely insurance risk Cycle management IP, u/w incentives, market overreaction, owner education 6 Additional persepctives 25.1.2 Strategic risk Various definition of strategic risk, no one clear definition, some overlaps with op risk Various research to date Elements of strategic risk from Baird and Thomas Category of strategic risk from slywotzky and drzik Scenario planning and the 10 Steps Look at the insurance example and the advantages of scneario planning Advance scenario planning Expend to several LoBs with dependencies Stochastic Agent based modeling (competitors, customers, regulators) "],
["operational-risk.html", "25.2 4.1 Operational Risk", " 25.2 4.1 Operational Risk Important cause of insurer insolvency \\(\\Rightarrow\\) Need to be aware and attempt to measure Definition from Basel: The risk of loss resulting from inadequate or failed internal process, people, and systems or from external events Include legal risk but exclude strategic and reputation risk 7 Types of Operational Risk Defined by Basel Committee Internal fraud External fraud Employment practices and workplace safety Clients, products and business practices Damage to physical assets Business disruption and system failures Execution, delivery and process management Still no consensus on how to define op risk Could include reputational and strategic risk, but as long as it is considered it is fine Understanding of op risk is in its infancy especially with quantitative modeling Do not lose sight of op risk despite it is soft, difficult, poorly understood and lack historical track record 25.2.1 Operational Risk to Insurers Study show that insurer failure are mostly attributed to under reserving and under pricing That is factually accurate, however in many case the underlying cause was actually operational risk e.g. Over accumulation of risk exposure (insufficient initial reserving or premature reserve release) and maybe just overly optimistic plan loss ratios Capital charges for premium, reserve, and growth are partially proxies for operational risk Clear that op risk have heavily contributed to insurance impairments and should be explicitly model 25.2.2 Insurer Op Risk: Plan Loss Ratios Mostly impact long tailed LoBs (50% @ 5yrs, 90% @ 10yrs) Optimistic loss ratio propagates forward Optimistic LR for older years lead to optimistic LR for th recent years Since the ELR is based on older years experience that might be too low, the planned LR is too low as well As the older years losses develop higher than expected \\(\\Rightarrow\\) increase in reserves for all years, and can lead to: Rating downgrade Downgrade of claims paying ability Massive non-renewals No new business Eventual runoff Three reasons why the planned LR did not work Model unable to forecast accurate LR If the market and other carriers did fine, then the model was effective Planning system did not deliver accurate LR, therefore op risk Unless everyone else failed to price properly, like asbestos Model was able to forecast accurate LR but was improperly used Op risk: What checks were there to be sure the model was used correctly? Model did accurately forecast the LR but indications were unpopular so ignored Op risk: What governance should be there around management’s decisions 25.2.3 Insurer Op Risk: Cycle Management Cycle management: Prudent management of u/w capacity as market pricing fluctuates in the u/w cycle Simply maintaining market share during soft markets increases the likelihood of impairment 3 questions to ask: Does the company have or need a proactive cycle management strategy Does the company know where in the cycle the market stands Are u/w-ers making decisions that are consistent with (1) and (2) 25.2.3.1 System Performance Perspective From a system performance perspective, company needs to be: Stable, available, reliable, and affordable Downgrade impacts the first 2 (stability, availability) Insolvent impacts the last 2 (reliability, affordability) 25.2.3.2 Cycle Management Areas impact by implementing cycle management: Planning, u/w, objective setting, incentive bonus Focus on the following to achieve meaningful process improvements: Intellectual Property Intangible and are time consuming to build but easy to destroy Experts in u/w, claims, finance, actuarial P/h info database Forecasting systems Market relationship Reputation Focus on retaining these core assets through cycle: Retain top talent, grow and develop their skills Maintain presence in core market channels Maintain consistent pattern of investment in IT and systems Underwriter Incentives Bonus should be based on u/w-er supporting the portfolio goals, which may include writing less business, and they should know that it won’t affect employment and bonus Market Overreaction Companies tend to overreact, taking prices too low and too high Companies with most capital during price-improvement phase will reap windfall profits, which will more than offset many years of small u/w losses Owner Education Owner/shareholders need to understand that some financial figures will be out of step with the market and does not appear healthy during soft markets: Premium volume \\(\\downarrow\\) Overhead expense ratio to premium \\(\\uparrow\\) 25.2.4 Additional Perspectives 25.2.4.1 Agency Theory Perespective Incentives of managers and u/w-ers of the firm are not always aligned with shareholders Rather than try to fund for this op risk, better to understand it: Giving management equity stakes \\(\\xrightarrow{\\text{Caveat}}\\) Manager too aggressive taking on more risk Equity is a large portion of management’s net worth \\(\\xrightarrow{\\text{Caveat}}\\) Too risk adverse Production incentives for u/w-ers \\(\\xrightarrow{\\text{Caveat}}\\) Sloppy u/w-ing or mispricing 25.2.4.2 Operational Risk Management in Banking and Manufacturing Risks that are common to all business Pension funding IT failure Other HR risks (loss of important staff, misdesign of comp program) Reputation risk Lawsuits 25.2.4.3 Control Self-Assessment (CSA) Definition from Institute of Internal Auditors A process through which internal control effectiveness is examined and assessed. The objective is to provide reasonable assurance that all business objectives will be met Objectives of internal control Reliability and integrity of information Compliance with policies, laws and regulations Safeguarding assets Economical and efficient use of resources Accomplishment of objectives and goals for operations or programs 25.2.4.4 Key Risk Indications (KRIs) Broad category of measures that monitor the activities and status of the control environment of an operational risk category Measured frequently Have threshold that lead to escalation Possible KRIs Production: retention ratio, rate/exposure Internal controls: audit results and frequency Staffing: turnover rate, premium/employee, training budget Claims: freq, sev 25.2.4.5 Six Sigma Tolerances for quality if \\(\\pm 3 \\sigma\\) based on manufacturing Applied in process improvement or predictive design of processes Useful in high volume processing Help identify and eliminate: inefficiencies, errors, overlaps, gaps in communication and coordination Insurance example: U/w-ing: exposure data verification, exposure data capture, classification Claims: coverage verification Reinsurance: treaty claim reporting, coverage verification, collateralization 25.2.4.6 Operational Risk Modeling Operational risk manager needs to decide if risk should be transferred or retained Steps for operational risk portfolio management: Identify exposure base for each risk source (premium, headcount, payroll) Measure the exposure level for each BU and risk source Estimate the loss potential per exposure for each op risk Step 2 + 3 = loss distn for each BU Estimate the impact of mitigation, process improvements, or risk transfer Requires significant expert opinion as there are no significant amount of loss data both before and after "],
["strategic-risk-1.html", "25.3 4.2 Strategic Risk", " 25.3 4.2 Strategic Risk Strategy: Planning and the use of resources to achieve an organization’s goals Strategic risk: Intentional risk taking to achieve an organization’s goals Scenario planning is a key part of strategic risk management Think through the possible scenarios the firm could find itself in and to plan responses to those scenarios Advanced scenario planning incorporates several LoBs and indcludes dependencies among the LoBs Further step is agent based modeling where other parties reactions are also modeled 25.3.1 History and Definition Strategy Definition from Mango: Science and art of planning Using political, psychological, and organizational resources To achieve major organizational goals Definition from GIRO: A strategy is a long term series of actions designed to take a company from its current state to its desired future state, and aim to provide a sustainable competitive advantage over other companies in the same market Strategy in NOT (from GIRO): Business planning, strategy considers a wider breadth of issues such as the market, and where the company sits Tactics, as these are short term Strategic Risk Definition (unclear, but can be interpreted as): Intentional risk taking Unintentional risk as by-products of strategy planning or execution Definition from OCC (more like op risk): Strategic risk is the risk to earnings or capital arising from averse business decisions or improper implementation of those decisions. … Its focus is on how plans, systems and implementation affect the franchise value Very similar to operation risk and not strategic risk 25.3.2 Research to Date 25.3.2.1 Miller Strategic risk: unpredictability in corporate outcomes (effects) Strategic uncertainty: unpredictability of environmental and organizational variables that impact corporate performance (sources) Our definition of risk is closer to Miller’s definition of uncertainty 25.3.2.2 Baird and Thomas Definition of risk too restrictive from an 1992 old paper: Risk: A condition in which the consequences of a decision and the probabilities associated with the consequences are know entities Baird and Thomas strategic risk definition (very broad): Corporate strategic moves which cause returns to vary, which involve venturing into the unknown, which may result in corporate ruin, where outcomes and probabilities maybe only be partially known and where hard to define goals may not be met Elements of strategic risk: Voluntariness of exposure (Choose risk because potential benefits are larger than consequences) Controllability of consequences (outcomes can be contained) Discounting in time (ability to delay consequences) Discounting in space (ability to shift risks to competitors) Knowledge of risky situation Magnitude of impact Group/individual factors (is the leader of the group in favor of risk acceptance) 25.3.2.3 Slywotzky and Drzik Categorize strategic risk Risk Examples Magnitude Comments Industry Capital Intensive; Overcapacity; Accommodation Very High Technology Shift; Patents; Obsolescence Low Possibly innovation in distn Brand Erosion; Collapse Moderate Reputation for fair claims handling Competitor Global Rivals; Unique Competitors Moderate Predatory pricing from competitors Customer Priority Shift; Power; Concentration Moderate Bigger issue for large commercial insurance Project Failure of R&amp;D; IT; Business Development; M&amp;A High Insurers has long history of failed mergers; Low investment in IT Stagnation Flat or declining volume; Price declines High Highly correlated to the cycle; Companies will keep writing business to cover fixed expenses 25.3.2.4 Hertz and Thomas Risk Analysis: An input for the strategy development process, aiding strategy formulation, evaluation, choice and implementation No distinction is drawn between strategic risk analysis and strategy formulation Both are viewed as part of a iterative, adaptive and flexible policy dialogue process 25.3.3 Scenario Planning Effective strategic planning begins with scenario planning: Limited # of scenarios: each tells a story of how elements may interact with each other under different conditions Test for internal consistency and plausibility Explore joint impact of several variables and change multiple variables at one time Include subjective interpretation of factors that cannot be explicitly modeled Attempts to capture the richness of range of possibilities \\(\\Rightarrow\\) Lead the makers to consider changes they might otherwise ignore Key Steps in Scenario Planning Process: Define scope: time frame, geography, market segment Identify major stakeholders: employees, owners, regulators, customers, competitors, suppliers Identify basic trends: their influence on organization Identify key uncertainties: leverage points of impact Construct initial scenario theme Check for consistency and plausibility: outcomes fit together? Develop learning scenarios: identify themes and naming scenarios Identify research needs: areas that need additional research Develop quantitative models: formalize some interactions in a quantitative model? Evolve toward decision scenarios: iterative process of reviewing scenarios with the goal of converging to test scenarios and generate new ideas 25.3.3.1 Insurance Example Scenarios will be less detailed than a typical strategy or plan document Traditional unilateral planning approach: Set planned premium and ELR based on current year LR, price trend, and loss trend Caveat: Not thinking through the possibility of a possible price decrease, where they might want to reduce premium volume instead of the inflexibility of making the planned premium volume Scenario planning: Allow u/w-ers to be flexible in their approach in the case prices are not as expected Must decide ahead of time: Range of scenarios Responses to those scenarios Plans must have enough detail to give them operational weight Advantages: Thinks through responses beforehand and can agree on best response ahead of time Saves time during a crisis Operational inertia is reduced, flexibility is built into the system 25.3.4 Advanced Scenario Planning &amp; ERM Model Expansion Expend into several LoBs Internal consistency requires coordination Need to model dependencies Responses will be politically charged, as limited u/w capacity must be allocated across the company Better to do this during planning than in the heat of a market crisis Stochastic Scenario Planning Generate stochastically simulated scenarios that have responses based on a set of rules Need to define goals e.g. profit or premium level etc Need to have downside constraints e.g. capital loss of x% with y% probability Need to determine the estimate environmental variables that define the state of the world e.g. size of market, price level, current market share, etc Define action rules that respond to environmental variables e.g. hold price, increase u/w capacity according to price adequacy levels 25.3.5 Agent-Based Modeling Agents can be competitors, customers, regulators Program each agent to respond to the state of the market Chase market share, focus on technical price, customer may reduce coverage if prices are high, etc Creates a complex system with emergent properties that describe the behavior of agents -->"],
["c5-era-5-4-approaches-to-modeling-the-underwriting-cycle-major.html", "Chapter 26 C5 ERA 5.4 Approaches to Modeling the Underwriting Cycle - Major ", " Chapter 26 C5 ERA 5.4 Approaches to Modeling the Underwriting Cycle - Major "],
["cliffs-summary-20.html", "26.1 Cliff’s Summary", " 26.1 Cliff’s Summary Characteristics of underwriting cycle 4 stage of insurance business First stage is the classic cycle and the next 3 shows how things stablize and then eventually breakdown Various theories of underwriting cycle Approaches to modeling the cycle Need to us combined ratio or someting as proxy for price Calculate and forecast predictor variables Gather competitor intellgence Styles of modeling: Soft, technical and behavioral Look at the different impact on supply and demand under different scenarios Gron supply curve Captial flow under different profitability How different components interact "],
["introduction-13.html", "26.2 Introduction", " 26.2 Introduction Important to consider how the firm interacts with a competitive environment in an ERM framework Price competition is inevitable due to: Low barriers of entry; lack of patent or copyright product Price is difficult to define since it depends on premium charged as well as limits, deductible, terms and conditions Underwriting cycle: Recurring increase and decrease of prices and profits Result of a dynamical system with both feedback and external shocks and slow adjustment Each LoB has it’s own cycle Capital plays an important role \\(\\Rightarrow\\) multiline insurers creates dependencies between LoBs’ cycle 26.2.1 4 Stages Insurance Business Steward describes the evolution of insurance business, sometimes takes decades to play out 26.2.1.1 1. Emergence Classic u/w-ing cycle here at this stage New LoB, thin data, inaccurate pricing \\(\\hookrightarrow\\) \\(\\uparrow\\) demand with erratic pricing \\(\\Rightarrow\\) Price wars \\(\\hookrightarrow\\) solvency crisis \\(\\Rightarrow\\) force out weak competitors \\(\\Rightarrow\\) price correction \\(\\hookrightarrow\\) \\(\\uparrow\\) profitability \\(\\Rightarrow\\) new entrants and repeat 26.2.1.2 2. Control Stop the cycle with help from rating bureau or insurance department 26.2.1.3 3. Breakdown Control regime breaks down due to new technology or social changes New type of competitors take business away 26.2.1.4 4. Reorganization Return to stage 1, new configuration of the market phase emerges "],
["theories-of-the-cycle.html", "26.3 Theories of the Cycle", " 26.3 Theories of the Cycle Different researches findings are contradictory Institutional Factors Lags in data Reporting and regulatory delays leads to 2nd order autoregression Competition Low prices by one competitor can push prices down Companies strategy is either aggressive growth or price maintenance 4 phases of cycle: cheating, pain, fear, restoration Capital Constraints Capital determines the available supply of insurance Capital is not replaced quickly once reduced Best clients may leave first when capital \\(\\downarrow\\) \\(\\Rightarrow\\) anti-selection Economic Linkages Profitability of firm links to investment income, cost of capital Expected losses may depend on interest rates, inflation Price of risk set by the market is generally ignored by insurers All of the Above No single theory can explain the u/w cycle completely "],
["approaches-to-modeling-the-cycle.html", "26.4 Approaches to Modeling the Cycle", " 26.4 Approaches to Modeling the Cycle First we define what it is we want to know and look for leading indicators that foretell the turn in the cycle Criterion Variable The concern is price, but impossible to define in insurance Use loss ratio or combined ratio etc as a proxy Can include investment income Predictor Variable To calculate the current period criterion and also forecast the forward period criterion Historical criterion variable: loss, expense Internal financial variables: reserve, capital, capital flow, reinsurance cessions Regulatory rating variables: rating changes Reinsurance sector financials: capital held Econometric variable: inflation, unemployment, GDP Financial market variables: interest rates, stock market returns Competitor Intelligence Gather information on customers at renewal time and competitors Trade publications and rate filings Need to beware of antitrust and legal issues 26.4.1 Styles of Modeling 26.4.1.1 Soft Approach Focuses on gathering expert opinion Data quality, variety, complexity, and human factors Human approach, focused on three methods: Scenarios Detailed written statement describing a possible future state of the world Delphi Method Gather expert opinion without biasing the group to the opinion of the most senior persons Multiple rounds of providing information and questions Formal Competitor Analysis For at financials, news item, behavioral metrics Look for unusual profitable or distressed financial conditions across a large number of firms 26.4.1.2 Technical Approach Focuses on statistical modeling, mathematical formalism and rigor Autoregressive Model Research shown cycle can be modeled with \\(AR(2)\\) or \\(AR(3)\\) model For \\(AR(2)\\): \\(X_t = b_o + b_1 X_{t_1} + b_2 X_{t-2} + \\sigma \\epsilon_t\\) Use autoregressive model to model P&amp;C industry combined ratios Results showed weak mean regression with lag 1 but strong mean regression at lag 2 Model can be used to forecast a few periods into the future and estimate the distribution for those forecasts VARMAX Generalized multivariate time series that can utilize external variables General Factor Model Looks like \\(AR(1)\\) but with non-normal mean and a moving temporary mean \\(X_t = c + d(Z_{t-1} - X_{t-1}) + \\tau \\delta_t\\) \\(Z_t = a + b \\cdot Z_{t-1} + \\sigma \\epsilon_t\\) \\(Z_t\\) maybe an unknown or unobservable variable Complicated to fit this model 26.4.1.3 Behavioral Modeling Econometric Modeling Sit between soft and technical model’s concern for structural insight (soft) and statistical validity (technical) Can be done at an industry level or company level Industry level can be more detailed Company level requires maintaining many individual models and their interactions May reveal emergent properties (See ERA 4.2) "],
["supply-and-demand.html", "26.5 Supply and Demand", " 26.5 Supply and Demand Plotting the supply and demand curve on quantity vs price Supply New competition or technology increases the quantity available at a given price \\(\\hookrightarrow\\) Shifting curve \\(\\searrow\\) Higher capital requirement \\(\\hookrightarrow\\) Shifting curve \\(\\uparrow\\) Demand Excess capital makes insurance more valuable \\(\\hookrightarrow\\) Shifting curve \\(\\nearrow\\) Shock to capital \\(\\hookrightarrow\\) Shifting curve \\(\\swarrow\\) Company demand curve will be flatter since customers can go to another company for small changes in price More intuitive to think of quantity as a function of price Market price is where supply meets demand Difficult to empirically estimate the curves because only the equilibrium is observable Gron Supply Curve Plot 4 and 5 above The first flat section: There is a minimum price = expected losses + marginal expenses Supply is perfectly elastic for a certain quantity, where the supply will increase without an increasing price The curved section: At certain point firm will require additional capital to support the business, and the price must increase Once price hits a a certain level, profits are enough to cover the additional capital and the supply curve approaches a price asymptotically Shock scenario: Curve would simply shift \\(\\leftarrow\\) Expected loss + marginal expense is still the same, while the amount of capital the company have before it needs to seek additional capital has changed "],
["capital-flows.html", "26.6 Capital Flows", " 26.6 Capital Flows Capital \\(\\uparrow\\) in a firm from retained earnings and capital infusions Capital \\(\\downarrow\\) from operating losses and capital withdrawal Capital is attracted when profit expectations are high Capital exits either voluntarily or under financial distress "],
["assembling-the-components.html", "26.7 Assembling the Components", " 26.7 Assembling the Components Economic factors and capital both affect the supply and demand curves Level of price and losses affect the supply curve Capital is impacted directly by premium and losses and specifically profitability Output of this model is an equilibrium price The behavioral model can be used to simulate a distn of this price "],
["conclusion-4.html", "26.8 Conclusion", " 26.8 Conclusion U/w cycle is the behavior of a complex dynamic system Output of the modeled u/w cycle should feed into the firm’s demand and retention models They help to forecast how clients will react to the difference between the firm’s pricing and that of other prices available in the market "],
["exam-7-2016-overall-summary.html", "Chapter 27 Exam 7 2016 Overall Summary ", " Chapter 27 Exam 7 2016 Overall Summary "],
["section-a.html", "27.1 Section A", " 27.1 Section A 27.1.1 A1 Least Square Loss Development Using Credibility - Brosius Least square methods Need to know how to calculate \\(b\\) Theoretical Bayesian method and also estimating the Bayesian credibility with 2 methods for getting \\(Z\\) Can do \\(Z = \\dfrac{b}{c}\\) or the VHM EVPV formula Under certain relationships between \\(X\\) and \\(Y\\) supports different methods Some formulas to remember if we do the theoretical Bayes Discussion on the caseload effect where the development and ultimate is not independent 27.1.2 A2 Benktander Credible Claims Reserve: The Benktander Method - T. Mack (2000) Benktander Method 27.1.3 A3 Credible Claims Reserve Credible Claims Reserve: Benktander, Neuhaus and Mack - W. Hurlimann Method that is based on the incremental loss ratio triangles Weight between the analogous development and BF method \\(R_i^c = Z_iR_i^{ind} + (1-Z_i)R_i^{coll}\\) \\(Z_i\\) Method 1 Chainladder; Individual LR 0 BF; Collective LR \\(p_i\\) Benktander \\(p_i \\times ELR\\) Neuhaus \\(Z = \\dfrac{p_i}{p_i + \\sqrt{p_i}}\\) Optimal 27.1.4 A4 High Deductible A Model for Reserving Workers Compensation High Deductibles - J. Siewert Covers 6 methods for dealing with deductibles each with their own pros and cons Loss Ratio Method: Apply occurence and aggregate charges Implied Development: Calculate as the difference between unlimited and limited Direct Development: \\(XSLDF_t^{L} = \\dfrac{Ult_{XS}}{S_t^{XS}} = \\dfrac{Ult \\cdot \\chi}{\\frac{Ult}{LDF_t} - \\frac{Ult\\cdot(1-\\chi)}{LDF_t^L}}\\) Credibility Weight Method: Weight between direct development and expected loss ratio method Development Method Severity needs to be trended Claim counts are developed separately ground up Severity LDF formulas, know them well to manipulate and know what formula requires what \\(LDF_t^L = LDF_t \\dfrac{R^L}{R_t^L}\\) \\(XSLDF_t^L = LDF_t \\dfrac{(1-R^L)}{(1-R_t^L)}\\) \\(LDF_t = R^L_t \\cdot LDF^L_t + (1 - R^L_t) \\cdot XSLDF^L_t\\) \\(\\dfrac{ILDF^L_t}{ILDF_t} = \\Delta R^L_t = \\dfrac{R^L_{t+1}}{R^L_t}\\) \\(\\dfrac{IXSLDF^L_t}{IXSLDF_t} = \\Delta (1 - R^L_t) = \\dfrac{1 - R^L_{t+1}}{1 - R^L_t}\\) Distribution Method 27.1.5 A5 Developement by Layer Claims Development by Layer - R. Sahasrabuddhe Know the process for setting up base triangle and then convert the base LDFs to any layer Main formulas to memorize: For loss capping: \\(LEV(X; \\Phi \\sim Exp(\\theta)) = \\theta \\: \\left[ 1 - \\operatorname{exp}\\left\\{-\\left(\\dfrac{x}{\\theta}\\right)\\right\\} \\right]\\) For conversion: \\(\\begin{align} F_{ij}^X = F_{nj}^B \\times \\frac{LEV(X;\\Phi_{i\\infty})\\div LEV(B;\\Phi_{n\\infty})}{LEV(X;\\Phi_{ij}) \\div LEV(B;\\Phi_{nj})} \\end{align}\\) We can do this for an XS layer as well If we don’t have the severity distribution by age, we can work with the severity at ultimate \\(\\begin{align} F_{ij}^X = F_{nj}^B \\times \\frac{LEV(X;\\Phi_{i\\infty})\\div LEV(B;\\Phi_{ {\\color{red}i} \\infty})}{R_j{(X,B)}} \\end{align}\\) 27.1.6 A6 Variability in CL Method Measuring the Variability of Chain Ladder Reserve Estimate - T. Mack (1994) Focus on cummulative losses Chain ladder assumptions: \\(\\operatorname{E}\\left [c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ] = c_{i,k} \\: f_k\\) \\(\\left \\{c_{i,1} \\cdots c_{i,I} \\right \\} \\: {\\perp\\!\\!\\!\\perp} \\: \\left \\{c_{j,1} \\cdots c_{j,I} \\right \\}\\) for \\(i \\neq\\ j\\) \\(\\operatorname{Var}\\left (c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ) = \\alpha_k^2 \\: c_{i,k}\\) LDF weight and variance assumptions: \\(\\begin{align} \\hat{f_k} = \\frac{\\sum_j \\overbrace{\\frac{c_{j,k+1}}{c_{j,k}}}^{LDF_j} \\: w_{j,k}}{\\sum_j w_{j,k}} \\end{align}\\) Weight \\(w_{j,k}\\) Description Variance Residual 1 Simple Average \\(\\alpha_k^2 \\times \\mathbf{c_{j,k}^2}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{j,k}^2}}}\\) \\(c_{j,k}\\) Weighted Average \\(\\alpha_k^2 \\times \\mathbf{c_{j,k}}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{j,k}}}}\\) \\(c_{j,k}^2\\) Least Square \\(\\alpha_k^2 \\times \\mathbf{1}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{1}}}\\) Mean squared error \\(\\begin{align} MSE(\\hat{c}_{i,I}) = \\hat{c}_{i,I}^2 \\Bigg \\{ \\sum_{k = I + 1 - i}^{I-1} \\frac{\\hat{\\alpha}_k^2}{\\hat{f_k}^2} \\Bigg ( \\frac{1}{\\hat{c}_{i,k}} + \\underbrace{\\frac{1}{\\sum_{j=1}^{I-k}c_{j,k}}}_{\\text{Column x latest}}\\Bigg ) \\Bigg \\} \\end{align}\\) \\(\\begin{align} \\hat{\\alpha}_k^2 = \\frac{1}{I - k - 1} \\sum_{j=1}^{I-k} c_{j,k} \\Big ( \\underbrace{\\frac{c_{j,k+1}}{c_{j,k}}-\\hat{f_k}}_{\\text{AY LDFs - Selected}} \\Big )^2 \\end{align}\\) For the last one \\(\\alpha^2_{I-1}\\): If the 3rd last one is lower, take that Else, you take the 2nd last times the ratio of the 2nd last to 3rd last Confidence Interval Normal: \\(\\hat{R}_i \\pm Z_\\alpha \\: s.e.(\\hat{R}_i)\\) LogNormal: \\(R_i \\operatorname{exp}\\left \\{ -\\dfrac{\\sigma_i^2}{2} \\pm \\: Z_\\alpha \\sigma_i \\right \\}\\) \\(\\sigma_i^2 = \\operatorname{ln} \\left [ 1 + \\left ( \\dfrac{s.e.(\\hat{R}_i)}{\\hat{R}_i} \\right)^2 \\right]\\) \\(\\mu_i = \\operatorname{ln}(\\hat{R}_i) - \\dfrac{\\sigma_i^2}{2}\\) Assumptions test Intercepts plot Residuals plot CY Test Correlation on adjacent LDFs 27.1.7 A7 ATA Assumptions Tests Testing the Assumptions of Age-to-Age Factors - G. Venter Standards used in this paper The \\(n\\) for this paper excludes the first column Coefficient \\(&gt; 2 \\sigma\\) is significant Error measures used are Adjusted SSE, AIC and BIC Testable implications from assumptions Statistical significance of \\(f(d)\\) Is there a better estimate for \\(q\\) than \\(f \\times c\\) \\(\\mathbf{q(w,d)}\\) Parameters Comments \\(f(d) c(w,d) + g(d)\\) \\(2m - 2\\) e.g. Least Squares Chainladder \\(m - 1\\) \\(f(d)h(w)\\) \\(2m-2\\) e.g. BF \\(f(d)h\\) \\(m-1\\) e.g. Cape Cod Method \\(\\mathbf{\\operatorname{Var}(q)}\\) \\(\\mathbf{f(d)}\\): Col Parameters \\(\\mathbf{h(w)}\\): Row Parameters BF (Constant Var, Least Square) \\(a(d); p=q=0\\) \\(f(d) = \\dfrac{\\sum_w h^2 \\frac{q}{h}}{\\sum_w h^2}\\) \\(h(w) = \\dfrac{\\sum_d f^2 \\frac{q}{f}}{\\sum_d f^2}\\) Cape Cod \\(a(d); p=q=0\\) \\(f(d) = \\dfrac{\\sum_w h^2 \\frac{q}{h}}{\\sum_w h^2}\\) \\(h = \\dfrac{\\sum_\\Delta f^2 \\frac{q}{f}}{\\sum_\\Delta f^2}\\) BF (Var \\(\\propto\\) \\(fh\\)) \\(a(d) \\cdot f \\cdot h; p=q=1\\) \\(f^2(d) = \\dfrac{\\sum_w h (\\frac{q}{h})^2}{\\sum_w h}\\) \\(h^2(w) = \\dfrac{\\sum_d f (\\frac{q}{f})^2}{\\sum_d f}\\) Here we have to do the calculation recursively starting with either the row or column parameters Check residuals against \\(c(w,d)\\) Stability of \\(f(d)\\) down the column No correlation among columns No particularly high or low diagonals 27.1.8 A8 Curve-Fitting LDF Curve-Fitting and Stochastic Reserving: A Maximum Likelihood Approach - D. Clark \\(x =\\) average age of AY % paid to date growth function: \\(G(x \\mid \\omega, \\theta)\\) Loglogistic: \\(G(x \\mid \\omega, \\theta) = \\dfrac{x^{\\omega}}{x^{\\omega} + \\theta^{\\omega}}\\) Weibull: \\(G(x \\mid \\omega, \\theta) = 1- \\operatorname{exp}\\left\\{ { - \\left( \\dfrac{x}{ \\theta } \\right)^{\\omega}} \\right \\}\\) Expected Ultimate Loss Method 1: Cape Cod \\(Premium_{AY} \\times ELR\\) Method 2: LDF \\(ULT_{AY}\\) Estimate Future Emergence Method 1: Cape Cod \\([G(y \\mid \\omega, \\theta) - G(x \\mid \\omega, \\theta)] \\times [Premium_{AY} \\times ELR]\\) Method 2: LDF \\([G(y \\mid \\omega, \\theta) - G(x \\mid \\omega, \\theta)] \\times ULT_{AY}\\) Truncation Method 1: Cape Cod Truncated @ age \\(x_t\\) Reserve = On-level Premium \\(\\times ELR \\times [(G(x_t) - G(x)]\\) Method 2: LDF Truncated @ age \\(x_t\\) \\(G&#39;(x) = \\dfrac{G(x)}{G(x_t)}\\) Variance of reserve Process Variance of \\(R\\): \\(\\sigma^2 \\sum_i \\mu_i\\) Parameter Variance of \\(R\\): \\(\\operatorname{Var}(\\operatorname{E}[R]) = (\\partial R)&#39;\\Sigma (\\partial R)\\) \\(\\dfrac{Variance}{Mean} = \\sigma^2 = \\dfrac{1}{n-p}\\sum\\limits_{i \\in \\Delta}^n\\dfrac{(c_i - \\mu_i)^2}{\\mu_i}\\) \\(n =\\) # of data points in triangle \\(p =\\) # of parameters Cape Cod \\(p=3\\) (\\(\\omega, \\theta, ELR\\)) LDF \\(p=2 +\\) # of AYs (\\(\\omega, \\theta,\\) row parameters) \\(c_i =\\) actual incremental loss emergence \\(\\mu_i =\\) expected incremental loss emergence 27.1.9 A9 Risk Margin A Framework for Assessing Risk Margins - K. Marshall et al Need framework because quantitative is not enough, need both quantitative and qualitative measures to examine the uncertainty Quantitative analysis requires lots of data Only captures historical risk Does not capture risk that did not have an episode (of systemic risk) in the experience period Independent Parameter and process variance model with stochastic model Internal Systemic Know the 3 components Score against best practice and calibrate to CoV Know how to score given actual examples as in past exam Hindsight analysis Extnernal Systemic Know the different risk categories And what lines they impact most Use benchmark similar to internal but select CoV directly Correlation Risk sources are independent of each other Independent: assume independence across lines, weight by liabilities Internal: base on correlation matrix \\(\\Sigma\\), again weighted by liabilities External: correlation between each valuation group and risk categories \\(\\Rightarrow\\) then roll up to the risk categories and assume they are independent of each other Risk Margin \\(\\text{Risk Margin} = \\underbrace{\\mu}_{\\text{Expected Loss}} \\times \\underbrace{\\phi }_{\\text{CoV}} \\times \\underbrace{Z_{\\alpha}}_{0.67\\text{ for the }75^{th}\\text{ percentile}}\\) Additional analysis Sensitivity, scenario testing Internal benchmarking, important to know the relationships and consistency, been heavily tested in the past External benchmarking Hindsight and mechanical hindsight Regularity of review 27.1.10 A10 Bootstrap Bootstrap Modeling: Beyond the Basics - Shapland Leong Works with incremental triangles \\(m_{wd} = \\operatorname{exp} \\left [\\alpha_w + \\sum_j \\beta_j \\right]\\) \\(\\operatorname{Var}[q(w,d)] = \\phi m_{wd}^z\\) with \\(z=1\\) for ODP 3 methods for dispersion factor First use either GLM or simplified GLM to get the mean and variance for each cell of the triangle and then do the bootstrap procedure Practical Issues Negative incremental values (fit and simulate) 3 potential methods for adjustment when doing model fit Simulate with \\(Gamma(-m_{wd}, -\\phi m_{wd}) + 2m_{wd}\\) Non-zero sum of residuals N-year wtd average Missing values Outliers Heteroskedasticity: Hetero adjustment Partial latest year exposure or partial diagonal Expsoure adjustment Parametric bootstrap Diagnostics Residual graph, normality test, outlier, parameter adj Review results: s.e. and CoV should make sense Other Multiple models: 2 methods for simulation Model outputs Correlations: location mapping or re-sort 27.1.11 A11 Expert Opinions Obtaining Predictive Distributions for Reserves Which Incorporate Expert Opinions - R. Verrall Works with incremental data Model is defined with losses following ODP and stochastic row parameters that follows gamma as prior Bayesian BF calculation and assumptions \\(\\operatorname{E}[c_{ij}] = Z_{ij} \\overbrace{[(\\lambda_j - 1) \\underbrace{D_{ij-1}}_{\\text{Actual up to }j-1}]}^{\\text{CL Results}} + (1- Z_{ij}) \\overbrace{[(\\lambda_j - 1) \\underbrace{M_i p_{j-1}}_{\\text{Expected up to }j-1}]}^{\\text{BF Results}}\\) \\(Z_{ij} = \\dfrac{p_{j-1}}{\\beta_i \\varphi + p_{j-1}}\\) \\(c_{ij} \\sim ODP(x_i \\cdot y_j, \\varphi)\\) \\(x_i \\sim \\Gamma(\\alpha_i, \\beta_i)\\) \\(\\operatorname{E}[x_i] = \\dfrac{\\alpha_i}{\\beta_i} = M_i\\) \\(\\operatorname{Var}[x_i] = \\dfrac{\\alpha_i}{\\beta_i^2}\\) Stochastic column parameters calculation \\(\\operatorname{E}[c_{ij}] = (\\gamma_i - 1) \\sum \\limits_{m=1}^{i-1} c_{m,j}\\) Calculate \\(\\gamma\\) pictorially Using vauge or strong priors for model specification 27.1.12 A12 Reinsurance Reserving Reinsurance Loss Reserving - Patrik Problems of reinsurance reserving: Longer report lag of claims Persistent Upward Development of Most Claim Reserves Reporting Pattern Differ Greatly Industry Statistics Not Useful Reports Received Lack Important Information Data Coding and IT System Problems Reserve to Surplus Ratio is Higher for Reinsurer 6 Components of reinsurance reserve Case reserve reported by cedent Additional case reserve from reinsurer IBNER Pure IBNR Discount for future investment income Risk Load Reinsurance reservering procedure: Partition \\(\\Rightarrow\\) Development Patterns \\(\\Rightarrow\\) Estimate \\(\\Rightarrow\\) Monitor and AvE Partition priority Considerations for short vs medium vs long tail lines AvE Stanard Buhlmann Must on-level the historical premium and adjust to be pure premium Remove commissions and brokerage and internal expenses (but might not worth the effort) Remove any suspected rate level differences \\(ELR = \\dfrac{\\sum C_k}{\\sum E_k p_k}\\) \\(C_k\\): reported to date; \\(E_k\\): Adj Prem; \\(p_k\\): expected % reported to date Estimate ELR using actual incurred loss Sensitive to the accurary of the onlevel EP Credibility IBNR Estimates Weight the CL method with SB or BF \\(R_k = Z \\cdot R_{CL} + (1-Z) \\cdot R_{SB}\\) \\(Z_k = p_k \\cdot CF\\), \\(CF \\in [0,1]\\) \\(CF\\) is the credibility factor, Benktander = 1, BF = 0 27.1.13 A13 Premium Asset Estimating the Premium Asset on Retrospectively Rate Policies - M. Teng and M. Perkins Retro formula \\(\\underbrace{P}_{\\text{Premium}} = [\\underbrace{BP}_{\\text{Basic Premium}} + (\\underbrace{CL}_{\\text{Capped Losses}} \\cdot \\underbrace{LCF}_{\\text{Loss Conversion Factor}})] \\cdot \\underbrace{TM}_{\\text{Tax Multiplier}}\\) Use PDLD to get the premium development based on the loss developement Know what each term means like basic premium factor or charge Formula approach for PDLD \\(\\begin{align} PDLD_1 =\\underbrace{\\left(\\frac{BP}{SP} \\right)}_{\\text{Basic Prem Factor}} \\frac{TM}{ELR \\cdot \\%Loss_1} + \\underbrace{\\left( \\frac{CL_1}{L_1}\\right)}_{\\text{Loss Capping Ratio}} \\cdot LCF \\cdot TM \\end{align}\\) To adjust for being too responsive: \\(\\begin{align} P_1 = \\underbrace{\\left( \\frac{BP}{SP} \\right) \\frac{TM}{ELR \\cdot \\%Loss_1}}_{\\text{Not }\\propto\\text{ Loss}_1} \\times \\operatorname{E}[L_1] + \\underbrace{\\left( \\frac{CL_1}{L_1} \\right) \\cdot LCF \\cdot TM}_{\\propto \\text{ Loss}_1} \\times L_1 \\end{align}\\) Subsequent PDLD: \\(\\begin{array}{cccl} PDLD_n &amp;= &amp;\\dfrac{CL_n - CL_{n-1}}{L_n - L_{n-1}}&amp;LCF \\cdot TM \\:\\:\\:\\:\\text{For }n&gt;1\\\\ &amp;= &amp;\\dfrac{\\Delta CL}{\\Delta L}&amp;LCF \\cdot TM\\\\ \\end{array}\\) Empirical approach for PDLD Assume premium lags Ratio selection Cumulate PDLD based on a weighted average with expected % future report for each future periods Know the practical application First adjustment period might cover more than one policy period Know the Teng Perkins improvements and assumptions 27.1.14 A14 Bayesian MCMC Stochastic Loss Reserving Using Bayesian MCMC Models - G. Meyer Know how they selected the underlying data set to avoid insurer with change in operations or ones that best suit the model The 3 main test used through out: KS test p-p plot Freq vs percentile LCL: \\(e^{\\mu_{wd}} = e^{\\alpha_w}e^{\\beta_d}\\) Same variance parameter (\\(\\sigma_d\\)) for each column of cumulative loss CCL: \\(\\mu_{wd} = \\alpha_w + \\beta_p + \\rho \\cdot \\left[ \\operatorname{ln}\\left(C_{w-1, d}\\right) - \\mu_{w-1,d} \\right]\\) CSR: \\(\\mu_{wd} = \\alpha_w + \\left[ \\beta_d \\cdot (1-\\gamma)^{w-1}\\right]\\) \\(\\gamma &gt;0\\) reflects increase in payment speed as \\((1-\\gamma)^{w-1} &lt; 1\\) \\(\\gamma\\) has less impact further out in the tail as there are less payments happening out there CIT: Uncorrelated log mean of each cell with CY trend \\(\\mu_{wd} = \\alpha_w + \\beta_d + \\tau \\cdot(w+d-1)\\) Draw \\(Z_{wd} \\sim Lognormal(\\mu_{wd},\\sigma_d)\\) \\(\\sigma_1 &gt; \\sigma_2 &gt; \\cdots &gt; \\sigma_{10}\\) Smaller less volatile claims should be settled early \\(\\tilde{I}_{wd} \\sim Normal(Z_{wd},\\delta)\\) Add correlation between AYs for rows after the first \\(\\tilde{I}_{wd} \\sim Normal(Z_{wd} + \\rho \\cdot (\\tilde{I}_{w-1,d} - Z_{w-1,d})\\cdot e^{\\tau},\\delta)\\) LIT: same as CIT but with \\(\\rho = 0\\) Skewed distributions "],
["section-b.html", "27.2 Section B", " 27.2 Section B 27.2.1 B1 Valuation Methods B1 P&amp;C Insurance Company Valuation - R. Goldfarb The key methods are DDM, FCFE, and AE \\(V_0 = \\dfrac{\\mathrm{E}[Div_1]}{k - g}\\) \\(FCFE = NI + (Non \\:Cash\\:Charges) - \\Delta Working \\:Capital - \\Delta Capital + \\Delta Debt\\) \\(\\begin{align} V_0 = BV_0 + \\sum_{t=1} \\frac{(ROE_t - k)BV_{t-1}}{(1+k)^t}\\end{align}\\) You can also look the P:Earning or P:Book multipes which have formulas that’s based on the DDM and AE method Watch out for some nuance on forward and trailing P:Earning Another important bit is to determine the growth rate, if it’s not given in a questions then you have to calculate \\(g\\) over a couple years and select "],
["section-c.html", "27.3 Section C", " 27.3 Section C "],
["formula-appendix.html", "27.4 Formula Appendix", " 27.4 Formula Appendix Calendar Year Test Step 1) Rank the LDFs in each column Step 2) Label them S and L and the median is discarded Step 3) For each diagonal with at least 2 elements, \\(z = \\operatorname{min}(\\text{# of S}, \\text{# of L})\\) Step 4) Calculate \\(\\operatorname{E}[z_n]\\) and \\(\\operatorname{Var}(z_n)\\) \\(\\operatorname{E}[z_n] = \\dfrac{n}{2} - c_n\\) \\(n =\\) # of elements in each diagonal excluding the throw away value \\(c_n = {n - 1 \\choose m}\\frac{n}{2^n}\\) \\(m = \\operatorname{floor}\\left[ \\dfrac{n-1}{2} \\right]\\) \\(\\operatorname{Var}(z_n) = \\dfrac{n(n-1)}{4} - c_n (n-1) + \\operatorname{E}[Z_n] - \\operatorname{E}[z_n]^2\\) \\(z \\sim\\) Normal Step 5) See if \\(Z\\) is in the CI based on the the above \\(Z = \\sum_{diagonal} z\\) Since \\(Z \\sim\\) Normal, can sum the mean and variance by assuming independence Test 95% CI: \\(\\operatorname{E}[Z] \\pm 2 \\times \\sigma\\) Correlation of Adjacent LDFs Use a relatively low threshold of 50% Step 1) Calculate Spearman’s correlation for each pair of adjacent LDFs \\(S = \\sum \\limits_{\\in rows} \\Big \\{ [Rank \\: Col \\: i \\: LDF] - [Rank \\: Col \\: j \\: LDF] \\Big \\}^2\\) Rank from low to high (i.e. lowest is 1) \\(T_k = 1 - \\dfrac{S}{n(n^2-1)/6}\\) \\(n =\\) # of rows For a 10 x 10 triangle, \\(k\\) is at most 7 because there’s only 9 LDFs so 8 pairs. And down to 7 because we don’t use the pair with only 1 row Step 2) Calculate \\(T\\) for the whole triangle \\(T = \\dfrac{\\sum T_k (n_k - 1)}{\\sum (n_k-1)} = \\dfrac{\\sum_k (I - k -1)T_k}{\\sum_k I - k -1}\\) \\(I =\\) size of triangle \\(k\\) starts at 2 Formula gives more weight to \\(T_k\\) with more data Weight goes down to 1 for the last one, the bottom is just the sum of all the weights Step 3) Compare \\(T\\) with CI based on distribution \\(\\operatorname{E}[T] = 0\\) \\(\\operatorname{Var}[T] = \\dfrac{1}{(I-2)(I-3)/2}\\) \\(\\operatorname{E}[T] \\pm Z \\sqrt{\\operatorname{Var}(T)}\\) Use \\(Z = 0.67\\) for range of [25%, 75%] Do not reject the \\(H_0\\) of uncorrelated LDFs if the \\(T\\) is in the CI Error Measures Adjusted SSE = \\(\\dfrac{SSE}{(n-p^2)}\\) \\(AIC \\approx SSE \\times e^{2p/n}\\) \\(BIC \\approx SSE \\times n^{p/n}\\) \\(n =\\) # of predicted data points EXCLUDING 1st column \\(p =\\) # of parameters \\(SSE = \\sum (A - E)^2\\) Here you exclude the first column when calculating the difference Pearson Correlation Correlation \\(r = \\dfrac{\\sum \\tilde{x} \\tilde{y}}{\\sqrt{\\sum \\tilde{x}^2\\sum \\tilde{y}^2}}\\) \\(\\tilde{x} = x - \\bar{x}\\) \\(\\tilde{y} = y - \\bar{y}\\) For the adjacent LDFs, we need to subtract 1 out first Test statistics: \\(T = r \\sqrt{ \\dfrac{n-2}{1-r^2} }\\) \\(T \\sim t_{n-2}\\) Look up the t-value from table for 90% If the absolute value of \\(T &lt;\\) table value \\(\\Rightarrow\\) Not correlated Dispersion Factor Pearson residual: \\(\\begin{array}{cccl} r_{wd} &amp; = &amp; \\dfrac{A - E}{\\sqrt{\\operatorname{Var}(A)}} &amp; \\\\ &amp; = &amp; \\dfrac{q(w,d) - m_{wd}}{\\sqrt{\\phi m_{wd}}} &amp; \\text{Mean &amp; Var Assumptions Above}\\\\ &amp; \\propto &amp; \\dfrac{q(w,d) - m_{wd}}{\\sqrt{m_{wd}}} &amp; \\text{Since }\\phi \\text{ is constant for all}\\\\ \\end{array}\\) Standard: \\(\\phi = \\dfrac{\\sum r_{wd}^2}{n-p}\\) \\(n =\\) # of data points (including first column) \\(p =\\) # of parameters \\(2m-1\\): one for each row, one for each column minus first column England &amp; Verrall: \\(\\phi^{EV} = \\dfrac{\\sum \\left(r^{EV}_{wd}\\right)^2}{n-p}\\) \\(\\begin{array}{llllc} r_{wd}^{EV} &amp; = &amp; r_{wd} &amp;\\times &amp;f \\\\ &amp; = &amp; r_{wd} &amp;\\times &amp;\\sqrt{\\dfrac{n}{n-p}}\\\\ \\end{array}\\) Standarized Residuals: \\(\\phi^H = \\dfrac{\\sum \\left(r_{wd}^H \\right)^2}{n}\\) \\(\\begin{array}{lllc} r_{wd}^H &amp;= r_{wd} &amp;\\times &amp;f_{wd}^H \\\\ &amp;= r_{wd} &amp;\\times &amp;\\sqrt{\\dfrac{1}{1-H_{ii}}} \\\\ \\end{array}\\) \\(H_{ii}\\) is the diagonal of the Hat Matrix Adjustment Factor: \\(H = X (X^TWX)^{-1}X^TW\\) The diagonal is labelled by going down the column of the triangle from left to right The denominator might actually be \\(n-2\\) but for the purpose of exam just use \\(n\\) as stated in the paper Hetero-Adjustment Group the residuals then calculate the \\(\\sigma\\) of the residuals in each group and scale up Hetero-adjustment factor: \\(h^i\\) = the largest \\(\\sigma\\) \\(\\div\\) each group’s \\(\\sigma\\) \\(r_{wd}^{i,H} = r_{wd} \\times f_{wd}^H \\times h^i\\) Residual \\(\\times\\) Hat Matrix Factor \\(\\times\\) Hetero Factor Need to divide the sampled residual by \\(h^i\\) to reflect the variability of group \\(i\\) \\(q^{i*}(w,d) = m_{wd} + \\dfrac{r^{i*}}{h^i}\\sqrt{m_{wd}}\\) Bootstrap Procedure Once we have the mean and residuals in each cell, repeat below: Create a sampled \\(triangle^*\\) from the residuals and the means Sample from residuals since data needs to be \\(iid\\) for bootstrap Use pearson residuals Simulated loss: \\(q^*(w,d) = m_{wd} + r_p \\sqrt{m_{wd}^z}\\) Simulate by sampling residuals with replacement Estimate \\(\\phi\\) for step 4: \\(\\phi = \\dfrac{\\sum r^2}{n-p}\\) Determine parameters from \\(triangle^*\\): \\((\\alpha_i, \\beta_j)\\) Use either GLM or Simplified GLM to project ultimate loss Calculate mean and variance: \\((m_{wd}, \\phi m_{wd})\\) For GLM use \\(m_{wd} = \\operatorname{exp} \\left [\\alpha_w + \\sum_j \\beta_j \\right]\\) For Simplified GLm, back out the \\(c^*(w,d)\\) by \\(\\dfrac{Ult_w}{CDF_d}\\) then get the \\(m_{wd}\\) For variance \\(\\phi m_{wd}^z\\) Add process variance: draw losses from \\(Gamma(m_{wd}^*,\\phi m_{wd}^*)\\) Randomly draw from the distribution for each cell Calculate simulated unpaid: sum of bottom half of triangle GLM For a \\(3\\times 3\\) triangle: Log Actual incremental losses \\(Y = \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix}\\) Solution Matrix \\(W\\) \\(\\begin{array}{ccccc} W &amp; = &amp; X &amp;\\times &amp; A \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\alpha_2 \\:\\:\\: \\alpha_3 \\:\\:\\: \\beta_2 \\:\\:\\: \\beta_3 &amp; &amp;\\\\ \\begin{bmatrix} ln[m_{11}] \\\\ ln[m_{21}] \\\\ ln[m_{31}] \\\\ ln[m_{12}] \\\\ ln[m_{22}] \\\\ ln[m_{13}] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - &amp; - &amp; - \\\\ - &amp; 1 &amp; - &amp; - &amp; - \\\\ - &amp; - &amp; 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 &amp; - \\\\ - &amp; 1 &amp; - &amp; 1 &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 &amp; 1 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\end{bmatrix} \\end{array}\\) \\(W =\\) Solution Matrix \\(X =\\) Design Matrix Defines the parameters used to estimate the losses \\(A =\\) parameters Solve for \\(A\\) by minimizing the difference2 (SSE) between \\(W\\) and \\(Y\\) Use recursive Newton-Raphson method to find the best fit parameters Simplified GLM GLM model = Chainladder w/ volume-weighted averages when: Variance \\(\\propto\\) Mean \\(\\varepsilon \\sim\\) Poisson; Poisson error Parameter for each row and column (x 1st column) KS Test \\(H_0\\): Distribution of \\(p_i\\) is uniform \\(D = \\operatorname{max} \\limits_i \\mid p_i - e_i \\mid\\) Maximum difference between the predicted and expected percentiles Reject \\(H_0\\) if \\(D &gt; \\dfrac{136}{\\sqrt{n}}\\%\\) @5% confidence level e.g. for \\(n = 50\\): 19.2%; \\(n=200\\): 9.6% p-p plot Model is too light tailed: Shallow slope near corner and steep in the middle Model is too heavy tailed: Steep slope near corner and shallow in the middle Model is biased upwards: Bow down Freq Percentile Plot Blue line is based on the uniform \\(e_i\\) set at \\(\\frac{n}{10}\\) for the 10 deciles Skewed Normal Skewed Normal: \\(X = \\mu + (\\omega \\cdot Z) \\cdot \\delta + (\\omega \\cdot \\varepsilon) \\cdot \\sqrt{1 - \\delta^2}\\) \\(\\varepsilon \\sim Normal(0,1)\\) \\(Z \\sim Truncated \\: Normal_{[0,\\infty]} (0,1)\\) \\(\\delta\\) is the weight between \\(\\varepsilon\\) and \\(Z\\) \\(\\omega\\) is the standard deviation Mixed Lognormal-Normal: \\(X \\sim Normal(Z,\\delta)\\) \\(Z \\sim Lognormal(\\mu,\\sigma)\\) Mixed \\(ln - n\\) Distribution Base Triangle Step 1: Setup the trend triangle: Trend = AY Trend \\(\\times\\) CY Trend The 1.000 typically starts at the top left corner but it doesn’t have to Step 2: Determine unlimited mean for each cell in triangle (Avg sev paid to date) Based on mean for the latest AY (bottom row) \\(\\operatorname{E}[C_{nj}] = \\theta_j\\) \\(C_{nj} \\sim \\Phi_{nj} = Exp(\\theta_j)\\) Detrend back up from the bottom row to fill the whole square Usually just need four points per the LDF conversion formula Step 3: Calculate LEV for each cell in triangle \\(L\\) and the last row for \\(B\\) \\(LEV(X; \\Phi \\sim Exp(\\theta)) = \\theta \\: \\left[ 1 - \\operatorname{exp}\\left\\{-\\left(\\dfrac{x}{\\theta}\\right)\\right\\} \\right]\\) Use the “square” of mean \\(\\theta\\) calculated from Step 2 Step 4: Calculate the adjusted triangle Convert triangle of actual losses by dividing it’s LEV at layer \\(L\\) then times it’s LEV at layer \\(B\\) \\(C_{ij}&#39; = \\underbrace{C_{ij}^L}_{\\text{Cum paid @ L}} \\times \\underbrace{\\dfrac{LEV(B;\\Phi_{nj})}{LEV(L;\\Phi_{ij})}}_{\\text{ILF w/ on-level to }n}\\) Step 5: Select base layer LDFs Convert LDFs \\(\\begin{align} F_{ij}^X = F_{nj}^B \\times \\frac{LEV(X;\\Phi_{i\\infty})\\div LEV(B;\\Phi_{n\\infty})}{LEV(X;\\Phi_{ij}) \\div LEV(B;\\Phi_{nj})} \\end{align}\\) \\(\\gamma\\) Calculation \\(\\gamma_1 = 1.000\\) always First calculate LDFs and calculate % unpaid and a-priori for each AY \\(U_i\\) = a-priori ultimate based on LDF for AY \\(i\\) \\(q_i\\) = % unpaid for AY \\(i\\) Note that in step 4 above if you don’t need the individual cells you can just take the \\(U_3 q_3\\) Internal Benchmarking: Compare CoVs within between OCL and PL and also with other valuation groups Independent risk: Large liability will have smaller CoV due to law of large numbers Short tail line will have a small CoV as well due to less volatility Therefore: Outstanding Claims Liability: \\(\\phi_{short \\: tail} &lt; \\phi_{long \\: tail} &lt; \\phi_{long \\: tail, \\: small \\: book}\\) Premium Liability - Long Tail: OCL &gt; PL \\(\\Rightarrow\\) \\(\\phi_{OCL} &lt; \\phi_{PL}\\) Because there are many AYs of claims in the reserves Premium Liability - Short Tail: OCL &lt; PL \\(\\Rightarrow\\) \\(\\phi_{OCL} &gt; \\phi_{PL}\\) Because OCL is small LoB with significant event risk will have different risk profiles for the PL and OCL Internal Systemic: If the methods to estimate liabilities is similar across valuation groups, we would expect the CoV to be similar for classes that have similar claim payment patterns External Systemic: Main sources are higher for long tail lines; Event risk is higher for property; Liability for HO can also be significant 3 components of internal systemic risk: Specification Error: From not perfectly modeling the insurance process because it’s too complicated or just don’t have the data Parameter Selection Error: Trend is particularly difficult to measure Data Error: Lack of data, lack of knowledge of the underlying pricing, u/w, and claim management process, inadequate knowledge of portfolio Risk categories for external Economic and Social Risks: Inflation, social trends Legislative, Political Risks, Claims Inflation Risks: Change in law, frequency of settlement vs suits to completion, loss trend (Long tail lines) Claim Management Process Change Risk: Change in process of managing claims e.g. case reserve practice Expense Risk: Cost of managing a run-off book Event Risk: natural or man-made CAT (Premium liabilities for property) Latent Claim Risk: Claim from source not currently considered to be covered Recovery Risk: Recoveries from reinsurers or non-reinsurers "],
["not-in-2017-syllabus-bootstrap-modeling-beyond-the-basics-shapland-leong.html", "A (Not in 2017 Syllabus) Bootstrap Modeling: Beyond the Basics - Shapland Leong", " A (Not in 2017 Syllabus) Bootstrap Modeling: Beyond the Basics - Shapland Leong Flow of the paper: Parametize \\(\\rightarrow\\) Bootstrap \\(\\rightarrow\\) Practical Issues \\(\\rightarrow\\) Correlations Remember this uses incremental triangles Parametize \\(m_{wd} = \\operatorname{exp} \\left [\\alpha_w + \\sum_j \\beta_j \\right]\\) \\(\\operatorname{Var}[q(w,d)] = \\phi m_{wd}^z\\) Parametize with GLM or Wtd average Bootstrap Create sample trianlge from mean and randomly sampled residuals \\(\\Rightarrow\\) Estimate parameters from sampled triangle \\(\\Rightarrow\\) Calculate mean and variance of the sampled triangle \\(\\Rightarrow\\) Draw losses from gamma Dispersion factor with standard, England &amp; Verrall, and standardized Practical Issues Negative incremental values (fit and simulate) Non-zero sum of residuals N-year wtd average Missing values Outliers Heteroskedasticity Partial latest year exposure or partial diagonal Expsoure adjustment Parametric bootstrap Diagnostics Other Multiple models Model outputs Correlations "],
["introduction-14.html", "A.1 Introduction", " A.1 Introduction Same notations as Venter \\(q(w,d)\\): incremental loss for AY \\(w\\) from age \\(d-1\\) to \\(d\\) \\(c(w,d)\\): cumulative loss Advantages Generates a distribution of the estimate of unpaid claims Can be tailored to statistical features of our data Reflects that loss distn are usually skewed to the right Disadvantages Takes more time to create, but okay once set up Chainladder assumes 1) LDFs are the same for each row 2) Each AY has a parameter representing it’s value, e.g. CL project based on level of losses to date A.1.1 Loss Distribution Mean losses for \\(q(w,d)\\): \\(m_{wd} = \\operatorname{exp} \\left [\\alpha_w + \\sum_j \\beta_j \\right]\\) \\(\\alpha_i\\) and \\(\\beta_j\\) are selected to minimize error between \\(\\operatorname{ln}(actual) - \\operatorname{ln}(forecast)\\) Equivalence for using Venter notation: \\(h(w) = e^{\\alpha}\\) \\(f(d) = e^{\\sum \\beta}\\) Variance \\(\\operatorname{Var}[q(w,d)] = \\phi m_{wd}^z\\) \\(\\phi\\): Dispersion factor Estimate from residual \\(z\\): Error distribution Paper focus on \\(z = 1\\) for Over Dispersed Poisson (ODP) z Distribution 0 Normal 1 Poisson 2 Gamma 3 Inverse Gaussian "],
["parameterize-with-glm-1.html", "A.2 Parameterize with GLM", " A.2 Parameterize with GLM For a \\(3\\times 3\\) triangle: Log Actual incremental losses \\(Y = \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix}\\) Solution Matrix \\(W\\) \\(\\begin{array}{ccccc} W &amp; = &amp; X &amp;\\times &amp; A \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\alpha_2 \\:\\:\\: \\alpha_3 \\:\\:\\: \\beta_2 \\:\\:\\: \\beta_3 &amp; &amp;\\\\ \\begin{bmatrix} ln[m_{11}] \\\\ ln[m_{21}] \\\\ ln[m_{31}] \\\\ ln[m_{12}] \\\\ ln[m_{22}] \\\\ ln[m_{13}] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - &amp; - &amp; - \\\\ - &amp; 1 &amp; - &amp; - &amp; - \\\\ - &amp; - &amp; 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 &amp; - \\\\ - &amp; 1 &amp; - &amp; 1 &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 &amp; 1 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\end{bmatrix} \\end{array}\\) \\(W =\\) Solution Matrix \\(X =\\) Design Matrix Defines the parameters used to estimate the losses \\(A =\\) parameters Solve for \\(A\\) by minimizing the difference2 (SSE) between \\(W\\) and \\(Y\\) Use recursive Newton-Raphson method to find the best fit parameters Paper did no cover in what sense these parameters are best fit (Not SSE?) "],
["simplified-glm-1.html", "A.3 Simplified GLM", " A.3 Simplified GLM GLM model = Chainladder w/ volume-weighted averages when: Variance \\(\\propto\\) Mean \\(\\varepsilon \\sim\\) Poisson; Poisson error Parameter for each row and column (x 1st column) Benefits: Replace GLM fitting with much simpler calculation LDFs are easier to explain Still works even when there are negative incremental values A.3.1 Residuals (Pearson) Memorize Formula \\(\\begin{array} Rr_p &amp; = &amp; \\dfrac{A - E}{\\sqrt{\\operatorname{Var}(A)}} &amp; \\\\ &amp; = &amp; \\dfrac{q(w,d) - m_{wd}}{\\sqrt{\\phi m_{wd}}} &amp; \\text{Mean &amp; Var Assumptions Above}\\\\ &amp; \\propto &amp; \\dfrac{q(w,d) - m_{wd}}{\\sqrt{m_{wd}}} &amp; \\text{Since }\\phi \\text{ is constant for all}\\\\ \\end{array}\\) Mean &amp; Var Note the residual for the 2 corners of the triangle are going to be 0 because only the row parameter or column parameter are used "],
["bootstrap-procedures-1.html", "A.4 Bootstrap Procedures", " A.4 Bootstrap Procedures Once we have the mean and residuals in each cell, repeat below: Create a sampled \\(triangle^*\\) from the residuals and the means Sample from residuals since data needs to be \\(iid\\) for bootstrap Use Pearson residuals Simulated loss: \\(q^*(w,d) = m_{wd} + r_p \\sqrt{m_{wd}^z}\\) Memorize Formula Simulate by sampling residuals with replacement Estimate \\(\\phi\\) for step 4: \\(\\phi = \\dfrac{\\sum r^2}{n-p}\\) This can vary see Dispersion Factor section Determine parameters from \\(triangle^*\\): \\((\\alpha_i, \\beta_j)\\) Use either GLM or Simplified GLM to project ultimate loss Calculate mean and variance: \\((m_{wd}, \\phi m_{wd})\\) For GLM use \\(m_{wd} = \\operatorname{exp} \\left [\\alpha_w + \\sum_j \\beta_j \\right]\\) For Simplified GLm, back out the \\(c^*(w,d)\\) by \\(\\dfrac{Ult_w}{CDF_d}\\) then get the \\(m_{wd}\\) For variance \\(\\phi m_{wd}^z\\) Add process variance: draw losses from \\(Gamma(m_{wd}^*,\\phi m_{wd}^*)\\) Randomly draw from the distribution for each cell Calculate simulated unpaid: sum of bottom half of triangle "],
["dispersion-factor-1.html", "A.5 Dispersion Factor", " A.5 Dispersion Factor Standard \\(\\phi = \\dfrac{\\sum r_{wd}^2}{n-p}\\) \\(n =\\) # of data points (including first column) \\(p =\\) # of parameters \\(2m-1\\): one for each row, one for each column minus first column England &amp; Verrall Degrees of freedom adjustments \\(\\phi^{EV} = \\dfrac{\\sum \\left(r^{EV}_{wd}\\right)^2}{n-p}\\) \\(\\begin{array}{llllc} r_{wd}^{EV} &amp; = &amp; r_{wd} &amp;\\times &amp;f \\\\ &amp; = &amp; r_{wd} &amp;\\times &amp;\\sqrt{\\dfrac{n}{n-p}}\\\\ \\end{array}\\) Standarized Residuals Pinheiro proposed to standardized residuals so they all have same variance \\(\\phi^H = \\dfrac{\\sum \\left(r_{wd}^H \\right)^2}{n}\\) \\(\\begin{array}{lllc} r_{wd}^H &amp;= r_{wd} &amp;\\times &amp;f_{wd}^H \\\\ &amp;= r_{wd} &amp;\\times &amp;\\sqrt{\\dfrac{1}{1-H_{ii}}} \\\\ \\end{array}\\) \\(H_{ii}\\) is the diagonal of the Hat Matrix Adjustment Factor: \\(H = X (X^TWX)^{-1}X^TW\\) The diagonal is labelled by going down the column of the triangle from left to right The denominator might actually be \\(n-2\\) but for the purpose of exam just use \\(n\\) as stated in the paper "],
["variations-to-the-odp-bootstrap-1.html", "A.6 Variations to the ODP Bootstrap", " A.6 Variations to the ODP Bootstrap Use incurred loss triangle Convert ultimate from incurred to a payment stream based on paid analysis Correlate the simulations from the 2 method so that if we have large payment @ an older age, the incurred should be large as well Use BF Ult for recent years Have the a-priori varies based on some \\(\\sigma\\) Generalizing the ODP Model Reduce # of parameters Combine accident years Use just 1 development year since the column parameters is just a measure of decay and can be used for multiple columns Add calendar year trend parameter \\(\\gamma_k\\) for each column except for the first Must use GLM to determine the parameters "],
["practical-issues-1.html", "A.7 Practical Issues", " A.7 Practical Issues A.7.1 Negative Incremental Values How to deal with negative incremental values in the triangle A.7.1.1 Model Fitting Method 1: \\(-ln(-q(w,d))\\) Works when there are a few cells with negatives Doesn’t work when the column sum to a negative value Method 2: Add a constant \\(\\Psi\\) Add \\(\\Psi\\) to every cell before running GLM then subtract \\(\\Psi\\) from each incremental loss once the means are calculated \\(ln[m_{wd} + \\Psi] = \\eta_{wd}\\) Can use this method combined with method 1 to take care of the extra large negative ones Method 3: Simplified GLM Use Chainladder with volume weighted average LDFs Only if the assumptions fit Need to make use the absolute value for the residual and re-sampling formula: \\(r_{wd} = \\dfrac{q(w,d)-m_{wd}}{\\sqrt{abs(m_{wd})}}\\) \\(q^*(w,d) = m_{wd} + r_p \\sqrt{abs(m_{wd})}\\) A.7.1.2 Simulating Negative Values Adjustment to the Gamma Distribution Use \\(Gamma(-m_{wd}, -\\phi m_{wd}) + 2m_{wd}\\) Maintaining the right tail but also having the mean of \\(m_{wd}\\) Just doing \\(-Gamma(-m_{wd}, -\\phi m_{wd})\\) doesn’t work since the tail would be flipped Extreme Outcomes from Negative Values Column with negative mean can results in vary large LDFs, 4 options to deal with that: Remove the extreme iterations But beware of understating the the likelihood of extreme outcomes Recalibrate the Model Review data used and parameter selection (e.g. remove the first AY as it does not represent current behavior) Limit Incremental Losses Below 0 Either in the original triangle, sampled or simulated loss (process var step); Replace with 0 Can just do it in certain columns Understand Understand why this is happening. e.g. if due to S&amp;S then you can just model them separately and then correlated them during simulation A.7.2 Non-Zero Sum of Residuals Since we assume \\(iid\\) and constant \\(\\sigma\\), the sum of residuals should be 0 Not necessarily the case since this is just a sample Consequence is that the simulated outcomes will be higher than the means 2 Options Keep it if we believe this to be characteristics of the data set Add a constant to each residual so that it sums to 0 and then sample from the adjusted residuals A.7.3 Using N-year Weighted Average GLM Exclude the older diagonals so that we have \\(N+1\\) diagonals of data to get \\(N\\) diagonals of LDFs Will have less CY parameters Simplified GLM Get N-year weighted average Exclude the diagonals not used for the LDFs when determining the residuals Sample residuals for the entire triangle when sampling for bootstrap since you need cumulative losses for each row The 2 methods will results in different results GLM: Models the incremental losses in the trapezoid Simplified GLM: Model the same losses but in relation to the cumulative losses, which include the non-modeled losses in the diagonals excluded A.7.4 Missing Value Solution Estimate from surrounding values Modify LDFs to exclude missing value Won’t have residual for this cell A.7.5 Outliers Remove extreme values that are not representative of the variability of the losses Remove a whole row or, Just remove the value and treat them as missing values or, Exclude in LDFs but continue to use them when calculating residuals and use them for the re-sampling A.7.6 Heteroskedasticity Non constant variance (bootstrap assumes residuals are \\(iid\\)) Stratified Sampling Split the triangle into groups with similar variance and only sample residuals that are in the group Cons: each group may not be that large Hetero-Adjustment Group the residuals then calculate the \\(\\sigma\\) of the residuals in each group and scale up Hetero-adjustment factor: \\(h^i\\) = the largest \\(\\sigma\\) \\(\\div\\) each group’s \\(\\sigma\\) \\(r_{wd}^{i,H} = r_{wd} \\times f_{wd}^H \\times h^i\\) Residual \\(\\times\\) Hat Matrix Factor \\(\\times\\) Hetero Factor Need to divide the sampled residual by \\(h^i\\) to reflect the variability of group \\(i\\) \\(q^{i*}(w,d) = m_{wd} + \\dfrac{r^{i*}}{h^i}\\sqrt{m_{wd}}\\) A.7.7 Heteroecthesious Data Accident years have different level of exposures Partial First Development Period Only want partial accident year No impact to residuals for bootstrap 2 options: Reduce the mean of the incremental cells by pro ration in the process variance step Prorate after the process variance step Partial Last Calendar Period Latest diagonal is partial Simplified GLM Determine LDF excluding latest diagonal then interpolate LDFs for ultimate GLM Adjust the exposure in the last diagonal to make them consistent with the rest of the triangle (probably means adjusting annualizing the loss) Then prorate the losses similar to the first scenario A.7.8 Exposure Adjustment Consider dividing the losses by the exposure in each AY if there are significant changes in exposure and model pure premium Multiply the PP results by the exposure after the process variance step A.7.9 Parametric Bootstrapping Might not have enough data to sufficiently represent the tail Fit a distribution to the residual and sample from the distribution instead "],
["diagnostics-1.html", "A.8 Diagnostics", " A.8 Diagnostics Judge the quality of the model Test Assumptions in model Gauge quality of model fit Guide the adjustments of model parameters A.8.1 Residual Graphs Graph residuals vs CY, AY, Age, forecast loss Want to see random variability around zero A.8.2 Normality Test Normality is not required, only need this if we’re doing parametric bootstrap with normal distribution Plot residuals against the normal best fit based on the percentiles Use p-value &gt; 5% as the test Or use something that penalize number of parameters \\(AIC = 2p + n \\left [ 1 + \\operatorname{ln}(2\\pi\\dfrac{RSS}{n})\\right]\\) \\(BIC = n \\operatorname{ln}\\left( \\dfrac{RSS}{n}\\right) + p \\operatorname{ln}(n)\\) \\(RSS\\) = actual residual - expected residual from normal A.8.3 Outlier Remove true outliers but do not want to remove points that are realistic extreme scenarios Use box &amp; whisker plot Shows 25%ile to 75%ile Whiskers are 3 times the inter quartile range A.8.4 Parameter Adjustments Test model with different sets of parameters Don’t need unique parameter for each row and column A.8.5 Review Model Results Read summarized output by AYs Mean, s.e., CoV, Min, Max, Median The all year s.e. should be greater than any individual year The all year CoV should be less than the CoV for any individual year CoV should be highest for older years due small mean unpaid CoV also high for most recent AY due to higher volatility Larger parameter uncertainty or volatility from CL method Check min max for reasonability For the triangles: Check incremental means as well in triangle form Check s.d. of incremental values "],
["using-multiple-models-1.html", "A.9 Using Multiple Models", " A.9 Using Multiple Models Use different methods (Paid/Inc’d Dev, BF, etc) by assigning weights by AYs Method 1: In the process variance step of bootstrap, use the same underlying U(0,1) to draw from each model then weight the models by the % Method 2: Run each model independently for each simulation (i.e. use different U(0,1)) then for each AY use the weights to randomly select one of the modeled results. Results will be a mixture of the various models Important to review the statistics in the above section for each output Fit the unpaid claim distribution to Normal, LogNormal, and Gamma. Then compare with the fit based on the actual residuals on various statistics Not sure what distribution this is talking about A.9.1 Other Model Outputs Estimated Cash Flow Results Since bootstrap generates simulation for each cell in the bottom half of the triangle we can use this to get cash flow forecasts by CY and the percentiles as well Estimated Ultimate Loss Ratio Results We can estimate the variability of ultimate loss ratio since we vary and simulate the whole “square” Distribution Graphs Draw a distribution of the simulated unpaid in a histogram Can also smooth the histogram with Kernel density function For each point it takes a weighted average of the points around it; giving less weight to points further from it "],
["correlation-1.html", "A.10 Correlation", " A.10 Correlation Correlate the loss distribution over several LoB Multivariate distribution requires the same underlying distribution which doesn’t work here for ODP Location Mapping When sampling the residuals, sample from the same place in the triangle for all the lines we want to correlate Disadvantages: Requires all LoB to have the same size triangle with no missing values or outliers Cannot stress the correlations among the LoBs Re-Sorting Use Iman-Conover algorithms or Copulas Advantages: Can accommodate different shapes and sizes Can make different correlation assumptions Can strengthen the correlation for extreme events (e.g. Copulas) Calculate correlation matrix using Spearman’s Rank Order Re-sorting based on the ranks of unpaid claims by AYs Using residuals to correlate LoBs (Both location mapping &amp; re-sorting) are both liable to create correlations close to zero Reserve Risk: Correlate total unpaid by correlating the incremental paid. May or may not be a reasonable approximation Pricing Risk: Correlate loss ratios over time Not as likely to be close to zero Use different correlation assumption than for reserve risk "],
["miscellaneous-1.html", "A.11 Miscellaneous", " A.11 Miscellaneous Model Testing Based on testing from General Insurance Reserving Oversight Committee, the ODP Bootstrap with England &amp; Verrall residual out perform the Mack model by forecasting the 99%-ile better ODP losses only exceed 99%-ile ~3% of the time compare to Mack’s 8-13% Future Research Test ODP bootstrap on realistic data from CAS loss simulation model Expand ODP bootstrap with Munich Chainladder, claim counts and severity Research other risk analysis measures and use for ERM Use for SII requirements Research in correlation matrix (difficult to estimate) "],
["past-exam-questions-15.html", "A.12 Past Exam Questions", " A.12 Past Exam Questions Exercises \\(\\star\\) Use simplied GLM and then back out the GLM parameters Reduce parameters Minimize square error for GLM Benefit of simplified GLM Residuals Dispersion Dispersion with hat matrix adj Simulate loss Setup GLM Negative values Simulat ngative Partial triangle Stratified Dealing with correlation Outliers Practical Issues 2013 #7: negatvie values 2014 #7: List 4 practical issues and solutions 2015 #10: Heteroscedasticity, why important, adjustments description 2015 #11: Negative values, outliers, exposure level Diagnostics \\(\\star\\) 2014 #9: Evaluate the results given mean unpaid, s.e., CoV by AYs A.12.1 Question Highlights n/a -->"]
]
