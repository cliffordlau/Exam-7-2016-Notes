[
["measuring-the-variability-of-chain-ladder-reserve-estimate-t-mack.html", "Chapter 4 Measuring the Variability of Chain Ladder Reserve Estimate - T. Mack", " Chapter 4 Measuring the Variability of Chain Ladder Reserve Estimate - T. Mack Know the 3 assumptions of chain ladder Know the 3 different weight and variance assumptions Weight \\(w_{j,k}\\) Description Variance Residual 1 Simple Average \\(\\alpha_k^2 \\times \\mathbf{c_{j,k}^2}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{j,k}^2}}}\\) \\(c_{j,k}\\) Weighted Average \\(\\alpha_k^2 \\times \\mathbf{c_{j,k}}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{j,k}}}}\\) \\(c_{j,k}^2\\) Least Square \\(\\alpha_k^2 \\times \\mathbf{1}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{1}}}\\) Mean squared error calculation \\(\\begin{align} MSE(\\hat{c}_{i,I}) = \\hat{c}_{i,I}^2 \\Bigg \\{ \\sum_{k = I + 1 - i}^{I-1} \\frac{\\hat{\\alpha}_k^2}{\\hat{f_k}^2} \\Bigg ( \\frac{1}{\\hat{c}_{i,k}} + \\underbrace{\\frac{1}{\\sum_{j=1}^{I-k}c_{j,k}}}_{\\text{Column x latest}}\\Bigg ) \\Bigg \\} \\end{align}\\) \\(\\begin{align} \\hat{\\alpha}_k^2 = \\frac{1}{I - k - 1} \\sum_{j=1}^{I-k} c_{j,k} \\Big ( \\underbrace{\\frac{c_{j,k+1}}{c_{j,k}}-\\hat{f_k}}_{\\text{AY LDFs - Selected}} \\Big )^2 \\end{align}\\) \\(\\hat{\\alpha}_{I-1}^2 = \\operatorname{min} \\left( \\dfrac{(\\hat{\\alpha}_{I-2}^2)^2}{\\hat{\\alpha}_{I-3}^2},\\hat{\\alpha}_{I-3}^2 \\right) = \\begin{cases} \\hat{\\alpha}_{I-3}^2 &amp; \\text{if } \\hat{\\alpha}_{I-3}^2 &lt; \\hat{\\alpha}_{I-2}^2 \\\\ \\hat{\\alpha}_{I-2}^2 \\times \\dfrac{\\hat{\\alpha}_{I-2}^2}{\\hat{\\alpha}_{I-3}^2} &amp; \\text{else}\\\\ \\end{cases}\\) Confidence Interval Normal Log-normal Know the 4 test of assumptions Test 1. Intercept Test 2. Residuals Test 3. CY Test Rank small and large \\(\\operatorname{E}[z_n] = \\dfrac{n}{2} - c_n\\) \\(c_n = {n - 1 \\choose m}\\frac{n}{2^n}\\) \\(m = \\operatorname{floor}\\left[ \\dfrac{n-1}{2} \\right]\\) \\(\\operatorname{Var}(z_n) = \\dfrac{n(n-1)}{4} - c_n (n-1) + \\operatorname{E}[Z_n] - \\operatorname{E}[z_n]^2\\) \\(Z = \\sum_{diagonal} z\\) Since \\(Z \\sim\\) Normal, can sum the mean and variance assuming \\(\\perp\\!\\!\\!\\perp\\) Test 95% CI: \\(\\operatorname{E}[Z] \\pm 2 \\times \\sigma\\) Test 4. Adjacent LDF Correlation \\(S = \\sum \\limits_{\\in rows} \\Big \\{ [Rank \\: Col \\: i \\: LDF] - [Rank \\: Col \\: j \\: LDF] \\Big \\}^2\\) \\(T_k = 1 - \\dfrac{S}{n(n^2-1)/6}\\) \\(T = \\dfrac{\\sum T_k (n_k - 1)}{\\sum (n_k-1)} = \\dfrac{\\sum_k (I - k -1)T_k}{\\sum_k I - k -1}\\) \\(\\operatorname{E}[T] \\pm Z \\sqrt{\\operatorname{Var}(T)}\\) \\(\\operatorname{E}[T] = 0\\) \\(\\operatorname{Var}[T] = \\dfrac{1}{(I-2)(I-3)/2}\\) Use \\(Z = 0.67\\) for range of [25%, 75%] "],
["cl-ass.html", "4.1 Chain Ladder Assumptions", " 4.1 Chain Ladder Assumptions Definition 4.1 Notations use for Mack \\(c_{i,k} =\\) cumulative losses for AY \\(i\\) @ age \\(k\\) \\(f_k =\\) LDF from \\(k\\) to \\(k + 1\\) \\(I =\\) size of the triangle Proposition 4.1 (Chain Ladder Assumption 1) \\(\\operatorname{E}\\left [c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ] = c_{i,k} \\: f_k\\) Expected incremental losses are \\(\\propto\\) losses reported to date Proportion depends on the age of AY Proposition 4.2 (Chain Ladder Assumption 2) \\(\\left \\{c_{i,1} \\cdots c_{i,I} \\right \\} \\: {\\perp\\!\\!\\!\\!\\perp} \\: \\left \\{c_{j,1} \\cdots c_{j,I} \\right \\}\\) for \\(i \\neq\\ j\\) Losses in each AYs are \\({\\perp\\!\\!\\!\\!\\perp}\\) of the losses in other AYs This assumption make our estimate unbiased Proposition 4.3 (Chain Ladder Assumption 3) \\(\\operatorname{Var}\\left (c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ) = \\alpha_k^2 \\: c_{i,k}\\) Variance of the incremental losses is \\(\\propto\\) losses reported to date Proportion depends on the age (\\(k\\)) of AY (i.e. same for each column but varies for each column) \\(\\hat{f}_k\\) is selected to minimize the variance Need to double check on the below: Proposition 4.4 (Chain Ladder Assumption 4) Variance is \\(\\propto\\) mean Based on assumption 1 and 3 (proposition 4.1 &amp; proposition 4.3) If variance follows distribution x, then it implies the variance is \\(\\propto\\) loss 4.1.1 Implications of Assumptions When using CL, we are making assumptions on how we select the factor \\(\\hat{f}_k\\) and the application \\(\\hat{c}_{i,k+1} = c_{i,k} \\times \\hat{f}_k\\) Which requires the following assumptions: Unbiased estimate of each \\(f_k\\) \\(\\mathrm{E}\\left[ \\hat{f}_k \\right] = f_k\\) \\(\\hat{f}_k\\) are representative of the true \\(f_k\\) Based on assumption 1 &amp; 2 (prop. 4.1 &amp; prop. 4.2) Unbiased estimate of Ultimate \\(\\mathrm{E}\\left[ \\hat{c}_{iI} \\right] = c_{ik} \\times \\hat{f}_k \\times \\cdots \\times \\hat{f}_{I-1} = \\mathrm{E}\\left[ c_{iI} \\right]\\) Multiplying the \\(\\hat{f}_k\\)’s by the paid to date will give us an unbiased estimate of the future losses Based on assumption 1 &amp; 2 (prop. 4.1 &amp; prop. 4.2) To use volume weighted average LDF Based on assumption 3 (prop. 4.3) To calculate the confidence interval Based on assumption 3 (prop. 4.3) 4.1.2 Proof for Assumption 3 Work in progress section To estimate \\(f_k\\) we can weight the historical LDFs in many different ways, putting it in general terms: \\[\\hat{f}_k = \\sum_i \\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right) \\times w_{i,k} \\:\\:\\:\\: : \\:\\:\\:\\: \\sum_i w_{i,k} = 1\\] Remark. We assume each of the \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) are an unbiased estimate of \\(f_k\\) From assumption 1 &amp; 2? \\(\\therefore\\) Any weighting of them is also unbiased Remark. When we use weighted volume average \\(w_{i,k} = \\dfrac{c_{i,k}}{\\sum \\limits_{j} c_{j,k}}\\) The weights that minimize the variance is inversely proportional to the variance of the item we are estimating We want weights \\(w_{i,k}\\) for each \\(i\\) on \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) The weight we apply to each \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) varies based on the variance of \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) And if \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) has high variance its weight will be lower to minimize the total variance of our estimate of \\(\\hat{f}_k = \\left( \\dfrac{c_{1,k+1}}{c_{1,k}} \\right) \\times w_{1,k} + \\cdots + \\left( \\dfrac{c_{I,k+1}}{c_{I,k}} \\right) \\times w_{I,k}\\) High variance estimate get lower weight: \\[\\dfrac{1}{w_{i,k}} \\propto \\mathrm{Var}\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\] Since \\(c_{i,k}\\) is known, we can pull it out of the variance term \\[\\dfrac{1}{w_{i,k}} \\propto \\dfrac{\\mathrm{Var}(c_{i,k+1})}{c_{i,k}^2}\\] and we get \\[\\begin{equation} w_{i,k} \\times \\mathrm{Var}(c_{i,k+1}) \\propto c_{i,k}^2 \\tag{4.1} \\end{equation}\\] Recall the weight for the volume weighted average is: \\[\\begin{align} w_{i,k} &amp;= \\dfrac{c_{i,k}}{\\sum \\limits_{j} c_{j,k}}\\\\ w_{i,k} &amp;\\propto c_{i,k}\\\\ \\end{align}\\] Applying the above to equation (4.1) we get: \\[\\begin{align} c_{i,k} \\times \\mathrm{Var}(c_{i,k+1}) &amp;\\propto c_{i,k}^2 \\\\ \\mathrm{Var}(c_{i,k+1}) &amp;\\propto c_{i,k} \\\\ \\mathrm{Var}(c_{i,k+1}) &amp;= \\alpha^2_k \\times c_{i,k} \\\\ \\end{align}\\] And we have chainladder assumption 3 (Prop. 4.3) "],
["ldf-selections-assumptions.html", "4.2 LDF Selections Assumptions", " 4.2 LDF Selections Assumptions \\(\\begin{align} \\hat{f_k} = \\frac{\\sum_j \\overbrace{\\frac{c_{j,k+1}}{c_{j,k}}}^{LDF_j} \\: w_{j,k}}{\\sum_j w_{j,k}} \\end{align}\\) Weight \\(w_{j,k}\\) Description Variance Residual 1 Simple Average \\(\\alpha_k^2 \\times \\mathbf{c_{j,k}^2}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{j,k}^2}}}\\) \\(c_{j,k}\\) Weighted Average \\(\\alpha_k^2 \\times \\mathbf{c_{j,k}}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{j,k}}}}\\) \\(c_{j,k}^2\\) Least Square \\(\\alpha_k^2 \\times \\mathbf{1}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{1}}}\\) Use different method for LDF selection based on variance assumption Variance and the weight always multiply to \\(c_{j,k}^2\\) "],
["mean-squared-error.html", "4.3 Mean Squared Error", " 4.3 Mean Squared Error Important Formulas \\(\\begin{align} MSE(\\hat{c}_{i,I}) = \\operatorname{E}[(c_{i,I}-\\hat{c}_{i,I})^2 \\mid D] \\end{align}\\) Average error between estimate and actual given data \\(D\\) Standard error: \\(s.e. = \\sqrt{MSE}\\) \\(\\begin{align} MSE(\\hat{c}_{i,I}) = \\hat{c}_{i,I}^2 \\Bigg \\{ \\sum_{k = I + 1 - i}^{I-1} \\frac{\\hat{\\alpha}_k^2}{\\hat{f_k}^2} \\Bigg ( \\frac{1}{\\hat{c}_{i,k}} + \\underbrace{\\frac{1}{\\sum_{j=1}^{I-k}c_{j,k}}}_{\\text{Column x latest}}\\Bigg ) \\Bigg \\} \\end{align}\\) The big \\(\\sum\\) sum over the remaining years till ultimate For bit outside the \\(\\sum\\) is same for the row Proportions for the variance \\(\\begin{align} \\hat{\\alpha}_k^2 = \\frac{1}{I - k - 1} \\sum_{j=1}^{I-k} c_{j,k} \\Big ( \\underbrace{\\frac{c_{j,k+1}}{c_{j,k}}-\\hat{f_k}}_{\\text{AY LDFs - Selected}} \\Big )^2 \\end{align}\\) \\(\\hat{\\alpha}_{I-1}^2 = \\operatorname{min} \\left( \\dfrac{(\\hat{\\alpha}_{I-2}^2)^2}{\\hat{\\alpha}_{I-3}^2},\\hat{\\alpha}_{I-3}^2 \\right) = \\begin{cases} \\hat{\\alpha}_{I-3}^2 &amp; \\text{if } \\hat{\\alpha}_{I-3}^2 &lt; \\hat{\\alpha}_{I-2}^2 \\\\ \\hat{\\alpha}_{I-2}^2 \\times \\dfrac{\\hat{\\alpha}_{I-2}^2}{\\hat{\\alpha}_{I-3}^2} &amp; \\text{else}\\\\ \\end{cases}\\) This one varies by age (columns), for each age: Calculate the difference2 of the LDFs for each AY and the selected Then multiply by each row’s respective cumulative loss for that age Sum the above and then divide by the number of rows minus 1 For the last one: If the 3rd last one is lower, take that Else, you take the 2nd last times the ratio of the 2nd last to 3rd last "],
["confidence-intervals.html", "4.4 Confidence Intervals", " 4.4 Confidence Intervals Can have different assumptions on the distribution of the unpaid Formulas applies to total reserve as well Need \\(MSE(\\hat{R})\\) Correlate the individual \\(\\hat{R}_i\\)’s. Because we use the same LDFs to forecast the estimates Can just use the square root rule to sum up the different AYs if we assume independence Normal \\(\\hat{R}_i \\pm Z_\\alpha \\: s.e.(\\hat{R}_i)\\) LogNormal Memorize Formulas \\(R_i \\operatorname{exp}\\left \\{ -\\dfrac{\\sigma_i^2}{2} \\pm \\: Z_\\alpha \\sigma_i \\right \\}\\) \\(\\sigma_i^2 = \\operatorname{ln} \\left [ 1 + \\left ( \\dfrac{s.e.(\\hat{R}_i)}{\\hat{R}_i} \\right)^2 \\right]\\) \\(\\mu_i = \\operatorname{ln}(\\hat{R}_i) - \\dfrac{\\sigma_i^2}{2}\\) "],
["chain-ladder-assumptions-test.html", "4.5 Chain Ladder Assumptions Test", " 4.5 Chain Ladder Assumptions Test Look at various test on the Chain Ladder assumptions 4.5.1 Intercept Test for assumption (1) Plot the losses at adjacent ages to see if the line of best fit goes through the origin 4.5.2 Residuals Test for assumption (1) For each age \\(k\\), graph the \\(c_{i,k}\\) with the residuals \\(\\varepsilon_{i,k}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\operatorname{Var}(c_{j,k})}}\\) e.g. \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{c_{j,k}}}\\) for the variance of the incremental losses is \\(\\propto\\) losses reported to date scenarios; Proportional term doesn’t affect the variance since it’s the same for the column For residuals @ t, you need LDFs from t-1 to t They should vary randomly around zero If passed \\(\\Rightarrow\\) expected losses are linear w.r.t. cumulative losses paid to date Can also use to test assumption (3) on the other variance assumption by calculating the \\(\\varepsilon\\) differently 4.5.3 Calendar Year Test Test for assumption (2) Step 1) Rank the LDFs in each column Step 2) Label them S and L and the median is discarded Step 3) For each diagonal with at least 2 elements, \\(z = \\operatorname{min}(\\text{# of S}, \\text{# of L})\\) Step 4) Calculate \\(\\operatorname{E}[z_n]\\) and \\(\\operatorname{Var}(z_n)\\) \\(\\operatorname{E}[z_n] = \\dfrac{n}{2} - c_n\\) Memorize Formulas \\(n =\\) # of elements in each diagonal excluding the throw away value \\(c_n = {n - 1 \\choose m}\\frac{n}{2^n}\\) \\(m = \\operatorname{floor}\\left[ \\dfrac{n-1}{2} \\right]\\) \\(\\operatorname{Var}(z_n) = \\dfrac{n(n-1)}{4} - c_n (n-1) + \\operatorname{E}[Z_n] - \\operatorname{E}[z_n]^2\\) Memorize Formulas \\(z \\sim\\) Normal Step 5) See if \\(Z\\) is in the CI based on the the above \\(Z = \\sum_{diagonal} z\\) Since \\(Z \\sim\\) Normal, can sum the mean and variance by assuming independence Test 95% CI: \\(\\operatorname{E}[Z] \\pm 2 \\times \\sigma\\) If the observed is outside the range \\(\\Rightarrow\\) There is calendar year effects, fail assumption (2) CY effect can be caused by changing or high inflation; change in claims handling; change in legal environment 4.5.4 Correlation of Adjacent LDFs Test assumption (1) where expected only depends on most recent Use a relatively low threshold of 50% Step 1) Calculate Spearman’s correlation for each pair of adjacent LDFs Memorize Formulas \\(S = \\sum \\limits_{\\in rows} \\Big \\{ [Rank \\: Col \\: i \\: LDF] - [Rank \\: Col \\: j \\: LDF] \\Big \\}^2\\) Rank from low to high (i.e. lowest is 1) \\(T_k = 1 - \\dfrac{S}{n(n^2-1)/6}\\) \\(n =\\) # of rows For a 10 x 10 triangle, \\(k\\) is at most 7 because there’s only 9 LDFs so 8 pairs. And down to 7 because we don’t use the pair with only 1 row Step 2) Calculate \\(T\\) for the whole triangle Memorize Formulas \\(T = \\dfrac{\\sum T_k (n_k - 1)}{\\sum (n_k-1)} = \\dfrac{\\sum_k (I - k -1)T_k}{\\sum_k I - k -1}\\) \\(I =\\) size of triangle \\(k\\) starts at 2 Formula gives more weight to \\(T_k\\) with more data Step 3) Compare \\(T\\) with CI based on distribution Memorize Formulas \\(\\operatorname{E}[T] = 0\\) \\(\\operatorname{Var}[T] = \\dfrac{1}{(I-2)(I-3)/2}\\) \\(\\operatorname{E}[T] \\pm Z \\sqrt{\\operatorname{Var}(T)}\\) Use \\(Z = 0.67\\) for range of [25%, 75%] Do not reject the \\(H_0\\) of uncorrelated LDFs if the \\(T\\) is in the CI Venter has a method too "],
["past-exam-questions.html", "4.6 Past Exam Questions", " 4.6 Past Exam Questions Haven’t done TIA practice questions No historical questions on the MSE yet Concepts 2011 #3 b: CL method assumptions 2012 #5: CL assumptions and whether they are met 2014 #2: CL assumptions on intercept and residuals 2014 #4 b: CL LS assumptions Assumption test \\(\\star\\) 2011 #3 a (fig 4.1): CY Test 2013 #3: Residuals test 2013 #1: CY test and potential cause \\(\\star\\) 2014 #4 a (fig 4.2): Adjacent LDFs correlation test Has to use method from Venter? 2015 #3: Residuals test and plot, remember to label them 2015 #4: CY Test 4.6.1 Question Highlights Figure 4.1: 2011 Question 3 Figure 4.1: 2011 Question 3 Figure 4.2: 2014 Question 4 Figure 4.2: 2014 Question 4 -->"]
]
