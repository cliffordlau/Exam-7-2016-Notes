[
["using-the-odp-bootstrap-model-a-practitioners-guide.html", "Chapter 9 Using the ODP Bootstrap Model: A Practitioner’s Guide", " Chapter 9 Using the ODP Bootstrap Model: A Practitioner’s Guide Notations are similar to Venter See Table 9.1 for sample triangle layout Model Parameters: Mean (9.1) Variance (9.3) and dispersion factor Need residual: unscaled (9.8), scaled (9.9), standardized (9.10) Parameterize with GLM or simplified (ODP) (Note the condition required for ODP) Simulation procedure Parameter variance (9.14) Process variance (9.15) Other variation of the bootstrap (e.g. incurred, BF, Cape Cod) Pros and Cons of GLM GLM vs ODP bootstrap Practical issues: Negative Incremental Values During fitting: (9.16) and (9.17) During simulation: Non‐Zero Sum of Residuals Using an L-year Weighted Average Missing Values Outliers Heteroscedasticity Stratified sampling Hetero adjustment to residuals Non-constant scale parameter Heteroecthesious Data Exposure Adjustment Tail Factors Fitting a Distribution to ODP Bootstrap Residuals 5 diagnostics Residual graphs Normality test Outliers Parameter adjustment Model results Multiple models Model testing "],
["introduction.html", "9.1 Introduction", " 9.1 Introduction Paper focus on over-dispersed Poisson (ODP) bootstrap Incremental losses are modeled as ODP random variable Goal is to generate a distribution of possible outcomes Just FYI, not important for exam Other papers on bootstrap Statistics: Bradley Efron (1979) Actuarial: England &amp; Verrall (1999; 2002), Pinheiro, et al. (2003), Kirschner, et al. (2008) Practical motivation for modeling loss distribution Definition of actuarial estimate in ASOP 43 can be based on a first moment from a distribution While ASOP 36 (SAO) focus on deterministic point estimates SEC is looking for more information on reserving risk in the 10-K Rating agencies are building dynamic risk models and welcome actuary input Companies that use dynamic risk models for internal risk management need unpaid claim distributions SII and IFAS are moving towards unpaid claim distribution Advantages of bootstrap Generates a distribution of the estimate of unpaid claims Can be tailored to statistical features of our data Reflects that loss distn are usually skewed to the right Disadvantages of bootstrap Takes more time to create, but okay once set up 9.1.1 Stochastic vs Static Model ODP bootstrap is a specific form of GLM Benefit of GLM: It can be specifically tailored to the statistical features found in the data Contrast with algorithms that force the data to be fit to a static model (fig. 9.1) Figure 9.1: Stochastic vs Static Model Diagram Just FYI, not important for exam Some authors define a model as having a defined structure and error distribution, so under this more restrictive definition bootstrapping would be considered to be a method or algorithm However, using a less restrictive definition of a model as an algorithm that produces a distribution, bootstrapping would be defined as a model "],
["shapland-notations.html", "9.2 Notations", " 9.2 Notations Definition 9.1 Same notations as Venter for \\(n \\times n\\) triangle \\(w\\): Accident (exposure) year \\(d\\): Development age \\(q(w,d)\\): incremental loss for AY \\(w\\) from age \\(d-1\\) to \\(d\\) \\(c(w,d)\\): cumulative loss \\(F(d)\\): Incremental LDF from age \\(d\\) to \\(d+1\\) \\(f(d)\\): \\(F(d) - 1\\), for forcasting incremental losses \\(G(w)\\): Factor relating to accdient (exposure) year \\(w\\); ultimate gross level \\(h(k)\\): Factor relating to the diagonal \\(k\\) along which \\(w+d\\) is constant Table 9.1: Incremental triangle with corresponding row and column parameters - \\(\\beta_2\\) \\(\\beta_3\\) \\(\\alpha_1\\) \\(q(1,1)\\) \\(q(1,2)\\) \\(q(1,3)\\) \\(\\alpha_2\\) \\(q(2,1)\\) \\(q(2,2)\\) \\(\\alpha_3\\) \\(q(3,1)\\) Chainladder assumptions: LDFs are the same for each row \\(F(w,d) = F(d)\\) Each AY has a parameter representing it’s level e.g. CL project based on level of losses to date "],
["bootstrap-model.html", "9.3 Bootstrap Model", " 9.3 Bootstrap Model Benefits of the bootstrap model: Allows us to estimate the distribution with very little data We don’t have to make any assumptions about the underlying distribution (non-parametric) The ODP part is the error distribution ODP bootstrap models: Incremental claims directly as the response With the same linear predictor as Kremer (1982) Using a GLM with log-link function and an ODP Poisson error Where a specific form of this model is identical to the volume weighted chain ladder Using bootstrap (sampling residuals with replacement) to estimate the distribution of point estimates (Instead of simulating from a multivariate normal for a GLM) 9.3.1 GLM Parameters Mean and variance for each \\(q(w,d)\\) in the triangle (per table 9.1) 9.3.1.1 Mean and log-mean for \\(q(w,d)\\) \\[\\begin{equation} \\mathrm{E}[q(w,d)] = m_{wd} = \\exp \\left [\\alpha_w + \\sum_{i=2}^d \\beta_i \\right] \\:\\: : \\: \\: w \\in [2, n] \\tag{9.1} \\end{equation}\\] \\[\\begin{equation} \\ln \\left( \\mathrm{E}[q(w,d)] \\right) = \\ln(m_{w,d}) = \\eta_{w,d} = \\alpha_w + \\sum_{i=2}^d \\beta_i \\:\\: : \\: \\: w \\in [2, n] \\tag{9.2} \\end{equation}\\] Remark. \\(\\alpha\\)’s are the individual level parameters \\(\\beta\\)’s adjust for the development trends after the first development period We don’t use \\(\\beta_1\\) which effectively means \\(\\beta_1 = 0\\) \\(\\alpha_i\\) and \\(\\beta_j\\) are selected to minimize error between \\(\\ln(actual) - \\ln(forecast)\\) Equivalence for using Venter notation: \\(h(w) = e^{\\alpha}\\) \\(f(d) = e^{\\sum \\beta}\\) 9.3.1.2 Variance for \\(q(w,d)\\) \\[\\begin{equation} \\mathrm{Var}[q(w,d)] = \\phi m_{wd}^z \\tag{9.3} \\end{equation}\\] \\(\\phi\\): Dispersion factor Scale factor estimated as part of the fitting procedure while setting the variance proportional to the mean Estimated from the residuals \\(z\\): Error distribution Paper focus on \\(z = 1\\) for Over Dispersed Poisson (ODP) Specifies the whole mean-variance relationship (not only the first 2 moments) Table 9.2: Distribution with corresponding \\(z\\) \\(z\\) Distribution 0 Normal 1 Poisson 2 Gamma 3 Inverse Gaussian 9.3.2 Fitted Triangle We can fit the \\(\\alpha\\)’s and \\(\\beta\\)’s defined above using the GLM framework, or the simplified GLM method 9.3.2.1 Parameterize with GLM Framework Start with a \\(3 \\times 3\\) incremental triangle Table 9.3: \\(3\\times 3\\) incremental triangle: w/d 1 2 3 1 \\(q(1,1)\\) \\(q(1,2)\\) \\(q(1,3)\\) 2 \\(q(2,1)\\) \\(q(2,2)\\) 3 \\(q(3,1)\\) Log transform of the triangle Table 9.4: \\(3\\times 3\\) log incremental triangle: w/d 1 2 3 1 \\(\\ln[q(1,1)]\\) \\(\\ln[q(1,2)]\\) \\(\\ln[q(1,3)]\\) 2 \\(\\ln[q(2,1)]\\) \\(\\ln[q(2,2)]\\) 3 \\(\\ln[q(3,1)]\\) Create a system of equations based on equation (9.2) \\[\\begin{equation} \\begin{split} \\ln[q(1,1)] &amp;= 1\\alpha_1 + 0\\alpha_2 + 0\\alpha_3 + 0\\beta_2 + 0\\beta_3 \\\\ \\ln[q(2,1)] &amp;= 0\\alpha_1 + 1\\alpha_2 + 0\\alpha_3 + 0\\beta_2 + 0\\beta_3 \\\\ \\ln[q(3,1)] &amp;= 0\\alpha_1 + 0\\alpha_2 + 1\\alpha_3 + 0\\beta_2 + 0\\beta_3 \\\\ \\ln[q(1,2)] &amp;= 1\\alpha_1 + 0\\alpha_2 + 0\\alpha_3 + 1\\beta_2 + 0\\beta_3 \\\\ \\ln[q(2,2)] &amp;= 0\\alpha_1 + 1\\alpha_2 + 0\\alpha_3 + 1\\beta_2 + 0\\beta_3 \\\\ \\ln[q(1,3)] &amp;= 0\\alpha_1 + 0\\alpha_2 + 1\\alpha_3 + 1\\beta_2 + 1\\beta_3 \\\\ \\end{split} \\tag{9.4} \\end{equation}\\] Express the above in matrix form \\[\\begin{equation} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\alpha_2 \\:\\:\\: \\alpha_3 \\:\\:\\: \\beta_2 \\:\\:\\: \\beta_3 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - &amp; - &amp; - \\\\ - &amp; 1 &amp; - &amp; - &amp; - \\\\ - &amp; - &amp; 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 &amp; - \\\\ - &amp; 1 &amp; - &amp; 1 &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 &amp; 1 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\end{bmatrix} \\end{array} \\tag{9.5} \\end{equation}\\] Remark. \\(\\mathbf{X}\\) is the design matrix that defines the parameters used to estimate the losses in each cell Use iteratively weighted least squares or MLE1 to solve for the parameters in the in \\(\\mathbf{A}\\) that minimize the squared difference between \\(\\mathbf{Y}\\) and \\(\\mathbf{S}\\), the solution matrix \\[\\begin{equation} \\mathbf{S} = \\begin{bmatrix} ln[m_{1,1}] \\\\ ln[m_{2,1}] \\\\ ln[m_{3,1}] \\\\ ln[m_{2,1}] \\\\ ln[m_{2,2}] \\\\ ln[m_{1,3}] \\\\ \\end{bmatrix} \\tag{9.6} \\end{equation}\\] After solving the system of equations we will have: \\[\\begin{equation} \\begin{split} \\ln[m_{1,1}] &amp;= \\eta_{1,1} &amp;= \\alpha_1 \\\\ \\ln[m_{2,1}] &amp;= \\eta_{2,1} &amp;= \\alpha_2 \\\\ \\ln[m_{3,1}] &amp;= \\eta_{3,1} &amp;= \\alpha_3 \\\\ \\ln[m_{1,2}] &amp;= \\eta_{1,2} &amp;= \\alpha_1 + \\beta_2\\\\ \\ln[m_{2,2}] &amp;= \\eta_{2,2} &amp;= \\alpha_2 + \\beta_2\\\\ \\ln[m_{1,3}] &amp;= \\eta_{1,3} &amp;= \\alpha_1 + \\beta_2 + \\beta_3\\\\ \\end{split} \\tag{9.7} \\end{equation}\\] The above solution shown as a triangle below Table 9.5: \\(3\\times 3\\) GLM fitted log incremental triangle: w/d 1 2 3 1 \\(\\ln[m_{1,1}]\\) \\(\\ln[m_{1,2}]\\) \\(\\ln[m_{1,3}]\\) 2 \\(\\ln[m_{2,1}]\\) \\(\\ln[m_{2,2}]\\) 3 \\(\\ln[m_{3,1}]\\) Exponentiate the triangle above to get our fitted (or expected) incremental results of the GLM model Table 9.6: \\(3\\times 3\\) GLM fitted incremental triangle: w/d 1 2 3 1 \\(m_{1,1}\\) \\(m_{1,2}\\) \\(m_{1,3}\\) 2 \\(m_{2,1}\\) \\(m_{2,2}\\) 3 \\(m_{3,1}\\) 9.3.2.2 Simplified GLM GLM model = Chainladder w/ volume-weighted averages when: Variance \\(\\propto\\) Mean \\(\\varepsilon(w,d) \\sim\\) Poisson A parameter for each row and column (except 1st column) Benefits: Replace GLM fitting with much simpler calculation LDFs are easier to explain Still works even when there are negative incremental values Procedure for fitting incremental triangle: Select LDFs based on vol-wtd Start from the last cumulative diagonal and divide backwards by each incremental LDFs to get the cumulative fitted triangle Subtracting out the cumulative diagonals to get your incremental fitted triangle 9.3.3 Residuals Unscaled Pearson residuals \\[\\begin{equation} \\begin{split} r_{w,d} &amp; = &amp; \\dfrac{A - E}{\\sqrt{\\mathrm{Var}(E)}} &amp;\\\\ &amp; = &amp; \\dfrac{q(w,d) - m_{wd}}{\\sqrt{m^z_{wd}}} &amp;\\\\ &amp; = &amp; \\dfrac{q(w,d) - m_{wd}}{\\sqrt{m_{wd}}} &amp; \\:\\:\\:\\: \\text{Recall }z = 1\\text{ for ODP Poisson}\\\\ \\end{split} \\tag{9.8} \\end{equation}\\] Mean and variance as defined above Residual for the right and bottom corners of the triangle are going to be 0 Because a unique parameter is used for those 2 cells Alternatively we can use Anscombe residual We prefer Pearson because its calculation is consistent with the scale parameter \\(\\phi\\) Scaled Pearson residuals (England &amp; Verrall) \\[\\begin{equation} r^S_{w,d} = r_{w,d} \\times \\underbrace{\\sqrt{\\dfrac{N}{N-p}}}_{f^{DoF}} \\tag{9.9} \\end{equation}\\] Degrees of freedom adjustment, to effectively allow for over dispersion of the residuals in the sampling process and add process variance to approximate a distribution of possible outcomes Increase the variability of the pseudo triangle Standardized residuals (Pinheiro et al.) \\[\\begin{equation} r^H_{w,d} = r_{w,d} \\times \\underbrace{\\sqrt{\\dfrac{1}{1-H_{i,i}}}}_{f^H_{w,d}} \\tag{9.10} \\end{equation}\\] \\[\\begin{equation} \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{WX})^{-1}\\mathbf{X}^T\\mathbf{W} \\tag{9.11} \\end{equation}\\] \\[\\begin{equation} \\mathbf{W} = \\begin{bmatrix} m_{1,1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; m_{2,1} &amp; 0 &amp; 0 \\\\ \\vdots &amp; 0 &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; \\cdots &amp; m_{1,n}\\\\ \\end{bmatrix} \\tag{9.12} \\end{equation}\\] Hat matrix adjustment factor \\(f^H_{w,d}\\) is based on the diagonal on the hat matrix \\(\\mathbf{H}\\) (Going down the column of the triangle from left to right) \\(\\mathbf{W}\\) is a \\(2n \\times 2n\\) matrix \\(\\mathbf{X}\\) is the design matrix from (9.5) Benefits: \\(f^H_{w,d}\\) account for the exclusion of zero-value residuals Or the zero-value residuals will have some variance but we just don’t know what it is yet so we should sample from the remaining residuals but not the zeros \\(f^H_{w,d}\\) is an improvement on \\(f^{DoF}\\) 9.3.4 Dispersion Factor Dispersion factor \\[\\begin{equation} \\phi = \\dfrac{\\sum r_{wd}^2}{N-p} \\tag{9.13} \\end{equation}\\] \\[N = \\dfrac{n (n+1)}{2}\\] \\[p = 2n-1\\] \\(N =\\) # of data points (including first column unlike Ventor) \\(N\\) can be less than indicated above if the tail incremental developments are all 0’s \\(p =\\) # of parameters One for each row, one for each column minus first column \\(p\\) can be less than \\(2n-1\\) if the later incremental values are all 0’s and therefore not needed for fitting Alternate method for \\(\\phi\\) \\[\\phi \\sim \\phi^H = \\dfrac{\\sum (r^H_{w,d})^2}{N}\\] We can still use the same dispersion factor even with the scaled and standardized residuals, this just give us another method to estimate \\(\\phi\\) You can also use other methods such as orthogonal decomposition or Newton-Raphson to solve for the parameters↩ "],
["odp-boot-sim.html", "9.4 Bootstrap Simulation Procedure", " 9.4 Bootstrap Simulation Procedure Bootstrap simulation procedure, repeat steps 1 - 5 at least 10,000 times Model our losses, determine mean and residual for each cell This can be based on GLM Framework of simplified GLM Create a sampled \\(triangle^*\\) from the residuals and the means Sample with replacement on the Pearson residuals (9.8) from our original triangle from Step 0. (Since data needs to be \\(iid\\) for bootstrap) (Note the distribution of the residual will be purely empirical) Simulated loss: \\[\\begin{equation} q^*(w,d) = m_{wd} + r_p^* \\sqrt{m_{wd}^z} \\tag{9.14} \\end{equation}\\] (\\(r^*_p\\) is the realized sample from i.) Compile the sampled cumulative triangle Estimate dispersion factor \\(\\phi\\) for Step 3 This can vary per Dispersion Factor section Determine parameters from \\(triangle^*\\): For GLM Framework, calculate the \\(\\alpha_w\\)’s, \\(\\beta_d\\)’s For Simplified GLM calculate the weighted average LDFs and Ultimate Loss Calculate mean and variance2 for the future cells (unpaid): \\((m^*_{wd}, \\phi m^*_{wd})\\) Mean: \\(m^*_{wd}\\) GLM Framework: \\(m^*_{wd} = \\mathrm{exp} \\left [\\alpha_w + \\sum \\limits_{i=2}^d \\beta_i \\right]\\) Simplified GLM Back out the \\(c^*(w,d)\\) by \\(\\dfrac{Ult_w}{CDF_d}\\) then get the \\(m^*_{wd}\\) Variance: \\(\\phi m_{wd}^{*}\\) (for ODP \\(z=1\\) on the \\(m^{*z}_{wd}\\)) Add process variance: draw losses3 from the following Gamma distribution for the future cells (unpaid): \\[\\begin{equation} Gamma(m_{wd}^*,\\phi m_{wd}^*) \\tag{9.15} \\end{equation}\\] Simulate loss from the gamma distribution for each future cells Use \\(u \\sim U[0,1]\\) and \\(F^{-1}_{gamma}(u)\\) Calculate simulated unpaid: sum the bottom half of triangle Step 3 and 3 were added in England &amp; Verrall (2002), in their 1999 paper it doesn’t have this step (stopped at 2) and instead just suggest you to add process variance by multiply the results by \\(f^{RoF}\\)↩ Poisson distribution can be used to remain more consistent with the underlying theory of GLM framework, but it is considerably slower to simulate, so gamma is a close substitute that performs much faster in simulation although it can be more skewed than the Poisson. Indeed other distributions could be used as well to better approximate the observed “skewness” of the residuals from the diagnostics↩ "],
["bootstrap-variation-shap.html", "9.5 Variations on the ODP Bootstrap", " 9.5 Variations on the ODP Bootstrap Reason to use paid data: For insurance risk it is best to focus on the claim payment stream: It measures the variability of the actual cash flows that directly affect the bottom line Case reserves temper the volatility Changes in case reserves and IBNR reserves will also impact the bottom line, but to a considerable extent the changes in IBNR are intended to counter the impact of the changes in case reserves To some degree, then, the total reserve movements can act to mask the underlying changes due to cash flows Reason to include case reserves: Case reserves contain valuable information about potential future payments 9.5.1 Bootstrapping the Incurred Loss Triangle 2 approaches to model the unpaid loss distribution using incurred loss triangle Method 1: Modeling the incurred data and convert the ultimate values to a payment pattern Run the paid and incurred data model in parallel For each iteration and each AY individually: Use the payment pattern (from paid model) to convert the ultimate values (from incurred model) to a payment stream Method 1 Advantages: We improve the ultimate estimates by incorporating the case reserves while still focusing on the payment stream for measuring risk Which effectively allows a distribution of IBNR to become a distribution of IBNR and case reserves Can make it more sophisticated by correlating some part of the paid and incurred models (e.g. the residual sampling and/or process variance portions) So that if we have large payment @ an older age, the incurred should be large as well Method 2: Applying the ODP bootstrap to the Munich chain ladder model See Liu and Verrall (2010) Method 2 Advantages: Don’t have to model the paid loss twice Explicitly measuring and imposing a framework around the correlation of the paid and outstanding losses 9.5.2 Bootstrapping the BF and Cape Cod Method ODP issue: Distribution for the most recent AYs can produce results with more variance than you would expect when compared to earlier AYs in the actual data Due More LDFs are used to extrapolate the sampled values for the most recent accident years and the random samples of incremental values Similar to the leverage effect of the deterministic chainladder Solution: Incorporate BF or Cape Cod Have the a-priori be stochastic e.g. draw the BF a-priori from a distribution or apply Cape Cod to each simulated triangle More complicated approach is to modify the underlying assumptions of the GLM framework which would results in a completely different set of residuals (this is beyond scope) "],
["GLM-bootstrap.html", "9.6 GLM Bootstrap Model", " 9.6 GLM Bootstrap Model Limitations of ODP Bootstrap carry over from chainladder Does not adjust for CY trend May over fit the data from using too many parameters We can solve the above by going back to the GLM framework instead of using the Simplified GLM when we’re at Step 2 of the simulation GLM benefits Not forced to use a specific number of parameters (e.g. GLM Variation 1 and 2) Allows for CY trend Can work with shapes that are non triangles (e.g. data with only the last \\(x\\) diagonals) We can forecast past the end of the triangle (e.g. have the \\(\\beta\\)’s continue the decay) Also see section on practical issues GLM drawbacks Solving GLM at each iteration can slow down the process Model not explainable using development factors Below subsections are just examples of the variations discussed above 9.6.1 GLM Variation 1: Reduce Row Parameters Use only 1 AY parameter \\(\\alpha_1\\) Similar to Venter Cap Code method ?? with \\(h(w) = h\\) Moves away from the Chainladder assumption that each AY has its own level Also note that the residual in cell (3,1) will no longer be zero since it shares the \\(\\alpha_1\\) with all the other rows Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\beta_2 \\:\\:\\: \\beta_3 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; - \\\\ 1 &amp; 1 &amp; - \\\\ 1 &amp; 1 &amp; - \\\\ 1 &amp; 1 &amp; 1 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] 9.6.2 GLM Variation 2: Reduce Column Parameters Use only 1 development year parameter \\(\\beta_2\\) This just assumes the losses decay by \\(e^{-\\beta_2}\\) for all ages Also note that the residual in cell (1,3) will no longer be zero since it shares the \\(\\beta_2\\) with all the other columns Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\alpha_2 \\:\\:\\: \\alpha_3 \\:\\:\\: \\beta_2 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - &amp; - \\\\ - &amp; 1 &amp; - &amp; - \\\\ - &amp; - &amp; 1 &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 \\\\ - &amp; 1 &amp; - &amp; 1 \\\\ 1 &amp; - &amp; - &amp; 2 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\beta_2 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] 9.6.3 GLM Variation 3: Reduce Row and Column Parameters Further reduce the parameters to just 1 row and 1 column parameter \\(\\alpha_1\\) and \\(\\beta_2\\) Flexibility of the GLM Bootstrap so that we’re not always stuck with \\(p = 2n-1\\) as stated earlier This will gives us 6 residuals to sample from (the corners will not longer be 0’s) Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) See diagnostics section on how to determine which parameters are statistically significant \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\beta_2 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - \\\\ 1 &amp; - \\\\ 1 &amp; - \\\\ 1 &amp; 1 \\\\ 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\beta_2 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] 9.6.4 GLM Variation 4: Calendar Year Parameter We can also add calendar year trend \\(\\gamma_k\\) where \\(k\\) is the CY \\(\\gamma_2\\) is the 2nd diagonal and etc \\(\\gamma_k\\)’s are incremental decay similar to the \\(\\beta_d\\)’s \\(\\therefore\\) The total impact on the 3rd diagonal is \\(e^{\\gamma_2 + \\gamma_3}\\) Note that the model here have 7 parameters and 6 values \\(\\therefore\\) It has no unique solution Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\alpha_2 \\:\\:\\: \\alpha_3 \\:\\:\\: \\beta_2 \\:\\:\\: \\beta_3 \\:\\:\\: \\gamma_2 \\:\\:\\: \\gamma_3 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -\\\\ - &amp; 1 &amp; - &amp; - &amp; - &amp; 1 &amp; -\\\\ - &amp; - &amp; 1 &amp; - &amp; - &amp; 1 &amp; 1\\\\ 1 &amp; - &amp; - &amp; 1 &amp; - &amp; 1 &amp; -\\\\ - &amp; 1 &amp; - &amp; 1 &amp; - &amp; 1 &amp; 1\\\\ 1 &amp; - &amp; - &amp; 1 &amp; 1 &amp; 1 &amp; 1\\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\gamma_2 \\\\ \\gamma_3 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] 9.6.5 GLM Variation 5: One Parameter for Each Dimension Again we can simplify things by having only 1 parameters for each dimension: \\(\\alpha_1\\), \\(\\beta_2\\), and \\(\\gamma_2\\) Use this as a starting point then add or remove parameters as needed Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\beta_2 \\:\\:\\: \\gamma_2 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; 1 \\\\ 1 &amp; - &amp; 2 \\\\ 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 2 \\\\ 1 &amp; 2 &amp; 2 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\beta_2 \\\\ \\gamma_2 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] "],
["odp-vs-glm-shap.html", "9.7 ODP vs GLM Bootstrap Summary", " 9.7 ODP vs GLM Bootstrap Summary ODP Bootstrap is a specific case of the GLM model with the following parameters: Parameters for every AY Parameters for every development year minus the first Variance of the incremental losses \\(\\propto\\) mean Solution is the same as volume weighted Chainladder GLM Bootstrap is the general model Can have as few as 1 row and/or column parameters Can include CY trend Variance of the incremental losses $\\(m^z_{wd}\\) (where \\(z=1\\) for this paper) "],
["odp-prac-issues.html", "9.8 Practical Issues", " 9.8 Practical Issues Practical issues we might run into with ODP bootstrap Negative Incremental Values Non‐Zero Sum of Residuals Using an L-year Weighted Average Missing Values Outliers Heteroscedasticity Heteroecthesious Data Exposure Adjustment Tail Factors Fitting a Distribution to ODP Bootstrap Residuals 9.8.1 Negative Incremental Values GLM doesn’t work with negative incremental values because of \\(\\ln[q(w,d)]\\) Need to work around this in: Model fitting (e.g. Step 0 and 1 of the Bootstrap process) Simulating for process variance with negative means (e.g. Step 4 of the Bootstrap process) Also additional work around on extreme outcomes from negative values 9.8.1.1 Model Fitting Method 1: Use \\(-ln(abs\\{q(w,d)\\})\\) \\[\\begin{equation} Cell_{w,d} = \\begin{cases} \\ln[q(w,d)] &amp; \\text{if } q(w,d) &gt; 0 \\\\ 0 &amp; \\text{if } q(w,d) = 0 \\\\ -\\ln[abs \\{ q(w,d) \\}] &amp; \\text{if } q(w,d) &lt; 0 \\\\ \\end{cases} \\tag{9.16} \\end{equation}\\] Remark. Doesn’t work when the column sum to a negative value Method 2: Subtract a negative constant \\(\\Psi\\) \\[\\begin{equation} q^+(w,d) = q(w,d) - \\Psi \\\\ \\ln[q^+(w,d)] \\text{ for all } Cell_{w,d} \\tag{9.17} \\end{equation}\\] Pick \\(\\Psi =\\) largest negative value in the column Apply (9.17) before solving the GLM system of equations (e.g. (9.2) and (9.4)) Then adjust the fitted values by adding back \\(\\Phi\\) to reduce each fitted incremental value \\[\\begin{equation} m_{w,d} = m^+_{w,d} + \\Psi \\tag{9.18} \\end{equation}\\] Can use this method combined with method 1 to take care of the extra large negative ones Need to make use the absolute value for the residual and re-sampling formula, modify (9.8) and (9.14) with below: \\[\\begin{equation} r_{wd} = \\dfrac{q(w,d)-m_{wd}}{\\sqrt{abs\\{m^z_{wd}\\}}} \\tag{9.19} \\end{equation}\\] \\[\\begin{equation} q^*(w,d) = m_{wd} + r^*_p \\sqrt{abs\\{m^z_{wd}\\}} \\tag{9.20} \\end{equation}\\] Method 3: Use simplified GLM Use ODP bootstrap (i.e. Chainladder with volume weighted average LDFs) This will yield different estimate than using the GLM framework with adjustment 1 or 2 9.8.1.2 Simulating Negative Values From above, we might have the fitted \\(m_{wd}\\) that are negative, which will be an issue when used in Step 4 of the bootstrap simulation, when we need to model the process variance with \\(Gamma(m_{wd},\\phi m_{wd})\\) Since \\(Gamma\\) only takes positive parameters Adjustment to the Gamma Distribution with negative \\(m_{wd}\\) \\[\\begin{equation} Gamma(abs\\{m_{wd}\\}, \\phi abs\\{m_{wd}\\}) + 2m_{wd} \\tag{9.21} \\end{equation}\\] This will maintain the right skew of Gamma while having the mean of \\(m_{wd}\\) Alternatively if we use \\(-Gamma(abs\\{m_{wd}\\}, \\phi abs\\{m_{wd}\\})\\) it’ll flip the curve to skew left 9.8.1.3 Extreme Outcomes from Negative Values Column with negative mean in the early ages can results in vary large LDFs (and lead to simulated outcomes that are 1,000 times greater than our mean) Negative mean causes one column of cumulative values to sum close to 0 and the next to sum to a much larger number resulting in extremely large LDF and there for projection that are extremely large Need to address this as it’ll throw off the mean even if you don’t care about the high percentiles 3 options to address this: Remove the extreme iterations Beware of understating the the likelihood of extreme outcomes Recalibrate the Model First need to identify the source of the negative losses Review data used and parameter selection e.g. remove the AYs that might not represent current behavior e.g. if due to S&amp;S then you can just model them separately and then correlate them during simulation Limit Incremental Losses to 0 Either with the simulated mean (Step 2) or the process var step (Step 4) Replace with negatives with 0s Can just do it in certain columns 9.8.2 Non-Zero Sum of Residuals Residuals are supposed to be iid with mean zero and constant variance \\(\\therefore\\) Sum of our residuals from the triangle should be 0 Not necessarily the case since this is just a sample Consequence: Simulated outcomes will be higher than the mean if sum of residuals are positive (and vice versa) 2 options to address this: Keep it if we believe this to be characteristics of the data set Add a constant to each non-zero residual so that it sums to 0 Then sample from the adjusted residuals If residuals are significantly different from zero then the fit of the model should be questioned 9.8.3 Using L-year Weighted Average Select LDFs based on the latest \\(L\\) years GLM Bootstrap Only use \\(L+1\\) diagonals of data to get \\(L\\) diagonals of LDFs Excluded diagonals are given zero weight and we’ll have less CY trend parameter (if we’re using it) In the simulation we’ll only sample residuals for the trapezoid used to parameterize the model (since that’s all we’ll need to estimate parameters) Simplified GLM Get L-year weighted average LDFs Will only have residuals (to sample from) for the most recent L + 1 diagonals In the simulation we’ll create the entire resampled triangle (Since we need the cumulative losses for each row) For projection using the resampled triangle we’ll still only use the L-year average LDFs The 2 methods will results in different results GLM Bootstrap: Models the incremental losses in the trapezoid Simplified GLM: Models the same losses but in relation to the cumulative losses, which include the non-modeled losses in the diagonals excluded 9.8.4 Missing Value ODP Bootstrap: Missing data impact: LDFs Fitted triangle (if missing value lies on the most recent diagonal) Residuals Degree of freedom Solutions: Impute from surrounding values Modify LDFs to exclude missing value Similar to the L-year weighted average: Missing value will be resampled so the cumulative losses can be calculated Projection from the resampled triangle will exclude the missing cell for resampled LDF selection GLM Bootstrap Impact on the is limited, we’ll just have less observations 9.8.5 Outliers Remove outliers if they are not representative of the variability of the losses, below are the options: Remove the entire row (easy if it’s the 1st row of the triangle) Remove the values and treat them as missing values Not use the residual but do create a sampled value in that cell Significant number of outliers might indicate bad model fit GLM Bootstrap Pick new parameters (grouping parameters) Change the error term distribution from \\(z=1\\) ODP Bootstrap Use L-year weighted average Heteroscedasticity may exist See adjustment next sub section and diagnostics Since we dont’ make a distribution assumption, the number of outliers could mean the data is quite skewed and it’s appropriate that is showing up in the simulation 9.8.6 Heteroskedasticity Issue of non constant variance ODP bootstrap assumes residuals are \\(iid\\) with constant variance No longer possible to sample the residuals from the whole triangle with heteroskedasticity GLM Bootstrap has the additional flexibility of choosing parameters to alleviate heteroscedasticity For ODP Bootstrap: 3 ways to deal with heteroscedasticity below They also work for GLM Bootstrap 9.8.6.1 Stratified Sampling Stratified Sampling Split the triangle into groups with similar variance Only sample residuals from the same group Cons Each group may not be that large, which limits the amount of variability in the possible outcomes 9.8.6.2 Hetero-Adjustment to the Residuals Calculate a hetero-adjustment factor to scale the residuals to the same level: Group the residuals with similar then calculate the \\(\\sigma\\) of the residuals in each group \\(i\\) Hetero-adjustment factor: \\(h^i\\) i.e. The largest \\(\\sigma\\) \\(\\div\\) each group’s \\(\\sigma\\) \\[\\begin{equation} h_i = \\dfrac{\\sigma \\left( \\bigcup_1^j r^H_{wd} \\right)}{\\sigma \\left( \\bigcup_i r^H_{wd} \\right)} \\:\\: : \\:\\: \\text{for each } 1 \\leq i \\leq j \\tag{9.22} \\end{equation}\\] Scale up the residuals: Residual (9.8) \\(\\times\\) Hat Matrix Factor (9.10) \\(\\times\\) Hetero Factor (9.22) \\[\\begin{equation} r_{wd}^{iH} = r_{wd} \\times f_{wd}^H \\times h^i \\tag{9.23} \\end{equation}\\] Need to divide the sampled residual by \\(h^i\\) to reflect the variability of group \\(i\\) \\[\\begin{equation} q^{i*}(w,d) = m_{wd} + \\dfrac{r^{i*}}{h^i}\\sqrt{m_{wd}} \\tag{9.24} \\end{equation}\\] Adjust the variance for the process variance step in the simulation \\[\\begin{equation} Gamma(m_{wd}, \\dfrac{\\phi m_{wd}}{h^i}) \\tag{9.25} \\end{equation}\\] Remark. The hetero adjustment factors are new parameters and will impact degrees of freedom and will impact the scale parameter (9.13) and the degrees of freedom adjustment factor (9.9) 9.8.6.3 Non-constant Scale Parameters Adjust the dispersion factor \\(\\phi\\) as well as the residuals (similar to above) Calculate the hetero adjustment factor \\(h_i\\) using formula (9.27) below: \\[\\begin{equation} h_i = \\sqrt{\\dfrac{\\phi}{\\phi_i}} \\tag{9.26} \\end{equation}\\] Perform step 3 and 4 from the hetero adjustment method above (i.e. Equation (9.23) and (9.24)) Calculate \\(\\phi_i\\) for each homogenious residual group \\(i\\) (\\(n_i\\) = number of residuals in group): \\[\\begin{equation} \\phi_i = \\dfrac{N}{N-p}\\dfrac{\\sum_{w,d \\in \\{i\\}}r^2_{w,d}}{n_i} \\tag{9.27} \\end{equation}\\] Use \\(\\phi_i\\) for the process variance step Remark. The \\(\\phi_i\\) here also amount to new parameters that will impact the degrees of freedom adjustment factor (9.9) The hetero adjustment factor (9.26) is more theoretically sound but in practice very similar to (9.22) 9.8.7 Heteroecthesious Data ODP bootstrap requirements: Symmetrical shape (annual by annual, quarter by quarterly, etc triangles) Homoecthesious data (similar exposure) Heteroecthesious = Accident years have different level of exposures Here we are focusing on heteroecthesious due to interim evaluation dates: Partial first development period Partial latest calendar period 9.8.7.1 Partial First Development Period This means the entire first development period is shorter than the rest e.g. Annual data evaluated as of 6/30 with 1/1-12/31 AYs We’ll have a triangle with development periods @6, 18, 30, 42, etc Pearson residuals use the square root of the fitted value to make them all exposure independent (debatable…) \\(\\therefore\\) No impact to residuals Adjustment: Scale down the most recent AY projection to the appropriate exposure period (e.g. half the exposure based on example above), we have 2 options: Prorate the mean of the incremental cells for the latest AY between step 3 and 4 of the bootstrap process and then proceed to Step 4 for the process variance as usual Prorate the simulated incremental cells for the latest AY after the process variance step (Step 4) 9.8.7.2 Partial Latest Calendar Period This is where the latest diagonal is partial diagonal e.g. Evaluate in between typical data evaluation date Evaluation @6/30 for a 1/1-12/31 AYs and 12-24-36 triangle Similar problem as partial first development period + partial data in most recent diagonal ODP Bootstrap Select LDF by excluding latest diagonal or prorating the latest diagonal to full year Adjusted simulation process Calculate sampled triangle as usual (diagonal will be of full year) Calculate full year LDFs and Ultimate as usual Additional steps: De-annualize the diagonal Interpolate the full year LDFs to match the diagonal Forecast loss Scale down the latest AY similar to the partial AY adjustment No change GLM Bootstrap Should be something similar 9.8.8 Exposure Adjustment Adjustment for when exposure changed dramatically over the years (e.g. rapid growth or run off) ODP Bootstrap Divide losses by exposure (model loss cost) Need to multiply the simulated results by the exposure (after the process variance step) GLM Bootstrap Adjust losses by exposure similar to above Fit to the exposure adjusted losses should be exposure weighted (i.e. exposure adjusted losses with higher exposure are assumed to have lower variance, see Anderson et al. (2007)) This will need fewer AY parameters since the exposure adjustment should capture a lot of the difference between AYs 9.8.9 Parametric Bootstrapping ODP Bootstrap See CAS Tail Factor Working Party Report (2013) Add tail factor to the algorithm by assuming the factor follows a distribution (other considerations such as process variance, hetero-adj can all be extended to include the tail factors) Should be an extrapolation of the incremental tail factors (instead of a single tail factor to ultimate) Tail factors typically have \\(\\sigma &lt;\\) 50% of the tail factor - 1 (But should compare to the \\(\\sigma\\) of the AtA factors leading up to the tail in both the actual and simulated data) GLM Bootstrap Continue to use the last \\(\\beta_d\\) to estimate the tail by continuing to apply it (similarly for CY parameter) 9.8.10 Fitting a Distribution to ODP Bootstrap Residuals Data points from triangle may not be representative of the underlying distribution Whether the most extreme observation is a 1-in-100, 1-in-1000 event Alternative is to fit a distribution to the residuals and sample from the distribution instead i.e. parametric bootstrapping "],
["odp-diagnostics.html", "9.9 Diagnostics", " 9.9 Diagnostics Use diagnostics to judge the quality of the model: Test model assumptions Gauge quality of model fit Guide the adjustments of model parameters 5 diagnostics Residual graphs Normality test Outliers Parameter adjustment Model results 9.9.1 Residual Graphs Plot residuals versus CY, AY, Age, forecast loss (on x-axis) Want to see random variability around zero Bare in mind that we don’t have the same number of residuals at each point (helpful to plot the line for average as well) Test assumption of iid residuals across the entire triangle This helps with grouping for hetero adjustment Plot the relative \\(\\sigma(r_{wd})\\) for each group to further help with the groupings Do all the plots again after adjustment 9.9.2 Normality Test Normality is not required, only need this if we’re doing parametric bootstrap with normal distribution Plot residuals against the normal best fit based on the percentiles QQ-plot Statistical tests: Check if p-value &gt; 5% \\(R^2\\) AIC \\[2p + n \\left [ 1 + \\ln(2\\pi\\dfrac{RSS}{n})\\right]\\] BIC \\[n \\ln\\left( \\dfrac{RSS}{n}\\right) + p \\ln(n)\\] \\(RSS\\) = actual residual - expected residual from normal then squared and summed 9.9.3 Outlier Remove true outliers but do not remove points that are realistic extreme scenarios Use box &amp; whisker plot Box hows 25%-tile to 75%-tile Whiskers are 3 times the inter quartile range (both side total) Residuals outside the range are graphed 9.9.4 Parameter Adjustments Test model with different sets of parameters using GLM bootstrap Check parameter significance based on t-statistics (&gt;2) Parameter selection process: Start with all the AY and Age parameters (\\(\\alpha_w\\) and \\(\\beta_d\\)) and remove the insignificant ones until only significant parameters are left Add CY parameter (\\(\\gamma\\)) and check for significance After selecting parameters: Check the diagnostics discussed above (e.g. residuals and normality) Make hetero adjustment if necessary Compare implied development with ODP 9.9.5 Review Model Results Review outputs once we have decided on a model and run the bootstrap Mean, s.e., CoV, Min/Max, and percentiles by AYs Incremental fitted mean, s.e. and CoV for each cell in the triangle Check for reasonability and consistency Table 9.7: Model output review format AY Mean Unpaid Standard Error CoV Min Max 50%-tile 75%-tile 95%-tile 99%-tile (1) (2) (3) = (2) / (1) (4) (5) (6) (7) (8) (9) 1 - - - - - - - - - \\(\\vdots\\) - - - - - - - - - \\(w\\) - - - - - - - - - Total \\(\\sum\\) - - - - - - - - Remark. Standard Error: Col (2) Total s.e. should be greater than any individual year but less than the straight sum of each AY’s s.e. Expect s.e. to increase going down the column Remark. Coefficient of Variation: Col (3) Total CoV should be less than any individual year (due to diversification of results across AYs) Except for the most recent AYs, CoV should decrease going down the column (due to larger based of unpaid losses for the more recent AYs) Higher CoV for the most recent AYs due to: More parameters used to forcase for the most recent AYs \\(\\therefore\\) parameter uncertainty \\(gg\\) process variance Model maybe overestimating the uncertainty \\(\\Rightarrow\\) Use BF of Cape Cod Remark. Min &amp; Max Col (4) - (5) Check for reasonability (e.g. extreme outcomes from negative values) Implausible results can affect the mean "],
["shapland-multi-mod.html", "9.10 Using Multiple Models", " 9.10 Using Multiple Models Use different methods (Paid/Inc’d Dev, BF, etc) and assigning weights by AYs Models should be reviewed and finalize individually before blending with weights Method 1: In the process variance step of bootstrap, use the same underlying \\(u \\sim U(0,1)\\) to draw from each model then weight the models by a set of deterministic %’s Use the same random variable or else we would reduce the variability of the outcomes Method 2: Run each model independently for each simulation (i.e. use different \\(u \\sim U(0,1)\\)) then for each AY use the weights to randomly select one of the modeled results Results will be a mixture of the various models Other considerations: Should consider both the mean and standard deviation (or CoV) in each model result when selecting weights Can also select the weights using Bayesian methods to account for the quality of each model’s forecast Perform the same model output review as in the above section for the best estimate Also review the IBNR by AYs to look for inconsistencies (e.g. negative IBNR) and compare to deterministic results 9.10.1 Additional Useful Output Using the best estimate total unpaid mean, s.e., and CoV from above to fit to Normal, LogNormal, and Gamma distribution. We can use these fitted distribution to: Assess quality of fit Parameterize a DFA model Smooth out extreme values 9.10.2 Estimated Cash Flow Results Since bootstrap generates simulation for each cell in the bottom half of the triangle we can use this to get cash flow forecasts by CY and their variability as well We can review the s.e. and CoV similar what we did in the diagnostics section 9.10.3 Estimated Ultimate Loss Ratio Results We can estimate mean and the variability of ultimate loss ratios by AYs Compile a similar table as before 9.7 but for loss ratio Useful for projecting pricing risk in a risk model 9.10.4 Estimated Unpaid Claims Runoff Results Project unpaid claims out by CY similar to the cash flow projection Useful for calculating risk margins using the cost of capital method 9.10.5 Distribution Graphs Plot the distribution of the simulated unpaid in a histogram Or smooth the histogram with a Kernel density function (for each point it takes a weighted average of the points around it, giving less weight to points further from it) For each point it takes a weighted average of the points around it; giving less weight to points further from it 9.10.6 Correlation Correlate the loss distribution over several LoB Multivariate distribution requires the same underlying distribution which doesn’t work here for ODP Method 1: Location Mapping When sampling the residuals, sample from the same place in the triangle for all the lines we want to correlate Disadvantages: Requires all LoB to have the same size triangle with no missing values or outliers Cannot stress the correlations among the LoBs (Can only use the historical correlations) Method 2: Re-Sorting Use Iman-Conover algorithms or Copulas (Not explained in paper) Advantages: Can accommodate different shapes and sizes Can make different correlation assumptions Can strengthen the correlation for extreme events (e.g. t Copula vs normal Copula) Calculate correlation matrix using Spearman’s Rank Order and re-sorting based on the ranks of unpaid claims by AYs Look at p-value for each correlation parameter to see that they’re significantly different from zero Additional comments: Using residuals to correlate LoBs (both location mapping&amp; re-sorting) are liable to create correlations close to zero Reserve Risk: Correlate total unpaid by correlating the incremental paid May or may not be a reasonable approximation Risk not modeled is contagion risk, where a single event results in claims in multiple lines of business (can change correlation assumptions to address this) Pricing Risk: Correlate loss ratios over time Not as likely to be close to zero Use different correlation assumption than for reserve risk "],
["shapland-testing.html", "9.11 Model Testing", " 9.11 Model Testing Important to test the model results against actual (see Meyers) Challenges: Don’t have underlying distribution to compare with Can’t wait for 10 years to see how it forecast General Insurance Reserving Oversight Committee test: GIROC created triangles that met the assumptions of Mack and ODP 30K 10 \\(\\times\\) 10 triangles were created Mack: losses were above the 99th percentile about 8-13% of the time \\(\\Rightarrow\\) Mack underestimates tail events ODP from England &amp; Verrall: losses were above the 99th percentile about 3% of the time This does not include the residual adjustments Future testing: Create datasets from claims transaction data and use them for model testing 9.11.1 Future Research Test ODP bootstrap on realistic data from CAS loss simulation model See how adjustments discussed here improve predictive power Expand ODP bootstrap with Munich Chainladder for incurred/paid Claim counts and average severity Different residuals (e.g. deviance or Anscombe residuals) Select weights using Bayesian methods by AYs Other risk analysis measures and use for ERM SII requirements Research in correlation matrix (difficult to estimate) "],
["past-exam-questions.html", "9.12 Past Exam Questions", " 9.12 Past Exam Questions Exercises \\(\\star\\) Use simplified GLM and then back out the GLM parameters Reduce parameters Minimize square error for GLM Benefit of simplified GLM Residuals Dispersion Dispersion with hat matrix adj Simulate loss Setup GLM Negative values Simulate negative Partial triangle Stratified Dealing with correlation Outliers Practical Issues 2013 #7: negative values 2014 #7: List 4 practical issues and solutions 2015 #10: Heteroscedasticity, why important, adjustments description 2015 #11: Negative values, outliers, exposure level Diagnostics \\(\\star\\) 2014 #9: Evaluate the results given mean unpaid, s.e., CoV by AYs 9.12.1 Question Highlights n/a -->"]
]
