[
["testing-the-assumptions-of-age-to-age-factors-g-venter.html", "Chapter 5 Testing the Assumptions of Age-to-Age Factors - G. Venter", " Chapter 5 Testing the Assumptions of Age-to-Age Factors - G. Venter Standards used in this paper The \\(n\\) for this paper excludes the first column Coefficient \\(&gt; 2 \\sigma\\) is significant Know the adj SSE, AIC and BIC Know the 6 implications Statistical significance of \\(f(d)\\) Is there a better estimate for \\(q\\) than \\(f \\times c\\) Number of parameters (Table 5.3) BF parameters (Table 5.4) Check residuals against \\(c(w,d)\\) Stability of \\(f(d)\\) down the column No correlation among columns Know the calculation for test No particularly high or low diagonals "],
["venter-def.html", "5.1 Definitions and Assumptions", " 5.1 Definitions and Assumptions Table 5.1: Tables of Definitions Venter Notations Definitions Mack Notations (??) \\(w\\) AYs \\(i\\) \\(d\\) Dev period; \\(d=0\\) is age @ end of year 1 \\(k\\) \\(c(w,d)\\) Cumulative losses for AY \\(w\\) age \\(d\\) \\(c_{i,k}\\) \\(c(w,\\infty)\\) Ultimate losses for AY \\(w\\) \\(c_{i,I}\\) \\(q(w, d+1)\\) Incremental losses for AY \\(w\\) age \\(d\\) to \\(d+1\\) \\(f(d)\\) Col parameters; (LDF - 1); applies to whole col \\(f_k - 1\\) \\(F(d)\\) LDF to ultimate that applies to \\(c(w,d)\\) \\(h(w)\\) Row parameters; applies to whole row 5.1.1 Mack’s Chainladder Assumptions Same assumptions as the Mack 1994 but stated in Venter’s terminology Venter refers these as assumptions needed for least-squares optimality to be achieved by the typical age-to-age factor method of loss development Proposition 5.1 (Mack Assumption 1) \\[\\mathrm{E}[q(w,d+1) \\mid \\text{Data to } w+d] = f(d) \\times c(w,d)\\] Remark. Here \\(f(d)\\) is LDF - 1 Expected incremental development is \\(\\propto\\) reported losses See also (Prop ??) from Mack (1994) Proposition 5.2 (Mack Assumption 2) \\[c(w,d) \\perp\\!\\!\\!\\!\\perp c(v,g) \\:\\:\\:\\: \\forall \\: d,g,v,w \\:\\:\\:\\: : \\:\\:\\:\\: v \\neq w\\] Remark. Losses not in the same row are independent of each other See also (Prop ??) from Mack (1994) Proposition 5.3 (Mack Assumption 3) \\[\\mathrm{Var}[q(w,d) \\mid \\text{Data to } w+d] = a_{fun}\\big(d,c(w,d)\\big)\\] Remark. Variance of incremental losses depends only on: Cumulative losses reported to date \\(c(w,d)\\) Age of the AY \\(d\\) (Does not vary by AY down the column) Different \\(a_{fun}(\\cdots)\\) leads to different \\(\\hat{f}(d)\\) estimate See also (Prop ??) from Mack (1994) 5.1.2 Variance Assumptions (for Chainladder) Same as shown in Mack 1994 Table ?? Table 5.2: Relationships between weight, variance and residual (Venter) Weight Description Variance \\(a_{fun}\\big(d,c(w,d)\\big)\\) LDF - 1 \\(f(d)\\) 1 Simple Average \\(k(d)c(w,d)^2\\) \\(\\dfrac{\\sum_w 1 \\frac{q(w,d+1)}{c(w,d)}}{\\sum_w 1}\\) \\(c(w,d)\\) Weighted Average \\(k(d)c(w,d)\\) \\(\\dfrac{\\sum_w c(w,d) \\frac{q(w,d+1)}{c(w,d)}}{\\sum_w c(w,d)}\\) \\(c(w,d)^2\\) Least Square \\(k(d)\\) \\(\\dfrac{\\sum_w c(w,d)^2 \\frac{q(w,d+1)}{c(w,d)}}{\\sum_w c(w,d)^2}\\) \\(c(w,\\infty) = F(d)c(w,d)\\) \\(F(d) = \\prod_{s \\geq d} (1 + f(s))\\) Recall \\(\\dfrac{q(w,d+1)}{c(w,d)}\\) are the empirical LDF - 1 \\(\\mathrm{E}[q(w,d+1)] = f(d)c(w,d)\\) \\(n = \\sum \\limits_{i=1}^{m-1} i = \\dfrac{m(m-1)}{2}\\) = predicted data point? \\(p=m-1\\) since we don’t predict the first column \\(m =\\) dimension "],
["testable-implications.html", "5.2 6 Testable Implications", " 5.2 6 Testable Implications 6 Testable Implications Statistical significance of \\(f(d)\\) Is there a better estimate for \\(q\\) than \\(f \\times c\\) Check residuals against \\(c(w,d)\\) Stability of \\(f(d)\\) down the column No correlation among columns Test for independence No particularly high or low diagonals Test for independence 5.2.1 Goodness of Fit Measurement Compare different fit of the models based on adjusted \\(SSE\\) (actual vs projected excluding 1st column) Adjusted SSE \\[\\begin{equation} \\dfrac{SSE}{(n-p^2)} \\tag{5.1} \\end{equation}\\] Akaike Information Criterion \\[\\begin{equation} AIC \\approx SSE \\times e^{2p/n} \\tag{5.2} \\end{equation}\\] Bayesian Information Criterion \\[\\begin{equation} BIC \\approx SSE \\times n^{p/n} \\tag{5.3} \\end{equation}\\] Remark. \\(n =\\) # of predicted data points EXCLUDING 1st column Exclude because when we do reserving we don’t predict anything from the first column Usually the triangle excluding the first column \\(p =\\) # of parameters \\(SSE = \\sum (A - E)^2\\) Here you exclude the first column when calculating the difference Venter use the adjusted SSE as the AIC can be too permissive of over parameterization for large data sets 5.2.2 Implication 1: Significance of Factors Check if the parameter coefficient is \\(&gt; 2 \\sigma\\) for 95% sure that the parameters are \\(\\neq 0\\) Can do \\(1.65 \\sigma\\) for 90% confidence Remember \\(f(d)\\) is LDF - 1 LDFs tend to fail towards the tail 5.2.3 Implication 2: Superiority of Alternative Emergence Patterns If an alternative emergence pattern provides a better explanation of the triangle, maybe it should be used Calculate \\(q(w,d)\\) under various emergence patterns (See Table 5.3) Calculate the Adjusted SSE (5.1) (based on every cell except the age 0 column) 5.2.3.1 Parameters: Alternative Emergence Pattern Table 5.3: Alternative emergence pattern on a \\(m \\times m\\) triangle Emergence Patterns # of Parameters \\(p\\) Comments \\(\\mathrm{E}[q(w,d+1) \\mid \\text{Data to }w+d] = f(d) c(w,d)\\) \\(m - 1\\) e.g. Chainladder \\(\\mathrm{E}[q(w,d+1) \\mid \\text{Data to }w+d] = f(d) c(w,d) + g(d)\\) \\(2m - 2\\) e.g. Least Squares \\(\\mathrm{E}[q(w,d) \\mid \\text{Data to }w+d] = f(d)h(w)\\) \\(2m-2\\) e.g. BF \\(\\mathrm{E}[q(w,d) \\mid \\text{Data to }w+d] = f(d)h\\) \\(m-1\\) e.g. Cape Cod Remark. \\(f(d) c(w,d) + g(d)\\) Often significant in forecasting age 1 Remark. \\(f(d)h(w)\\) Here \\(f(d)\\) is related to the % of losses emerged in period \\(d\\) Not LDF -1 \\(h(w)\\) can be think of as an estimate of ultimate losses for AY \\(w\\) Like an a-priori The -2 for the BF is due to \\(f(0)\\) and constant If BF is better \\(\\Rightarrow\\) Loss emergence is more accurately represented as fluctuating around a proportion of expected ultimate losses (rather than proportion of reported losses) Cape Cod works when the loss ratio is stable (stable book of business) Use \\(h(w) = h \\times Premium(w)\\), so we only need stable ELR Cape Cod works out to an additive model \\(q(w,d) = h \\times f(d)\\) Can further reduce parameters by combining some row and column parameters Might be intuitively appealing to sum up the recent and tail years of the \\(h(w)\\) since there’s little empirical data to support different estimates We’ll be mostly focusing on this form 5.2.3.2 Variance Assumptions: Alternative Emergence Pattern Consider \\(\\mathrm{E}[q(w,d) \\mid \\text{Data to }w+d] = f(d)h(w)\\) from Table 5.3 We need to minimize the sum of squared residuals to get the optimal \\(f(d)\\) and \\(h(w)\\): \\[\\sum_{w,d} \\varepsilon^2(w,d) = \\sum_{w,d} [q(w,d) - \\underbrace{f(d)h(w)}_{\\mathrm{E}[q(w,d)]}]^2\\] Remark. Since this is a non-linear model, we need an iterative method to minimize to SSE We can use weighted least squares if the variances of the residuals are not constant over the triangle We need to minimize the variance of each residual \\(\\varepsilon(w,d)\\) \\[\\mathrm{Var}(\\varepsilon(w,d)) \\propto f(d)^p h(w)^q\\] Remark. \\(p\\) &amp; \\(q\\) typically \\(\\in [0,1,2]\\) And regression weights (applied to each \\(f(d)\\) or \\(h(w)\\)) will be \\(\\dfrac{1}{f(d)^p h(w)^q}\\) (inversely proportional to variance, similar to Mack 1994) Since \\(\\mathrm{E}[q(w,d)] = f(d)h(w)\\) \\(f(d) = \\dfrac{\\mathrm{E}[q(w,d)]}{h(w)}\\), and \\(h(w) = \\dfrac{\\mathrm{E}[q(w,d)]}{f(d)}\\) So the actual \\(\\dfrac{q(w,d)}{h(w)}\\) is an estimate of \\(f(d)\\) and vice versa And we can estimate \\(f(d)\\) based on a weighted average of each observation Different variance assumption for \\(\\varepsilon\\) \\(\\Rightarrow\\) different parameters (weight) similar to the Chainladder method in table 5.2 Table 5.4: Variance and parameters for various form of \\(\\mathrm{E}[q(w,d) \\mid \\text{Data to }w+d] = f(d)h(w)\\) Method \\(\\mathrm{Var}(\\varepsilon(w,d)) \\propto f(d)^p h(w)^q\\) \\(\\mathbf{f(d)}\\): Col Parameters \\(\\mathbf{h(w)}\\): Row Parameters BF1 \\(p=q=0\\) \\(f(d) = \\dfrac{\\sum_w h^2 \\frac{q}{h}}{\\sum_w h^2}\\) \\(h(w) = \\dfrac{\\sum_d f^2 \\frac{q}{f}}{\\sum_d f^2}\\) Cape Cod2 \\(p=q=0\\) \\(f(d) = \\dfrac{\\sum_w h^2 \\frac{q}{h}}{\\sum_w h^2}\\) \\(h = \\dfrac{\\sum_\\Delta f^2 \\frac{q}{f}}{\\sum_\\Delta f^2}\\) BF (Var \\(\\propto\\) \\(fh\\)) \\(p=q=1\\) \\(f^2(d) = \\dfrac{\\sum_w h \\left( \\frac{q}{h} \\right) ^2}{\\sum_w h}\\) \\(h^2(w) = \\dfrac{\\sum_d f \\left( \\frac{q}{f} \\right)^2}{\\sum_d f}\\) BF: assumes each \\(q(w,d)\\) have constant variance (least square, standard regression) For \\(h(w)\\) weight is \\(f(d)^2\\) and cells with higher expectes loss get higher weight For \\(f(d)\\) weight is expected ultimate losses squared, years with higher expected losses get higher weight Cape Cod: also assumes constant variance Recall from Mack 1994, we need to look at the residual graphs to determine which variance assumption is appropriate 5.2.3.3 Iteration Process Work in progress section Need to seed one of them and iterate until convergence \\(f(d)\\) \\(\\sum \\downarrow\\); \\(h(w)\\) \\(\\sum \\rightarrow\\) (for the constant var BF) Use the above to estimate the parameters and then calculate the unpaid When combining parameters, don’t count the \\(f(0)\\) and always subtract 1 Step 1) Start iteration with the \\(f(d)\\) from Chainladder For age greater than 0, these are the incremental ATA factors \\(\\div\\) ATU factors For age 0, subtract the sum of the other factors from unity Step 2) Find \\(h(w)\\)’s that minimize the SSE One regression for each \\(w\\) Step 3) Find \\(f(d)\\)’s that minimize the SSE based on the \\(h(w)\\) above Step 4) Do this till convergence 5.2.3.4 Counting n and p for SSE For n For reserving: number of cell in the triangle minus the first column \\(\\dfrac{m(m-1)}{2}\\) Since we don’t need to forecast the first column to calculate the unpaid For pricing: number of cell in the triangle For p (Just a walk through of how we get the \\(m\\) in Table @ref(#tab:venter-alt-pattern)) BF: Start with \\(2m\\) parameters all the \\(f(d)\\)’s and \\(h(w)\\)’s We don’t need \\(f(0)\\) for reserves again since we don’t need to forcast the first column Less one more due to degree of freedom (?) Since you can fix any one of the parameters and still get the same results So we have \\(p = 2m-2\\) Similarly for Cape Cod but we only start with \\(m + 1\\) so we ended up with \\(m-1\\) taking out the \\(f(d)\\) and the degree of freedom Grouped BF is similar as well if you don’t group the \\(f(0)\\) If you group the \\(f(0)\\) you can’t subtract one for that as you’ll be using the parameter Chainladder you have \\(m\\) for all the \\(f(d)\\)’s to start with and less the \\(f(d)\\) 5.2.4 Implication 3: Linearity The forecast incremental losses might not be a linear function (e.g. \\(q(w,d) = a\\sqrt{c(w,d)}\\)) Look at residuals as function of previous Test for linearity by making sure the residuals are not a sequence of positive then negative and vice versa Do this for each regression (i.e. every \\(d\\)) 5.2.5 Implication 4: Stability Look at empirical LDFs \\(f(d)\\) down a column Use the entire history if factors are stable Take more recent average if unstable or follow a trend Can compare rolling 5 year averages as well 5.2.6 Implication 5: No Correlation on Columns (Test for independence) Calculate Pearson correlation for every pair of columns with at least 3 LDFs This is a test on the LDFs Test with only 2 LDFs will either be 1 or -1 Not just adjacent LDF pairs like in Mack-1994 \\(\\therefore\\) For a \\(m \\times m\\) triangle we have \\({7 \\choose 2}\\) pairs (7 because only 9 columns of LDFs and we take out the 2 columns with less than 3 LDFs) Correlation: \\[r = \\dfrac{\\sum \\tilde{x} \\tilde{y}}{\\sqrt{\\sum \\tilde{x}^2\\sum \\tilde{y}^2}}\\] Where \\(\\tilde{x} = x - \\bar{x}\\) and \\(\\tilde{y} = y - \\bar{y}\\) \\(x\\)’s and \\(y\\)’s are incremental LDFs - 1 But actually not necessary since -1 doesn’t affect the correlation Test statistics for significance is \\(T \\sim t_{n-2}\\) \\[T = r \\sqrt{ \\dfrac{n-2}{1-r^2} }\\] Look up the t-value from table for 90% If \\(|T| &lt;\\) table value \\(\\Rightarrow\\) Not correlated Perform test for all columns We deem the triangle have significant correlations if more than \\(0.1 x + \\sqrt{x}\\) pairs are correlated x = # of pair tested = \\({m - 3 \\choose 2}\\) for a \\(m \\times m\\) triangle 5.2.7 Implication 6: No High of Low Diagonals (Test for independence) Similar to Mack’s CY Test Key Idea: Run regression on the triangle with a dummy variable for each diagonal Each \\(q(w,d)\\) is regressed against the prior cumulative losses + dummy variable for which diagonal it is in \\(q(w,d) \\sim c(w,d-1) + dummy_{CY}\\) If losses are significantly higher or lower in a diagonal \\(\\Rightarrow\\) The coefficient of the dummy variable would be statistically significant (i.e. coefficient is double the \\(\\sigma\\)) Only includes diagonals that forcast at least 2 elements Caveat: Diagonal effect is additive More likely to see multiplicative impact. e.g. from inflation This can be implement with a regression on the logarithm of the losses 5.2.7.1 Diagonal Trend as Inflation Consider CY trend as inflation and model \\(q(w,d)\\) with a diagonal parameter \\(g(w+d)\\), where \\(w+d\\) is the diagonal \\[\\mathrm{E}[q(w,d)] = f(d)h(w)g(w+d)\\] This will have parameters for each row, column, and diagonal Can be reduce similar to the grouped BF We can model a constant CY trend to reduce the parameters e.g. \\(g(w+d) = (1+j)^{w+d}\\) "],
["past-exam-questions.html", "5.3 Past Exam Questions", " 5.3 Past Exam Questions Haven’t done TIA practice questions Concepts 2011 #4: Implication 1 2012 #5: Mack assumptions Implication Tests 2014 #4: Implication 5 correlation tests 2015 #5: Implication 5 correlation tests 5.3.1 Question Highlights n/a -->"]
]
