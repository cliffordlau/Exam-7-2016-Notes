[
["index.html", "CAS Exam 7 Study Notes Overview", " CAS Exam 7 Study Notes Cliff Lau 2017-05-04 Overview CAS Exam notes for the 2017 Spring sitting. Notes are broken down in to 3 main sections: I. Estimation of Policy Liabilities (14 chapters) Learning Objective 1: 10-14% Brosius Loss Development Using Credibility Mack (2000) Credible Claims Reserve: The Benktander Method Hürlimann Credible Claims Reserve: Benktander, Neuhaus and Mack Learning Objectives 2 &amp; 3: 16-18% Mack (1994) Measuring the Variability of Chain Ladder Reserve Estimate Venter Factors Testing the Assumptions of Age-to-Age Factors Clark LDF Curve-Fitting and Stochastic Reserving: A Maximum Likelihood Approach Learning Objective 4: 5-7% Siewert A Model for Reserving Workers Compensation High Deductibles - J. Siewert Sahasrabuddhe Claims Development by Layer - R. Sahasrabuddhe Learning Objectives 5-10: 22-24% Shapland (new 2017) Using the ODP Bootstrap Model: A Practitioner’s Guide Verall Obtaining Predictive Distributions for Reserves Which Incorporate Expert Opinions Meyers (new 2016) Stochastic Loss Reserving Using Bayesian MCMC Models Marshall et al A Framework for Assessing Risk Margins Learning Objectives 11-13: 6-9% Patrik Reinsurance Loss Reserving Learning Objective 14: 4-5% Teng &amp; Perkins Estimating the Premium Asset on Retrospectively Rate Policies Insurance Company Valuation (1 chapter) Learning Objective 1-3: 8-12% Goldfarb P&amp;C Insurance Company Valuation Enterprise Risk Management (11 chapter) Learning Objectives 1-6: 13-17% ERA 1 Historical Context Overview of Enterprise Risk Management Enterprise Risk Modeling Overview ERA 2.1 Corporate Decision Making Using an Enterprise Risk Model ERA 2.2 Risk Measures and Capital Allocation ERA 2.3 Regulatory and Rating Agency Capital Adequacy Models ERA 2.4 Asset-Liability Management ERA 2.5 Measuring Value in Reinsurance ERA 3.1 Considerations on Implementing Internal Risk Model ERA 3.2 Modeling Parameter Uncertainty ERA 3.3 Modeling and Dependency: Correlations and Copulas Learning Objectives 7 &amp; 8: 4-6% ERA 4.1 &amp; 4.2 Operational Risk Strategic Risk ERA 5.4 Approaches to Modeling the Underwriting Cycle "],
["loss-development-using-credibility-e-brosius.html", "Chapter 1 Loss Development Using Credibility - E. Brosius", " Chapter 1 Loss Development Using Credibility - E. Brosius Least Square Least Square formula (prop. 1.1) and Table 1.1 Know how to do them on the calculator (Notation for \\(a\\) and \\(b\\) on TI-30XS is flipped) Remember to adjust for exposure if needed Know the conclusions on LS Know the caveat on least square Additional considerations for calculations when using LS Method comparisons LS formula for each method (Table 1.1) Pros and cons of each method Theoretical Bayesian Poisson - Binomial (1.2) Use bayesian credibility: 1.8 Negative Binomial - Binomial (1.3) Use bayesian credibility: 1.9 Bayesian Credibility Best linear estimator formula (1.4) Know which method is best given the Cov and Var relationships (Table 1.2) Development Formula: Use the credibility formula Estimate from data (prop. 1.5): Same as the LS equation Alt form for \\(Z\\) with equation (1.1) that gives you same as LS results Use \\(VHM\\) and \\(EVPV\\) for \\(Z\\) if historical not a good predictor Case load effect "],
["method-assumptions.html", "1.1 Method Assumptions", " 1.1 Method Assumptions Table 1.1: Formula for a given method Method Formula Restrictions Least Squares \\(y = a + bx\\) No Restriction Chainladder \\(y = bx\\) a = 0 BF \\(y = a + x\\) b = 1 ELR \\(y = a\\) b = 0 Proposition 1.1 (Least Squares Formula) \\[b = \\dfrac{\\overline{xy} - \\bar{x}\\bar{y}}{\\overline{x^2}-\\bar{x}^2} = \\dfrac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)}\\] \\[a = \\bar{y} - b\\bar{x}\\] Notes: In the example here we are looking at the ATA development This is effectively a credibility weighting system (giving more or less weight to the observed x as appropriate) Above calculation can be done with the table features on TI-30XS Note that TI-30XS use the formula y = ax + b, different from us 1.1.1 Caveat with Traditional Method Chainladder: Difficult to select LDF when they vary greatly from year to year BF: Doesn’t work well with negative development ELR: Ignores actual experience Difficulty with parameter estimation when loss patterns are changing When the nature of loss experience is changing the use of unadjusted data can lead to errors Variability within a stable book still have sampling error \\(\\Rightarrow\\) b that doesn’t reflect the true underlying characteristics 1.1.2 Least Squares Pros &amp; Cons Pros Good when distn is the same across multiple years As we are assuming a common \\(Y\\) and \\(X\\) over the years Good when there is little data and fluctuations in the year to year losses Good when the randomness of the data is primarily driven by process variance Cons Bad when systemic shift year to year e.g. inflation, legal environment (Tort reform) \\(\\therefore\\) best to adjust for inflation and putting all the years on a constant dollar basis before using the LS method Also should adjust for exposure As the expected value increase \\(\\propto\\) exposure and the covariances increase \\(\\propto\\) squared exposure 1.1.3 Practical Considerations for the Least Square Method Normalize the losses by dividing with premium since LS assume constant distribution and also adjust for inflation Calculates ATU, so needs the tail factor first and also start from the oldest period Recursively go backwards using the previous estimates Notes on parameters: \\(a\\) = Projected ultimate if no losses are reported AY matures \\(\\Rightarrow a \\: \\downarrow\\); \\(Z \\uparrow\\); \\(c \\downarrow\\) If a &lt; 0 \\(\\Rightarrow\\) Use Chainladder to make a = 0 If b &lt; 0 \\(\\Rightarrow\\) Use ELR If b = 1 \\(\\Rightarrow\\) Same as BF "],
["best-estimate-based-on-bayes-with-theoretical-distribution.html", "1.2 Best Estimate based on Bayes (with Theoretical Distribution)", " 1.2 Best Estimate based on Bayes (with Theoretical Distribution) Proposition 1.2 (Poisson - Binomial) Assume the loss reporting follows a theoretical distribution \\(Y =\\) (Ultimate) # of claims incurred each year \\(\\sim Poi(\\mu)\\) \\(X =\\) # of claims reported by year end \\(\\sim Bin(y,d)\\) i.e. each claim have probability \\(d\\) of being reported in the first year Ultimate claims = \\(Q(x) = x + \\mu(1-d)\\) Unreported claims = \\(R(x) = \\mu(1-d)\\) (Expected # of claims) \\(\\times\\) (Expected % Unreported) Similar to the BF Method Proof. \\(Q(x)\\) is the estimator of \\(Y\\). It’s the sum of all possible \\(y\\)’s \\(\\times\\) probability of the result being \\(y\\) given \\(x\\) \\(\\begin{align} Q(x) &amp;= \\sum \\limits_{y = x}^{\\infty} y \\Pr(Y = y \\mid X = x) \\\\ Q(x) &amp;= \\sum \\limits_{y = x}^{\\infty} y \\dfrac{\\Pr(Y = y)\\Pr(X = x \\mid Y = y)}{\\sum_i \\Pr(Y = i) \\Pr(X = x \\mid Y = i)} \\\\ \\end{align}\\) And we get Ultimate claims = \\(Q(x) = x + \\mu(1-d)\\) Proposition 1.3 (Negative Binomial - Binomial) Assume the loss reporting follows a theoretical distribution \\(Y =\\) (Ultimate) # of claims incurred each year \\(\\sim NB(r, p)\\) \\(Y =\\) # of failures until \\(r\\) success with success probability = \\(p\\) \\(\\mathrm{E[Y]} = \\dfrac{r(1-p)}{p}\\) \\(X =\\) # of claims reported by year end \\(\\sim Bin(y,d)\\) Unreported claims = \\(R(x) = \\dfrac{s}{1-s}(x + r)\\) \\(s = (1-d)(1-p)\\) 1.2.1 Comparing Loss Development Methods Simulate loss based on one of the theoretical distribution Apply the various loss development method and calculate their respective parameters (\\(y = a + b x\\)) Compare the estimated parameters with the true parameters based on the underlying theoretical distribution Also compare the MSE from different methods "],
["bay-cred.html", "1.3 Bayesian Credibility", " 1.3 Bayesian Credibility We can’t use the Bayes theorem as in the previous section if we don’t know the underlying distribution Proposition 1.4 Estimate the ultimate losses using the best linear estimator of Y|X, \\(L(x)\\) \\[L(x) = (x - \\mathrm{E[X]})\\dfrac{Cov(X,Y)}{Var(X)} + \\mathrm{E[Y]}\\] Y = Ultimate Losses; X = Reported Losses Use this when we don’t know the distribution of the random variable \\(L(x) = Q(x)\\) when \\(Q(x)\\) is linear Remark. This is like the Bühlmann method, where \\(L\\) is a linear function that minimizes \\(\\mathrm{E}_X\\left[\\left(Q(X) - L(X)\\right)^2\\right]\\) If \\(L(x) = a + b x\\) then we minimize \\(\\mathrm{E}_X\\left[\\left(Q(X) - a - bX \\right)^2\\right]\\) Table 1.2: Intuitive interpretation of \\(L(x)\\) Scenarios Implications Interpretation \\(x = \\mathrm{E[X]}\\) \\(L(X) = \\mathrm{E[Y]}\\) Losses are coming in as expected, estimate of ultimate losses is unchanged \\(\\mathrm{Cov(X,Y)} \\approx 0\\) \\(L(X) \\cong \\mathrm{E[Y]}\\) \\(X\\) and \\(Y\\) are only loosely related \\(\\Rightarrow\\) Use ELR Method \\(\\mathrm{Cov(X,Y)} \\ll \\mathrm{Var(X)}\\) \\(L(X) \\cong \\mathrm{E[Y]}\\) \\(X\\) and \\(Y\\) don’t vary together \\(\\mathrm{Cov(X,Y)} \\approx \\mathrm{Var(X)}\\)1 \\(L(X) \\approx x + \\left[\\mathrm{E}[Y] - \\mathrm{E}[X] \\right]\\) Same as BF Method \\(\\mathrm{Cov(X,Y)} \\gg \\mathrm{Var(X)}\\)2 \\(X\\) and \\(Y\\) move together, \\(Y\\) is significantly influenced by \\(X\\) Use Dev Method Also means change in the reported should not affect the IBNR Also means greater than expected reported amount should lead to an increase in IBNR 1.3.1 Practical Application (LS Development) Proposition 1.5 (Development Formula 1) Estimate \\(\\mathrm{E}[X]\\), \\(\\mathrm{Var}(X)\\), and \\(\\mathrm{Cov}(X,Y)\\) from data (i.e. a series of past years) assuming a common \\(Y\\) and \\(X\\) This \\(L(x)\\) here is the same as the least-square estimate as in 1.1 Proof. Start with \\(y = a + bx\\) and plug in \\(a\\) and \\(b\\) from proposition 1.1 We get: \\(\\begin{align} y &amp;= (\\bar{y} - b\\bar{x}) + bx \\\\ &amp;= \\bar{y} + b \\left(x - \\bar{x}\\right) \\\\ &amp;= \\left(x - \\bar{x}\\right) \\dfrac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)} + \\bar{y} \\\\ \\end{align}\\) Which equals \\(L(x)\\) \\(\\therefore\\) the least-square estimate is the best linear estimate of \\(Q(x)\\) Remark. If not for sampling error, the least square method will give us the best linear approximation to the Bayesian estimate, regardless of the distributions of \\(X\\) or \\(Y\\) 1.3.2 Credibility Form of the Dev’ Formula Alternative way to express \\(L(x)\\), following Bühlmann credibility, we express \\(L(x)\\) in terms of: Expected Value of the Process Variance (\\(EVPV\\)) \\(\\mathrm{E}_Y\\left[\\mathrm{Var}(X \\mid Y )\\right]\\) Variance of the Hypothetical Mean (\\(VHM\\)) \\(\\mathrm{Var}_Y(\\mathrm{E}\\left[X \\mid Y \\right])\\) Sidebar: We can read \\(VHM\\) as distrust in underwriters and \\(EVPV\\) distrust in the claims department Use the method below when the least-square assumption fails (i.e. Year to year changes in loss and loss distributions are small, or can be corrected for) Formula below requires additional hypothesis (in paper appendix?) Proposition 1.6 (Development Formula 2) Suppose that there is a real number \\(d \\neq 0\\) such that \\(\\mathrm{E}\\left[X \\mid Y = y \\right] = dy\\) for all \\(y\\) \\[L(x) = Z \\underbrace{\\dfrac{x}{d}}_{\\begin{array}{c} \\text{Dev&#39;}\\\\ \\text{Method}\\\\ \\end{array}} + (1-Z)\\underbrace{\\mathrm{E[Y]}}_{ELR}\\] Formula is the credibility weighting of the chainladder estimate and ELR estimate If \\(EVPV = 0\\) \\(\\Rightarrow\\) Full weight to the chainladder If \\(VHM = 0\\) \\(\\Rightarrow\\) Full weight to the \\(\\mathrm{E}[Y]\\) Proof. Start with the proposition 1.4 and we set \\(\\dfrac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)} = \\dfrac{1}{d} \\dfrac{VHM}{VHM + EVPV}\\) \\(\\begin{align} L(x) &amp;= (x - \\mathrm{E[X]})\\dfrac{Cov(X,Y)}{Var(X)} + \\mathrm{E[Y]} \\\\ &amp;= (x - \\mathrm{E[X]}) \\left \\{ \\dfrac{1}{d} \\dfrac{VHM}{VHM + EVPV} \\right \\} + \\mathrm{E[Y]} \\\\ &amp;= (x - \\mathrm{E[X]}) \\left \\{ \\dfrac{1}{d} Z \\right \\} + \\mathrm{E[Y]} \\\\ &amp;= Z \\dfrac{x}{d} - Z \\dfrac{\\mathrm{E}[X]}{d} + \\mathrm{E}[Y] \\\\ &amp;= Z \\dfrac{x}{d} - Z \\mathrm{E}[Y] + \\mathrm{E}[Y] \\\\ &amp;= Z \\dfrac{x}{d} + (1 - Z) \\mathrm{E}[Y] \\\\ \\end{align}\\) Which is what we have above in proposition 1.6 Proposition 1.7 (Method for Z) Calculate Z to use with formula in proposition 1.6 \\(Z = \\dfrac{VHM}{VHM + EVPV} = \\dfrac{\\mathrm{Var_Y(E[X|Y])}}{\\mathrm{Var_Y(E[X|Y])}+\\mathrm{E_Y[Var(X|Y)]}}\\) \\(VHM = d^2 \\sigma^2_Y\\) \\(EVPV = \\sigma^2_d[\\sigma^2_Y + \\mathrm{E[Y]}^2]\\) \\(d =\\) % reported Remark. We use the above \\(Z\\) when underlying distribution not stable (historical not a good predictor) LS only works when the underlying distn are stable We assumes the following \\(d \\: {\\perp\\!\\!\\!\\!\\perp} \\: X\\): reporting speed does not vary with the volume of claims \\(D = \\dfrac{X}{Y}\\) Here we typically assume the \\(\\sigma_{\\frac{X}{Y}}\\) does not depend on \\(Y\\) Results sensitive to \\(\\mathrm{E[Y]}\\) and \\(\\mathrm{E[D]}\\) but not the \\(\\sigma\\) Remark. Alternatively, you can use: \\[\\begin{equation} Z = \\dfrac{b}{c} \\tag{1.1} \\end{equation}\\] where \\(c\\) is the CDF and \\(b\\) is from the LS This yield the LS results (Where we assume there’s no change in the underlying year to year data) Assume same \\(d\\) for any size of \\(y\\); Not necessarily true for large or small \\(y\\) Proof. Start with \\(\\dfrac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)} = \\dfrac{1}{d} \\dfrac{VHM}{VHM + EVPV}\\) Based on 1.7 we have \\(Z = \\dfrac{VHM}{VHM + EVPV}\\) And \\(b = \\dfrac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)}\\) from 1.1 Then we have \\(b = \\dfrac{1}{d} Z\\) Next, \\(\\dfrac{1}{d} = \\dfrac{1}{\\text{% Reported}} = \\text{Reported CDF} = c\\) Finally we have \\(Z = \\dfrac{b}{c}\\) Proposition 1.8 (Poisson-Binomial Special Case) \\(L(x) = x + (1-d)\\mathrm{E(Y)}\\) \\(Z = d\\) Same as BF BF is optimal when claim counts follow Poi - Bin Proof. Start with \\(EVPV\\) and \\(VHM\\) under the Poisson-Binomial case as discussed in proposition 1.2 \\(EVPV = \\mathrm{E}[yd(1-d)] = \\mu d(1-d)\\) \\(VHM = \\mathrm{Var}(yd) = \\mu d^2\\) Then \\(Z = \\dfrac{\\mu d^2}{\\mu d^2 + \\mu d(1-d)} = d\\) And we get what we have from proposition 1.8 \\(L(x) = x + (1-d)\\mathrm{E(Y)}\\) Remark. Note that \\(L(x) = Q(x)\\) (as in proposition 1.2) since \\(Q(x)\\) is linear, so the best linear estimate = the bayesian estimate Proposition 1.9 (Negative Bin-Binomial Special Case) \\(L(x) = \\dfrac{x}{d + p(1-d)} + \\dfrac{\\mu p (1-d)}{d + p(1-d)}\\) \\(Z = \\dfrac{d}{d + p(1-d)}\\) Remark. \\(VHM\\) is larger here than in the Poi-Binomial case while the \\(EVPV\\) is the same \\(\\therefore\\) \\(Z\\) is larger \\(\\Rightarrow\\) Chainladder method gets more weight Also note that since \\(Q(x)\\) is still linear, \\(L(x) = Q(x)\\) 1.3.3 Caseload Effect In proposition 1.6 we assumed the expected number of claims reported is \\(\\propto\\) number of claims incurred Not necessarily true: e.g. claim is more likely to be reported in a timely fashion when the caseload (case reserve) is low, and we expect the development ratio \\(\\dfrac{\\mathrm{E}[X \\mid Y = y]}{y}\\) to be not a constant decreasing function of \\(y\\) When \\(D\\) and \\(Y\\) not independent the credibility-based development formula still works (i.e. constant development ratio is not essential for a credibility-based development formula) e.g. \\(d\\) larger for small \\(y\\) since small claims are reported more timely, so settle faster e.g. the opposite situation when large \\(y\\) has as larger \\(d\\): For a property book when large weather event happens, report quicker Proposition 1.10 (Development Formula 3) Supposed there are real numbers \\(d \\neq 0\\) and \\(x_0\\) such that \\(\\mathrm{E}[X \\mid Y= y] = dy + x_0\\) for all \\(y\\) \\[L(x) = Z \\dfrac{x - x_0}{d} + (1-Z)\\mathrm{E[Y]}\\] \\(Z = \\dfrac{VHM}{VHM + EVPV}\\) Remark. Assumptions: \\(\\mathrm{E[X|Y=y]} = d \\cdot y + x_0\\) \\(x_0\\) is for the fixed salary \\(d \\neq 0\\) Development ratio \\(= d + \\dfrac{x_0}{y}\\) Which does decrease as \\(y\\) gets larger This gives \\(\\mathrm{E}[X \\mid Y = 0] = x_0 &gt; 0\\) Which is okay consider claims department in real life Impossible to determine \\(x_0\\) and \\(d\\) in practice but this shows that the least square methods still make sense when development ratio varies with caseload "],
["conclusions.html", "1.4 Conclusions", " 1.4 Conclusions LS is the best linear estimate of \\(Q(x)\\), the theoretical best estimate of the ultimate loss LS produce more reasonable results when year to year fluctuations are severe LS does not do well when the variations are due to underlying changes in the payment pattern (due to internal or external changes) e.g. When there is a systematic shift in the business Subject to sampling error when estimating parameters similar to other methods "],
["past-exam-questions.html", "1.5 Past Exam Questions", " 1.5 Past Exam Questions Concepts 1996 - #28: Poi-bin process, \\(a\\) and \\(b\\) for methods 1996 - #50: When to use LS 2000 - #2: Method assumptions 2001 - #22: Which method is better given the Cov(X,Y) and Var(X) relationships 2003 - #3: \\(a\\) and \\(b\\) for different methods \\(\\star\\) 2006 - #4: Cov(X,Y) and Var(X) relationships (phrased a bit differently) 2006 - #15 d: LS assumptions 2007 - #42: Method assumptions 2009 - #3 b: LS assumptions (LS not best given \\(b &lt; 0\\)) 2014 #2: Know assumptions of methods for Dev (fitted line and residuals) Full Calculations \\(\\star\\) 2000 - #41(fig 1.1): LS, dev and ELR from triangle; Weight the 2 with \\(Z = \\dfrac{b}{c}\\) to get to LS \\(\\star\\) 2001 - #31(fig 1.2): Use VHM and EVPV for 2002 - #21: LS, dev and ELR from triangle and weight for LS 2008 - #9: LS, dev and ELR from triangle \\(\\star\\) 2008 - #10a: VHM and EVPV calc \\(\\sigma_y = \\sigma_{LR} \\times Prem\\) \\(\\star\\) 2011 #1: LS w/ exposure adjustment Assess LS parameters resonability \\(\\star\\) Draw the relationship of losses to date vs ultimate for the 3 methods 2012 ‐ #1 b: Solving equations to back out numbers using BK and BF method \\(\\star\\) 2012 - #4: LS and know that LS gives the optimal linear weight based on \\(\\dfrac{b}{c}\\) between linked ratio method and LR method 2014 #1: VHM and EVPV Why LS is not appropriate: Coverage levels have changes, no guarantee that the payout pattern remains 2016 #2: LS calculation LS parameters potential problems \\(VHM\\) and \\(EVPV\\): Use a priori based on average from historical Simple Plug and Play 1998 - #19: BF with LS \\(\\star\\) 2003 - #22: Dev and ELR, key is to adjust for exposure 2005 - #12: LS calc by \\(a\\) and \\(b\\) 2006 - #5: LS calc by \\(a\\) and \\(b\\) 2006 - #15 a-c: Dev, BF, and cred 2009 - #3 a: LS Calc 2012 ‐ #1 a: Dev, BF, Benktander 1.5.1 Question Highlights Figure 1.1: 2000 Question 41 Figure 1.1: 2000 Question 41 Figure 1.2: 2001 Question 30 Figure 1.2: 2001 Question 30 -->"],
["credible-claims-reserve-the-benktander-method-t-mack.html", "Chapter 2 Credible Claims Reserve: The Benktander Method - T. Mack", " Chapter 2 Credible Claims Reserve: The Benktander Method - T. Mack Know the GB method formula (2.1) and properties Benktander recognize actual emergence faster Difference between working with Ultimate or Reserve When is the \\(GB\\) \\(MS\\) lower than \\(BF\\) Iterative form of BF and GB and it’s extension Theorem 2.1 (where GB is the 2nd iteration) "],
["mack-2000-gb-method.html", "2.1 Gunnar Benktander Method (GB)", " 2.1 Gunnar Benktander Method (GB) Proposition 2.1 (Benktander Reserve) Reserve under the Benktaner Method \\[R_{GB} = q_kU_{BF}\\] \\(q_k = (1 - p_k)\\) = % unpaid loss @ \\(k\\) \\(U_{BF} = C_k + q_kU_0\\) Weight on \\(U_0\\) is \\(q_k^2\\) Remark. Benktander recognizes actual emergence faster \\(\\Rightarrow\\) Less weight on the a-priori BF reduces the use of actual loss to the extent of the complement credibility \\(\\begin{align} U_{GB} &amp;= (1-q_k)U_{CL} + q_kU_{BF} &amp; \\cdots (1)\\\\ &amp;= (1-q_k^2)U_{CL} + q_k^2 U_0 &amp; \\cdots (2)\\\\ \\end{align}\\) Crediblity weight \\(U_{BF}\\) and \\(U_{CL}\\) Crediblity weight \\(U_{CL}\\) and a priori 2.1.1 Method Comparison Table 2.1: Comparison of Ultimate Loss and Reserve for Different Methods Method Ultimate (\\(U\\)) Reserve (\\(R\\)) Chain Ladder \\(\\dfrac{C_k}{p_k}\\) \\(q_k U_{CL} = q_k \\dfrac{C_k}{p_k}\\) BF Method \\(C_k + q_kU_0\\) \\(q_k U_0\\) GB Method \\(C_k + q_kU_{BF}\\) \\(q_k U_{BF}\\) Theorem 2.1 For an arbitrary starting point \\(U^{(0)} = U_0\\) and the iteration rule \\(R^{(m)} = q_k U^{(m)}\\) and \\(U^{(m+1)} = C_k + R^{(m)}\\), \\(m = 0, 1, 2, ...\\) gives credibility mixtures \\[U^{(m)} = (1-q^m_k)U_{CL} + q^m_k U_0\\] \\[R^{(m)} = (1-q^m_k)R_{CL} + q^m_k R_0\\] between \\(BF\\) and \\(CL\\) which start at \\(BF\\) and lead via \\(GB\\) to \\(CL\\) for \\(m= \\infty\\) Remark. The reserve formula of theorem 2.1 seems to be wrong in Mack but it’s right in Hurlimann 2.1.2 MSE MSE of Benktander is almost as small as the MSE of the optimal credibility in most cases \\[MSE(R_{GB}) &lt; MSE(R_{BF})\\] When \\(p_k \\in [0, 2c^*]\\); \\(c^*\\) is the optimal credibility \\(R_{GB}\\) doesn’t have the lowet \\(MSE\\) only when \\(p_k &gt; 2c^*\\) Doesn’t hold if \\(c^*\\) is small and \\(p_k\\) is large Remark. \\[MSE(R_c) = \\mathrm{E}[R_c - R]^2\\] \\(R_c = c R_{CL} + (1-c)R_{BF}\\) \\(R = U - C_k = C_n - C_k\\) Where: \\(c = 0\\) for \\(BF\\) \\(c = p_k\\) for \\(GB\\) \\(c = c^*\\) for optimal credibility where \\(c \\in [0, 1]\\) "],
["notation.html", "2.2 Notation", " 2.2 Notation \\(U =\\) Ultimate Loss \\(R =\\) Estimate of Unpaid losses \\(U_0 =\\) a priori estimate of Ultimate Losses \\(p_k =\\) % of total losses paid at \\(k\\) \\(q_k = 1 - p_k =\\) % of total losses unpaid a \\(k\\) \\(C_k =\\) Actual paid losses at \\(k\\) "],
["past-exam-questions-1.html", "2.3 Past Exam Questions", " 2.3 Past Exam Questions Concepts 2006 - #16 b, c: GB method vs BF and dev GB has lower MSE and gives weight to both a-priori and emerged 2007 - #46 c,d: BF weight, BF drawback, GB advantages over BF 2008 - #10 b-c: Comparison of methods and GB as credibility weight 2013 - #4 b: GB approaches CL as more iterations are done Simple Plug and play 2004 - #31: GB method ultimate 2005 - #16: GB method ultimate 2006 - #16 a: GB method ultimate 2007 - #46 a, b: BF weight, BF drawback, GB advantages over BF 2012 - #1 a: GB BF Dev \\(\\star\\) 2014 - #5: Credibility weight methods from the Clark paper on LDF curve fitting Weight to the Dev method is \\((1-q_k)\\) TIA 1: Straight calculate TIA 2: Back out numbers with formulas Long calculation \\(\\star \\star\\) 2016 - #1: Use GB with incremental LR projection Know the GB method extension theorem This is from Hurlimann equation with optimal credibility (3.1) and you need that additional assumption for the formula to work Arithmetics \\(\\star\\) 2012 - #1 b (fig 2.1): Minor arithmetic \\(\\star\\) 2013 - #4 a: Back out LDFs with BF and GB methods TIA 3: Long calc for all years reserve with BF and GB 2.3.1 Question Highlights Figure 2.1: 2012 Question 1 Figure 2.1: 2012 Question 1 -->"],
["credible-claims-reserve-benktander-neuhaus-and-mack-w-hurlimann.html", "Chapter 3 Credible Claims Reserve: Benktander, Neuhaus and Mack - W. Hurlimann", " Chapter 3 Credible Claims Reserve: Benktander, Neuhaus and Mack - W. Hurlimann Everything is based on a special method to calculate the \\(ELR\\) and \\(LDFs\\) \\(ELR\\) is based on the whole triangle \\(\\sum\\)-ing up column incremental LRs \\(LDFs\\) are based \\(\\dfrac{\\text{Col LRs}}{ELR}\\) Key is just to watch out for the given data is incremental vs cumulative Know the \\(Z\\) for different methods (Table 3.1) The \\(Z\\) is for weighting the reserve Optimal credibility formula assumes: \\(U_i^{BC} {\\perp\\!\\!\\!\\!\\perp} \\: C_i\\) and \\(R_i\\) \\(\\mathrm{Var}(U_i) = \\mathrm{Var}(U_i^{BC})\\) Basis of optimal credibility 3.2 MSE for the credibility formula All the methods can be done using traditional LDFs Remember “shortcuts” to calculate the CDFs given incremental triangles Also shortcut to calculate the unpaid based on total “unused” premium "],
["lr-claim-res-def.html", "3.1 Loss Ratio Claims Reserve Definition", " 3.1 Loss Ratio Claims Reserve Definition \\(m_k\\): Expected loss ratio @ each age \\(k\\) Based on incremental column paid loss ratios \\(k \\in \\{1, ..., n \\}\\) For \\(n\\) development periods \\(ELR\\): Expected loss ratio: \\[ELR = \\sum \\limits_{k=1}^n m_k\\] a priori ELR for collective loss ratio approach Use for the entire triangle \\(p_k\\): % Losses emerged for exposure period \\(k\\) \\[p_k = \\dfrac{\\sum \\limits_{j=1}^{n} m_j}{ELR}\\] Based on column loss ratios \\(m_k\\) Loss ratio payout factor or loss ratio lag-factor \\(q_k = 1 - p_k\\) is the loss ratio reserve factor 3.1.1 Loss Ratio Claims Reserve Summary \\[\\begin{equation} R_i^c = Z_i \\times R_i^{ind} + (1-Z_i) \\times R_i^{coll} \\tag{3.1} \\end{equation}\\] Table 3.1: Comparison of \\(Z_i\\) for Different Methods \\(\\mathbf{Z_i}\\) Method 1 Chainladder; Individual LR 0 BF; Collective LR \\(p_k\\) Benktander (GB)3 \\(p_k \\times ELR\\) Neuhaus (WN)1 \\(\\dfrac{p_k}{p_k + \\sqrt{p_k}}\\) Optimal Credibility2 Remark. Neuhaus gives low credibility to lines with low loss ratios Since Neuhaus use loss ratio, \\(\\Delta\\) exposure base will \\(\\Delta\\) result Neuhaus credibility = expected loss ratio to date Optimal credibility is capped @ 0.5 There’s additional assumptions made to get to the optimal credibility formula This is the same as Mack-2000 but just defined with \\(p_k\\) Note we’re weighting the reserve here Benktander and Neuhaus reduce the MSE of the reserve estimate nearly to an optimal level outperforming individual and collective Proposition 3.1 (Individual Loss Ratio Claims Reserve) Analogous to chainladder \\(\\begin{align} R_i^{ind} &amp;= \\dfrac{C_{ik}}{p_k} \\times q_k \\\\ &amp;= \\dfrac{C_{ik}}{p_k} - C_{ik} \\\\ &amp;= U_i^{ind} - C_{ik} \\\\ \\end{align}\\) Proposition 3.2 (Collective Loss Ratio Claims Reserve) Analogous to BF \\(\\begin{align} R_i^{Coll} &amp;= q_k(V_i \\times ELR) \\\\ &amp;= q_k(U_i^{BC}) \\\\ \\end{align}\\) BC = Burning Cost \\(V_i\\) = premium for year \\(i\\) 3.1.2 Optimal Credibility Weights Optimal credibility weights for loss ratio claims reserve \\(Z^*_i\\) is the credibility that minimizes the \\(MSE(R_i^c) = \\mathrm{E}[(R_i^c - R_i)^2]\\) Theorem 3.1 Optimal credibility factor \\(c^*\\) that minimizes \\(MSE(R_i^c) = \\mathrm{E}[(R_i^c - R_i)^2]\\) is \\[Z^*_i = \\dfrac{p_i}{q_i} \\dfrac{Cov(C_i, R_i) + p_i q_i Var(U_i^{BC})}{Var(C_i) + p_i^2 Var(U_i^{BC})}\\] Table 3.2: Impact of different components on \\(Z_i^*\\) Impact on \\(\\mathbf{Z_i^*}\\) Comments Losses emerge Increase Since \\(\\dfrac{p_i}{q_i}\\) increases as losses emerge \\(\\mathrm{Cov}(C_i, R_i)\\) increase Increase Large covariance implies that \\(C_i\\) is predictive of \\(R_i\\) \\(\\Rightarrow\\) More weight on \\(CL\\) method \\(\\mathrm{Var}(C_i)\\) increase Decrease If \\(C_i\\) is volatile, we want to rely less on \\(CL\\) method \\(\\mathrm{Var}(U_i^{BC})\\) increases Increase Trust \\(CL\\) method more when a-priori is volatile Remark. Assumes \\(U_i^{BC} {\\perp\\!\\!\\!\\!\\perp} \\: C_i\\) and \\(R_i\\) Large \\(Var(U_i^{BC})\\) \\(\\Rightarrow\\) \\(Z \\approx \\dfrac{p}{q} \\times \\dfrac{pq}{p^2} = 1\\) Assumes \\(\\mathrm{E}\\left[ \\dfrac{C_{ik}}{U_i} \\mid U_i \\right] = p_k\\) and \\(\\mathrm{Var}\\left( \\dfrac{C_{ik}}{U_i} \\mid U_i \\right) = p_k q_k \\beta^2(U_i)\\) Theorem 3.2 Under the additional assumptions above, we have \\[Z_i^* = \\dfrac{p_k}{p_k + t_k}\\] Where \\(t_k = \\dfrac{\\mathrm{E}[\\alpha^2_i(U_i)]}{\\mathrm{Var}(U^{BC}_i) + \\mathrm{Var}(U_i) - \\mathrm{E}[\\alpha^2_i(U_i)]}\\) Theorem 3.3 If we assume \\(\\mathrm{Var}(U_i) = \\mathrm{Var}(U_i^{BC})\\) then \\[Z_i^* = \\dfrac{p_k}{p_k + \\sqrt{p_k}}\\] Where the above assumption lead to \\(t_k \\sim \\sqrt{p_k}\\) Remark. We would actually expect the \\(\\mathrm{Var}(U_i) &gt; \\mathrm{Var}(U_i^{BC})\\), but the above is just an assumption to make things simplier The i’s and the k’s are sort of interchangable 3.1.3 MSE \\[MSE(R^{ind}_i) = \\mathrm{E}\\left[\\alpha_i^2(U_i)\\right] \\cdot \\left( \\dfrac{Z_i^2}{p_i} + \\dfrac{1}{q_i} + \\dfrac{(1-Z_i)^2}{t_i} \\right) \\cdot q^2_i\\] \\(MSE(R^{ind}_i)\\) when \\(Z_i = 1\\) \\(MSE(R^{coll}_i)\\) when \\(Z_i = 0\\) "],
["hur-remark.html", "3.2 Remark 6.1", " 3.2 Remark 6.1 Doing all the above with more “Traditional” method Chainladder Replace \\(p_k\\) with \\(p_k^{CL}\\), inverse of the CDF \\(R^{ind}_i = \\dfrac{C_i}{p_k} - C_i\\) Cape Cod \\(ELR = \\dfrac{\\sum\\limits_{i,k}S_{ik}}{\\sum\\limits_i V_i \\times p_i^{CL}}\\) Sum of cumulative paid loss \\(\\div\\) used up premium \\(R^{Coll}_i = q_k \\times (V_i \\times ELR)\\) Benktander with \\(Z_i = p_k\\) \\(R^{GB}_i = Z_i \\times R^{ind}_i + (1 - Z_i) \\times R^{coll}_i\\) Optimal Cape Cod with \\(Z_i = \\dfrac{p_k}{p_k+\\sqrt{p_k}}\\) BF use some other a-priori that varies by AY and \\(Z = 0\\) Note that we are talking about applying \\(Z\\) to the formula above (Not the weighting between ultimates) "],
["notation-1.html", "3.3 Notation", " 3.3 Notation For \\(n \\times n\\) triangle and losses fully developed at \\(n\\) Where \\(i\\) is exposure period and \\(k\\) is the age \\(S_{ik} =\\) Incremental Paid \\(C_{ik} =\\) Cumulative Paid \\(U_i =\\) Ultimate loss \\(V_i =\\) Exposure base \\(m_k =\\) expected loss ratio \\(\\hat{m}_k =\\) estimate of \\(m_k\\) "],
["past-exam-questions-2.html", "3.4 Past Exam Questions", " 3.4 Past Exam Questions Full Calculation 2013 #2 (fig 3.1): Reserve calc \\(\\star\\) 2015 #1: Neuhaus and optimal \\(\\star\\) 2016 #1: combined with Mack-2000 TIA 1: Full calc all AYs for Neuhaus and Optimal TIA 2: Full calc all AYs for Optimal TIA 3: Full calc all AYs for traditional LDF, and individual loss ratio method and 1 year of optimal \\(\\star\\) remember shortcuts to get your wtd incremental LDFs from an incremental triangle TIA 4: Full calc all AYs for Optimal based on traditional LDFs \\(\\star\\) remember to use cape cod for the LR under traditional method ### Question Highlights Figure 3.1: 2013 Question 2 Figure 3.1: 2013 Question 2 -->"],
["measuring-the-variability-of-chain-ladder-reserve-estimate-t-mack.html", "Chapter 4 Measuring the Variability of Chain Ladder Reserve Estimate - T. Mack", " Chapter 4 Measuring the Variability of Chain Ladder Reserve Estimate - T. Mack \\(\\star\\) 3 underlying assumptions of chain ladder that makes the implicit assumptions hold and when they might be violated Expected incremental losses are \\(\\propto\\) losses to date, dependent on age (prop. 4.1) Losses in each AY are \\(\\perp\\!\\!\\!\\!\\perp\\) of the losses in other AYs (prop. 4.2) Variance of the incremental losses is \\(\\propto\\) losses reported to date dependent on age (prop. 4.3) \\(\\star\\) 3 different weight and variance assumptions from table 4.1 \\(\\star\\) Mean squared error calculation The big MSE formula (4.4) How to get the \\(\\alpha\\)’s we need for the MSE formula (4.5) and (4.6) Confidence Interval Normal: equation (4.7) \\(\\star\\) Log-normal: equation (4.8) Use log-normal when \\(s.e.(\\hat{R}_i) &gt; \\dfrac{R_i}{2}\\) \\(\\star\\) When the above assumptions might be violated and know the 4 test of assumptions to check Test 1. Intercept Test 2. Residuals Formula for residual (4.10) Test 3. CY Test How to get \\(z\\) (4.11) Expected value (4.12) and variance (4.13) Or just memorize them up to 6 (Table 4.2) Test 4. Adjacent LDF Correlation \\(T\\) for age \\(k\\) (4.15) and \\(S\\) (4.14) \\(T\\) for the whole triangle (4.16) CI to compare with resutls (4.17) This is test at a lower CI This is similar to Venter’s test Why test the whole triangle versus just pairs of LDFs "],
["cl-ass.html", "4.1 Chainladder Underlying Assumptions", " 4.1 Chainladder Underlying Assumptions Definition 4.1 Notations use for Mack \\(c_{i,k} =\\) cumulative losses for AY \\(i\\) @ age \\(k\\) \\(f_k =\\) LDF from \\(k\\) to \\(k + 1\\), \\(k \\in [1:I-1]\\) \\(I =\\) size of the triangle Proposition 4.1 (Chain Ladder Assumption 1) \\[\\mathrm{E}\\left [c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ] = c_{i,k} \\times f_k\\] Remark. Expected incremental losses are \\(\\propto\\) losses to date Proportion depends on the age \\(k\\) of AY Our best estimate of ultimate depends only on the losses to date Ignores prior period losses Corollary 4.1 Restating Chain Ladder Assumption 1: \\[\\mathrm{E}\\left[ \\dfrac{c_{i,k+1}}{c_{i,k}} \\mid c_{i,1},...,c_{i,k} \\right]\\] Remark. Expected LDF is unbiased Implies that development to \\(c_{i,k+1}\\) is independent of the size of losses at \\(c_{i,k}\\) Implies that adjacent LDFs are independent Proposition 4.2 (Chain Ladder Assumption 2) \\[\\left \\{c_{i,1} \\cdots c_{i,I} \\right \\} \\: {\\perp\\!\\!\\!\\!\\perp} \\: \\left \\{c_{j,1} \\cdots c_{j,I} \\right \\} \\:\\: : \\:\\: i \\neq\\ j\\] Remark. Losses in each AYs are \\({\\perp\\!\\!\\!\\!\\perp}\\) of the losses in other AYs This assumption make our estimate unbiased Proposition 4.3 (Chain Ladder Assumption 3) \\[\\mathrm{Var}\\left (c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ) = \\alpha_k^2 \\: c_{i,k}\\] Remark. Variance of the incremental losses is \\(\\propto\\) losses reported to date Proportion depends on the age (\\(k\\)) of AY (i.e. same for each column but varies for each column) \\(\\hat{f}_k\\) is selected to minimize the variance 4.1.1 Implicit Assumptions Taking a step back and look at the implicit assumptions we make when using CL For these implicit assumptions to hold, the 3 underlying assumptions have to be true We are making assumptions on how we select the factor \\(\\hat{f}_k\\) and the application: \\[\\hat{c}_{i,k+1} = c_{i,k} \\times \\hat{f}_k\\] Which requires the following assumptions: Unbiased estimate of each \\(f_k\\) \\(\\mathrm{E}\\left[ \\hat{f}_k \\right] = f_k\\) \\(\\hat{f}_k\\) are representative of the true \\(f_k\\) Based on assumption 1 &amp; 2 (prop. 4.1 &amp; prop. 4.2) See proof in Mack appendix A Unbiased estimate of Ultimate \\(\\mathrm{E}\\left[ \\hat{c}_{iI} \\right] = c_{ik} \\times \\hat{f}_k \\times \\cdots \\times \\hat{f}_{I-1} = \\mathrm{E}\\left[ c_{iI} \\right]\\) Multiplying the \\(\\hat{f}_k\\)’s by the paid to date will give us an unbiased estimate of the future losses Based on assumption 1 &amp; 2 (prop. 4.1 &amp; prop. 4.2) See proof in Mack appendix C To use volume weighted average LDF Based on assumption 3 (prop. 4.3) To calculate the confidence interval Based on assumption 3 (prop. 4.3) 4.1.2 Proof for Assumption 3 To estimate \\(f_k\\) we can weight the historical LDFs in many different ways, putting it in general terms: \\[\\begin{equation} \\hat{f}_k = \\sum_i \\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right) \\times w_{i,k} \\:\\:\\:\\: : \\:\\:\\:\\: \\sum_i w_{i,k} = 1 \\tag{4.1} \\end{equation}\\] Remark. We assume each of the \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) are an unbiased estimate of \\(f_k\\) (See proof below) \\(\\therefore\\) Any weighting of them is an unbiased estiamte of \\(\\hat{f}_k\\) Proof. \\[\\begin{align} \\mathrm{E}\\left[ \\dfrac{c_{i, k+1}}{c_{i,k}} \\right] &amp;= \\mathrm{E} \\left[ \\mathrm{E} \\left[ \\dfrac{c_{i, k+1}}{c_{i,k}} \\mid c_{i,1}, ...,c_{i,k}\\right] \\right] &amp;\\cdots \\:\\:\\: (1)\\\\ &amp;= \\mathrm{E} \\left[ \\mathrm{E} \\left[ c_{i, k+1} \\mid c_{i,1}, ...,c_{i,k}\\right] / c_{i,k} \\right] &amp;\\cdots \\:\\:\\: (2)\\\\ &amp;= \\mathrm{E} \\left[ c_{i,k} \\: f_k \\:/ c_{i,k} \\right] &amp;\\cdots \\:\\:\\: (3)\\\\ &amp;= \\mathrm{E} \\left[f_k\\right] \\\\ &amp;= f_k &amp;\\cdots \\:\\:\\: (4) \\\\ \\end{align}\\] Remark. Because the iterative rule \\(\\mathrm{E}[X] = \\mathrm{E}[\\mathrm{E}(X \\mid Y)]\\) Because \\(c_{i,k}\\) is scalar since it’s given From assumption 1 (prop. 4.1) Because \\(c_{i,k}\\) is scalar Remark. When we use weighted volume average \\(w_{i,k} = \\dfrac{c_{i,k}}{\\sum \\limits_{j} c_{j,k}}\\) Based on theory of point estimation, among several unbiased estimators, prefrence should be given to the one with the smallest variance The weights that minimize the variance is inversely proportional to the variance of the item we are weighting, i.e.: We want weights \\(w_{i,k}\\) for each \\(i\\) on \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) The weight we apply to each \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) varies based on the variance of \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) And if \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) has high variance its weight will be lower to minimize the total variance of our estimate of \\(\\hat{f}_k = \\left( \\dfrac{c_{1,k+1}}{c_{1,k}} \\right) \\times w_{1,k} + \\cdots + \\left( \\dfrac{c_{I,k+1}}{c_{I,k}} \\right) \\times w_{I,k}\\) Mack appendix B has a proof of this High variance estimate get lower weight: \\[\\dfrac{1}{w_{i,k}} \\propto \\mathrm{Var}\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\] Since \\(c_{i,k}\\) is known, we can pull it out of the variance term \\[\\dfrac{1}{w_{i,k}} \\propto \\dfrac{\\mathrm{Var}(c_{i,k+1})}{c_{i,k}^2}\\] and we get \\[\\begin{equation} w_{i,k} \\times \\mathrm{Var}(c_{i,k+1}) \\propto c_{i,k}^2 \\tag{4.2} \\end{equation}\\] Recall the weight for the volume weighted average is: \\[\\begin{align} w_{i,k} &amp;= \\dfrac{c_{i,k}}{\\sum \\limits_{j} c_{j,k}}\\\\ w_{i,k} &amp;\\propto c_{i,k}\\\\ \\end{align}\\] Applying the above to equation (4.2) we get: \\[\\begin{align} c_{i,k} \\times \\mathrm{Var}(c_{i,k+1}) &amp;\\propto c_{i,k}^2 \\\\ \\mathrm{Var}(c_{i,k+1}) &amp;\\propto c_{i,k} \\\\ \\mathrm{Var}(c_{i,k+1}) &amp;= \\alpha^2_k \\times c_{i,k} \\\\ \\end{align}\\] And we have chainladder assumption 3 (Prop. 4.3) 4.1.3 LDF Selections Assumptions Recall equation (4.1) and (4.2) Table 4.1: Relationships between weight, variance and residual (Mack) Weight \\(w_{i,k}\\) Description Variance Residual (4.10) 1 Simple Average \\(\\alpha_k^2 \\times \\mathbf{c_{i,k}^2}\\) \\(\\varepsilon_{i,k} = \\dfrac{c_{i,k+1} - c_{i,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{i,k}^2}}}\\) \\(c_{i,k}\\) Weighted Average \\(\\alpha_k^2 \\times \\mathbf{c_{i,k}}\\) \\(\\varepsilon_{i,k} = \\dfrac{c_{i,k+1} - c_{i,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{i,k}}}}\\) \\(c_{i,k}^2\\) Least Square2 \\(\\alpha_k^2 \\times \\mathbf{1}\\) \\(\\varepsilon_{i,k} = \\dfrac{c_{i,k+1} - c_{i,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{1}}}\\) Note the least square here we are forcing the intercept through the origin Assumption for LS is that variance is the same for each exposure year (See Brosius) Use different method for LDF selection based on variance assumption Variance and the weight always multiply to \\(c_{j,k}^2\\) 4.1.4 Violation of Assumptions Correlation between on AYs and another Assumption 2 (Prop. 4.2) is violated Not necessarily assumption 1 (Prop. 4.1) e.g. Strong calendar year effects (i.e. faster payment, changing inflation) will lead to correlation along a diagonal Check using calendar year test A single \\(\\hat{f}_k\\) is not appropriate for all years \\(i\\) Assumption 1 (Prop. 4.1) is violated Dependence among columns Assumption 1 (Prop. 4.1) is violated Not necessarily assumption 2 (Prop. 4.2) If losses in the follow period are inversely correlated to the losses in the current period, then we’ll have correlation between adjacent LDFs but still maintain independence of AYs If residuals are not random around zero Assumption 3 (Prop. 4.3) is violated If we see any trends or change in magnitude Check with residual test 4.1.5 Strength/Weakness of Chainladder Here we are talking about the weakness and not the limitation due to it’s implicit assumptions Weakness Tail LDFs depend on very few observations High variability in the reported claims in the most recent years lead to uncertainty \\(c_{I,1} = 0\\) \\(\\Rightarrow\\) \\(\\hat{R}_{I,1} = 0\\), which is not reasonable Results need to be judged by someone who knows the business under consideration Unexpected future changes can make the observations obsolete Strength User knows exactly how the method works and it’s weakness Can be easily explained to non-actuaries "],
["mack-mse.html", "4.2 Mean Squared Error", " 4.2 Mean Squared Error We wish to measure the average error between the ultimate losses \\(c_u\\) and the estimate \\(\\hat{c}_u\\) \\[\\begin{align} MSE(\\hat{c}_{i,I}) = \\mathrm{E}\\left[(c_{i,I}-\\hat{c}_{i,I})^2 \\mid D\\right] \\end{align}\\] Remark. Above represents the average error between estimate and actual utlimate given data to date \\(D\\) Standard error: \\(s.e. = \\sqrt{MSE}\\) Applying the properties of \\(\\mathrm{Var}(X) = \\mathrm{E}[X^2] - \\mathrm{E}[X]^2\\) we get \\[MSE(\\hat{c}_{i,I}) = \\mathrm{E}\\left[(c_{i,I}-\\hat{c}_{i,I})^2 \\mid D\\right] = \\mathrm{Var}\\left((c_{i,I}-\\hat{c}_{i,I}) \\mid D\\right) + \\left[\\mathrm{E}\\left[(c_{i,I}-\\hat{c}_{i,I}) \\mid D\\right]\\right]^2\\] We can pull out \\(\\hat{c}_{i,I}\\) from the variance and expectation term to get \\[\\begin{equation} MSE(\\hat{c}_{i,I}) = \\underbrace{\\mathrm{Var}\\left(c_{i,I} \\mid D\\right)}_{\\text{Process Variance}} + \\underbrace{\\left[\\mathrm{E}\\left[c_{i,I} \\mid D\\right] - \\hat{c}_{i,I} \\right]^2}_{\\text{Parameter Variance}} \\tag{4.3} \\end{equation}\\] \\(R_i\\) is the unpaid for year \\(i\\): \\[R_i = c_{i,I} - c_{i,k}\\] So the estimate of the unpaid claim is: \\[\\hat{R}_i = \\hat{c}_{i,I} - c_{i,k}\\] Since the difference between \\(\\hat{R}_i\\) and \\(\\hat{c}_{i,I}\\) is just paid to date (constant) so the MSE are the same \\[MSE(\\hat{R}_i) = MSE(\\hat{c}_{i,I})\\] 4.2.1 Applying the MSE Formula \\[\\begin{equation} MSE(\\hat{c}_{i,I}) = \\left[ s.e.(\\hat{c}_{i,I}) \\right]^2 = {\\hat{c}_{i,I}}^2 \\Bigg \\{ \\sum_{k = I + 1 - i}^{I-1} \\frac{{\\hat{\\alpha}_k}^2}{{\\hat{f}_k}^2} \\Bigg ( \\frac{1}{\\hat{c}_{i,k}} + \\underbrace{\\frac{1}{\\sum_{j=1}^{I-k}c_{j,k}}}_{\\text{Column x latest}}\\Bigg ) \\Bigg \\} \\tag{4.4} \\end{equation}\\] Remark. The big \\(\\sum\\) sum over the remaining years till ultimate For bit outside the \\(\\sum\\) is same for the row 4.2.1.1 Estimating \\(\\alpha\\) \\(\\alpha\\) is the proportions for the variance \\[\\begin{equation} {\\hat{\\alpha}_k}^2 = \\frac{1}{I - k - 1} \\sum_{j=1}^{I-k} c_{j,k} \\Big ( \\underbrace{\\frac{c_{j,k+1}}{c_{j,k}}-\\hat{f_k}}_{\\text{AY LDFs - Selected}} \\Big )^2 \\tag{4.5} \\end{equation}\\] Remark. This one varies by age (columns), for each age: Calculate the difference2 of the LDFs for each AY and the selected Then multiply (weight) by each AY’s cumulative loss for given age Sum the above and then divide by the number of rows minus 1 \\(\\alpha^2_{I-1}\\) (e.g. \\(\\alpha^2_9\\) for a 10 \\(\\times\\) 10 triangle) \\[\\begin{equation} {\\hat{\\alpha}_{I-1}}^2 = \\mathrm{min} \\left( {\\hat{\\alpha}_{I-2}}^2 \\times \\dfrac{{\\hat{\\alpha}_{I-2}}^2}{{\\hat{\\alpha}_{I-3}}^2},{\\hat{\\alpha}_{I-3}}^2 \\right) = \\begin{cases} {\\hat{\\alpha}_{I-3}}^2 &amp; \\text{if } {\\hat{\\alpha}_{I-3}}^2 &lt; {\\hat{\\alpha}_{I-2}}^2 \\\\ {\\hat{\\alpha}_{I-2}}^2 \\times \\dfrac{{\\hat{\\alpha}_{I-2}}^2}{{\\hat{\\alpha}_{I-3}}^2} &amp; \\text{else}\\\\ \\end{cases} \\tag{4.6} \\end{equation}\\] Remark. For the last \\(\\alpha\\): If the 3rd last \\(\\alpha\\) is lower than the 2nd last \\(\\alpha\\), use the 3rd last \\(\\alpha\\) (If the final \\(\\alpha\\)’s are not trending down, take the lower one) If the final \\(\\alpha\\)’s are decreasing, then just take the same % of decrease to get the last \\(\\alpha\\) Also note that if we believe the claims development have stopped at some age \\(j\\) then we can set \\(\\alpha^2_j = 0\\) Finally, you can also fit a logarithmic curve to the \\(\\alpha^2_k\\)’s to estimate \\(\\alpha^2_{I-1}\\) 4.2.2 MSE Calculation Flow For a \\(10 \\times 10\\) triangle Step 1: Calculate all the \\(\\hat{f}_k\\) Step 2: Calculate all the \\({\\hat{\\alpha}_k}^2\\) Calculate a \\(\\alpha_{i,k}^2\\) triangle Each cell is \\(c_{j,k} \\Big ( \\underbrace{\\frac{c_{j,k+1}}{c_{j,k}}-\\hat{f_k}}_{\\text{AY LDFs - Selected}} \\Big )^2\\) \\({\\hat{\\alpha}_k}^2\\) is the sum of column \\(k\\) over the number of cells - 1 See special case for the last \\(\\alpha_k^2\\) Step 3: Calculate the projected triangle using the \\(\\hat{f}_k\\) Step 4: Calculate a \\(MSE(\\hat{c}_{i,k+1})\\) triangle Figure 4.1: Breakdown of MSE formula for each cell Step 5: Get the \\(MSE\\) for the AY by summing up a given row of \\(MSE(\\hat{c}_{i,I})\\) Step 6: Get the total \\(MSE\\) for the total reserve (all AYs) you have to consider the dependencey between AYs since we use the same LDFs "],
["mack-CI-methods.html", "4.3 Confidence Intervals", " 4.3 Confidence Intervals Can have different assumptions on the distribution of the unpaid Normal Estimation \\[\\begin{equation} \\hat{R}_i \\pm Z_\\alpha \\: s.e.(\\hat{R}_i) \\tag{4.7} \\end{equation}\\] Remark. Under CLT we can assume that \\(\\hat{R}_i\\) is normally distributed given that the outstanding claims are large We can get the 95% CI with \\(Z_{0.975} = 1.96\\) Log-Normal Estimation \\[\\begin{equation} e^{\\mu_i + Z_{\\alpha} \\sigma_i} = \\hat{R}_i \\times \\exp\\left \\{ -\\dfrac{\\sigma_i^2}{2} \\pm \\: Z_\\alpha \\sigma_i \\right \\} \\tag{4.8} \\end{equation}\\] \\[\\sigma_i^2 = \\mathrm{ln} \\left [ 1 + \\left ( \\dfrac{s.e.(\\hat{R}_i)}{\\hat{R}_i} \\right)^2 \\right]\\] \\[\\mu_i = \\mathrm{ln}(\\hat{R}_i) - \\dfrac{\\sigma_i^2}{2}\\] Remark. Use log-normal when the true distribution of \\(R_i\\) is skewed Especially true when \\(s.e.(\\hat{R}_i) &gt; \\dfrac{R_i}{2}\\) This would give us negative value for the bottom end of the CI if we use normal distribution 4.3.1 CI for All Years Reserves Mean is easy: \\(\\hat{R} = \\sum \\limits_{i} \\hat{R}_i\\) But since \\(\\hat{R}_{i}\\) rely on the same LDFs they are not independent and we need to include a correlation factor for the \\(MSE(\\hat{R})\\) \\[\\begin{equation} [s.e.({\\hat{R}})]^2 = \\sum \\limits_{i=2}^I \\left\\{ [s.e.(\\hat{R}_i)]^2 + \\hat{c}_{i,I} \\left( \\sum \\limits_{j = i+1}^{I} \\hat{c}_{j,I} \\right) \\left( \\sum \\limits_{k = I + 1 - i}^{I - 1} \\dfrac{2 {\\hat{\\alpha_k}}^2 \\big/ {\\hat{f_k}}^2}{\\sum_{n=1}^{I-k}c_{n,k}}\\right) \\right\\} \\tag{4.9} \\end{equation}\\] If we want to simplify things we can use the square root rule to sum up the different AYs if we assume independence 4.3.2 CI Application &amp; Range Total CI Consider the ratio \\(\\dfrac{s.e.(\\hat{R}_i)}{\\hat{R}_i}\\) Can be high for older years since reserve is small but absolute s.e. is also small \\(\\therefore\\) not important Good to calculate the ratio since we need it for \\(\\sigma_i^2\\) Tend to be high for most recent AY as well Driven by the large uncertainty around the development from age 12 - 24 Recall from previous section we mentioned that we should use log-normal distribution for the CI if \\(s.e.(\\hat{R}_i) &gt; \\dfrac{R_i}{2}\\), this is equivalent to \\(\\dfrac{s.e.(\\hat{R}_i)}{\\hat{R}_i} &gt; 50\\%\\) Then calculate the total reserve CI Total ratio should be greater than the simple sum because the LDFs are positively correlated Allocate CI to each AY After calculating the all year CI, we can allocate the upperbound the total CI to each AYs Through trial &amp; error to figure out the \\(Z_{\\alpha}\\) for each AY that would yield the total upperbound CI Empirical Limits Use the max/min historical LDFs for each age \\(k\\) to get the upper and lower limit of the confidence interval Caveat: CI is too small for older ages as there are only a few LDFs (and too large for younger ages) \\(\\therefore\\) Not very useful "],
["chain-ladder-assumptions-test.html", "4.4 Chain Ladder Assumptions Test", " 4.4 Chain Ladder Assumptions Test Tests on the various Chain Ladder assumptions Intercept Residuals Calendar year test Correlation of adjacent LDFs 4.4.1 Intercept Test for assumption 1 (prop. 4.1) Test Procedure Plot the losses at adjacent ages Do this for every age \\(k\\) vs age \\(k+1\\) Results Interpretation We expect to see the line of best fit goes through the origin if the chain ladder assumption holds 4.4.2 Residuals Test for assumption 3 (prop. 4.3) Test Procedure For each age \\(k\\), plot the \\(c_{i,k}\\) with the residuals \\(\\varepsilon_{i,k+1}\\) x-axis is the \\(c_{i,k}\\) and y-axis is \\(\\varepsilon_{i, k+1}\\) These are weighted residuals (Clark is normalized residual and bootstrap is pearson residual) \\[\\begin{equation} \\varepsilon_{i,k+1} = \\dfrac{c_{i,k+1} - c_{i,k} \\: \\hat{f_k}}{\\sqrt{\\mathrm{Var}(c_{i,k})}} \\tag{4.10} \\end{equation}\\] Remark. We can take out the \\(\\alpha^2_k\\) term since it’s constant for the same \\(k\\) e.g. \\(\\varepsilon = \\dfrac{c_{i,k+1} - c_{i,k} \\: \\hat{f_k}}{\\sqrt{c_{i,k}}}\\) for weighted average assumption For residuals @ \\(k\\), you need LDFs from \\(k-1\\) to \\(k\\) Note that the results would change depending on the unit (e.g. dollar, thousand, etc) but shouldn’t affect your conclusion You can calculate the different weighted LDFs with the table features on TI-30XS y terms can be the LDFs and x is the weight (ci, k2 or ci, k depending on the assumption) Then the LDF will be \\(\\dfrac{\\sum x y}{\\sum x}\\) Results Interpretation Residuals should vary randomly around zero across \\(c_{i,k}\\) Test can be used to test the various variance assumptions by calculating the \\(\\varepsilon\\) differently (See Table 4.1) If passed \\(\\Rightarrow\\) expected losses are linear w.r.t. cumulative losses paid to date 4.4.3 Calendar Year Test Test for assumption 2 (prop. 4.2) Test Procdeure Rank the LDFs in each column (1 = lowest) Label them \\(S\\) (small) and \\(L\\) (large) and the median is discarded For each diagonal \\(d\\) with at least 2 elements: \\[\\begin{equation} z^d = \\mathrm{min}(\\text{# of }S, \\text{# of }L) \\tag{4.11} \\end{equation}\\] Calculate \\(\\mathrm{E}[z_n]\\) and \\(\\mathrm{Var}(z_n)\\) for each diagonal \\(d\\) \\[\\begin{equation} \\mathrm{E}[z_n] = \\dfrac{n}{2} - c_n \\tag{4.12} \\end{equation}\\] \\[\\begin{equation} \\mathrm{Var}(z_n) = \\dfrac{n(n-1)}{4} - c_n (n-1) + \\mathrm{E}[z_n] - \\mathrm{E}[z_n]^2 \\tag{4.13} \\end{equation}\\] Remark. \\(n =\\) # of elements in each diagonal excluding the throw away value \\(c_n = {n - 1 \\choose m}\\frac{n}{2^n}\\) \\(m = \\mathrm{floor}\\left[ \\dfrac{n-1}{2} \\right]\\) \\(z \\sim\\) Normal Table 4.2: \\(\\mathrm{E}[z_n]\\) and \\(\\mathrm{Var}(z_n)\\) up to \\(n=6\\) n \\(\\mathrm{E}[z_n]\\) \\(\\mathrm{Var}(z_n)\\) 2 0.5 0.25 3 0.75 0.188 4 1.25 0.438 5 1.563 0.37 6 2.062 0.62 See if the observed \\(Z\\) is in the CI \\[\\begin{equation*} Z = \\sum_{d} z^d \\end{equation*}\\] \\[\\begin{equation*} \\mathrm{E}[Z_n] = \\sum_{d} \\mathrm{E}[z_n^d] \\end{equation*}\\] \\[\\begin{equation*} \\mathrm{Var}[Z_n] = \\sum_{d} \\mathrm{Var}[z_n^d] \\end{equation*}\\] Remark. Since \\(z \\sim Normal\\), can sum the mean and variance by assuming independence Test 95% CI: \\(\\mathrm{E}[Z_n] \\pm 2 \\times \\sqrt{\\mathrm{Var}(Z_n)}\\) Results Interpretation If the observed \\(Z\\) is outside the CI range \\(\\Rightarrow\\) There is calendar year effects and assumption (2) is violated 4.4.4 Correlation of Adjacent LDFs Test assumption (1) (prop. 4.1) Measures correlation between each column and the adjacent column We want to test if there is a correlation among columns for the triangle as a whole \\(\\therefore\\) We define one test statistics for the whole triangle Use rank correlation (e.g. Spearman’s correlation coefficient \\(T\\)) instead of value correlation (e.g. Pearson correlation) Because LDFs down the column for a given age \\(k\\) have different variance See Venter for his method too Spearman correlation coefficient is defined as the Pearson correlation coefficient between the ranked variables We are testing for independence Which is more strict than just testing for 0 correlation Threshold use is relatively low, at 50%, as an indicator that we need to investigate further Reason to consider the correlation of a triangle as whole instead of between pairs of columns More important to know whether correlations globablly prevail than to find a samll part of the triangle with correlation At 10% significance 10% of the pairs will show up as significant just by random (see more on Venter) Avoid an accumulation of error probabilities Test Procedure Calculate Spearman’s correlation coefficient \\(T_k\\) for each pair of adjacent LDFs \\[\\begin{equation} S_k = \\sum \\limits_{i} \\Big \\{ rank(f_{i,k-1}) - rank(f_{i,k}) \\Big \\}^2 \\tag{4.14} \\end{equation}\\] \\[\\begin{equation} T_k = 1 - \\dfrac{S_k}{n_k(n_k^2-1)/6} \\tag{4.15} \\end{equation}\\] Remark. Rank is for each column \\(k\\) from low to high (i.e. lowest is 1) \\(n_k =\\) number of pairs For a 10 x 10 triangle, \\(k \\in [2 , 8]\\) Only 9 LDFs so 8 pairs And we don’t use the column with only 1 row \\(k\\) starts at 2 by convention Calculate Spearman’s correlation coefficient \\(T\\) for the whole triangle \\[\\begin{equation} T = \\dfrac{\\sum T_k (n_k - 1)}{\\sum (n_k-1)} = \\dfrac{\\sum_k (I - k -1)T_k}{\\sum_k I - k -1} \\tag{4.16} \\end{equation}\\] Remark. Formula is the weighted average of the \\(T_k\\)’s, weight = \\(n_k - 1\\) \\(I =\\) size of triangle Formula gives more weight to \\(T_k\\) with more data Compare \\(T\\) with CI based on distribution \\[\\begin{equation} \\begin{array}{c} CI = \\mathrm{E}[T] \\pm Z \\sqrt{\\mathrm{Var}(T)} \\\\ \\mathrm{E}[T] = 0 \\\\ \\mathrm{Var}[T] = \\dfrac{1}{(I-2)(I-3)/2} \\\\ \\end{array} \\tag{4.17} \\end{equation}\\] Remark. Assume \\(T \\sim Normal(0, \\sqrt{\\mathrm{Var}(T)})\\) Use \\(Z_{75\\%} = 0.67\\) for range of [25%, 75%] Results Interpretation If the \\(T\\) is in the CI \\(\\Rightarrow\\) Do not reject the \\(H_0\\) of uncorrelated LDFs "],
["past-exam-questions-3.html", "4.5 Past Exam Questions", " 4.5 Past Exam Questions Concepts 2011 #3 b: CL method assumptions 2012 #5: CL assumptions and whether they are met 2014 #2: CL assumptions on intercept and residuals \\(\\star\\) 2014 #4 b: Related to both Venter and Mack CL LS assumptions TIA 1: Mack’s 3 assumptions TIA 3b: Cause of CY trend TIA 6c: Shortcomings of the empirical LDF method for range TIA 8b: Assumption being test with adj ldfs test \\(\\star \\star\\) TIA 13: Clark vs Mack assumptions Clark’s variance proportional constant is to the whole triangle (constant the same for the entire triangle) Assumption test \\(\\star\\) 2011 #3 a (fig 4.2): CY Test 2012 #3: Residuals test 2013 #1: CY test and potential cause \\(\\star\\) 2014 #4 a (fig 4.3): Adjacent LDFs correlation test From Venter 2015 #3: Residuals test and plot, remember to label them 2015 #4: CY Test 2016 #5: CY Test and potential reasons \\(\\star \\star\\) 2016 #6: LDF correlation test From Venter Spearman’s \\(T\\) for the whole triangle \\(\\star\\) Why test adjacent of whole triangle 2016 #7: Residual test \\(\\star\\) TIA 3a: (fig 4.4): CY test TIA 7: Residual graph under different variance assumptions \\(\\star\\) Residual graph for age 36 under Mack variance Note that you need 24-36 LDF and the plot is based on losses @ 24 \\(\\star\\) Residual graph under the LS variance assumption \\(\\star\\) TIA 8a: Adjacent LDF test \\(\\star\\) TIA 9: CY trend, with 90% CI (1.65) TIA 11: Adj LDF corr test and know what the conclusion is if they are outside of the CI Calclulation \\(\\star\\) 2011 #8: MSE calc TIA 2: simple average LDF and reserve calc (given the variance of loss) TIA 4: Reserve with weighted vol. average \\(\\star\\) Lognormal CI \\(\\star \\star\\) TIA 5: full MSE calculation \\(\\star\\) TIA 6: Applying the CI Empirical LDF method for upper bound \\(\\star \\star\\) TIA 10: 1-in-20 upper bound reserve Test to use normal or log normal CI Allocation by trial and error \\(\\star\\) TIA 11: MSE Calc 4.5.1 Question Highlights Figure 4.2: 2011 Question 3 Figure 4.2: 2011 Question 3 Figure 4.3: 2014 Question 4 Figure 4.3: 2014 Question 4 Figure 4.4: TIA Question 2a Figure 4.4: TIA Question 2a -->"],
["testing-the-assumptions-of-age-to-age-factors-g-venter.html", "Chapter 5 Testing the Assumptions of Age-to-Age Factors - G. Venter", " Chapter 5 Testing the Assumptions of Age-to-Age Factors - G. Venter Definitions and standards used for Venter \\(n\\), \\(m\\), \\(p\\) \\(\\star\\) The \\(n\\) excludes the first column Important for implication 2 counting parameters for adj SSE \\(\\star\\) Project incremental losses, \\(q(w,d+1)\\) here Column parameter \\(f(d)\\) is LDF - 1 \\(\\star \\star\\) Goodness of fit measure: Adj SSE (5.1) AIC (5.2) BIC (5.3) 6 testable implications Statistical significance of factors Coefficient \\(&gt; 2 \\sigma\\) is significant \\(\\star \\star\\) Is there a better estimate for \\(q\\) than \\(f \\times c\\) Number of parameters (Table 5.3) Counting parameters \\(\\star \\star\\) BF parameters (Table 5.4) Iteration process Check residuals against \\(c(w,d-1)\\) Stability of \\(f(d)\\) down the column \\(\\star \\star\\) No correlation among columns Remember we are testing \\(f(d)\\) so LDF - 1 Pearson correlation calculation (5.4) and test statistics (5.5) To test for all different pairs (not just adjacent pairs) (5.6) Similar to Mack’s adjacent LDF test No particularly high or low diagonals Setting up regression with CY dummy variable (5.7) "],
["venter-def.html", "5.1 Definitions and Assumptions", " 5.1 Definitions and Assumptions Table 5.1: Tables of Definitions Venter Notations Definitions Mack Notations (4.1) \\(w\\) AYs \\(i\\) \\(d\\) Dev period; \\(d=0\\) is age @ end of year 1 \\(k\\) \\(c(w,d)\\) Cumulative losses for AY \\(w\\) age \\(d\\) \\(c_{i,k}\\) \\(c(w,\\infty)\\) Ultimate losses for AY \\(w\\) \\(c_{i,I}\\) \\(q(w, d+1)\\) Incremental losses for AY \\(w\\) age \\(d\\) to \\(d+1\\) \\(f(d)\\) Col parameters; (LDF - 1); applies to whole col \\(f_k - 1\\) \\(F(d)\\) LDF to ultimate that applies to \\(c(w,d)\\) \\(h(w)\\) Row parameters; applies to whole row 5.1.1 Mack’s Chainladder Assumptions Same assumptions as the Mack 1994 but stated in Venter’s terminology Venter refers these as assumptions needed for least-squares optimality to be achieved by the typical age-to-age factor method of loss development Proposition 5.1 (Mack Assumption 1) \\[\\mathrm{E}[q(w,d+1) \\mid \\text{Data to } w+d] = f(d) \\times c(w,d)\\] Remark. Here \\(f(d)\\) is LDF - 1 Expected incremental development is \\(\\propto\\) reported losses See also (Prop 4.1) from Mack (1994) Proposition 5.2 (Mack Assumption 2) \\[c(w,d) \\perp\\!\\!\\!\\!\\perp c(v,g) \\:\\:\\:\\: \\forall \\: d,g,v,w \\:\\:\\:\\: : \\:\\:\\:\\: v \\neq w\\] Remark. Losses not in the same row are independent of each other See also (Prop 4.2) from Mack (1994) Proposition 5.3 (Mack Assumption 3) \\[\\mathrm{Var}[q(w,d) \\mid \\text{Data to } w+d] = a_{fun}\\big(d,c(w,d)\\big)\\] Remark. Variance of incremental losses depends only on: Cumulative losses reported to date \\(c(w,d)\\) Age of the AY \\(d\\) (Does not vary by AY down the column) Different \\(a_{fun}(\\cdots)\\) leads to different \\(\\hat{f}(d)\\) estimate See also (Prop 4.3) from Mack (1994) 5.1.2 Variance Assumptions (for Chainladder) Same as shown in Mack 1994 Table 4.1 Table 5.2: Relationships between weight, variance and residual (Venter) Weight Description Variance \\(a_{fun}\\big(d,c(w,d)\\big)\\) LDF - 1 \\(f(d)\\) 1 Simple Average \\(k(d)c(w,d)^2\\) \\(\\dfrac{\\sum_w 1 \\frac{q(w,d+1)}{c(w,d)}}{\\sum_w 1}\\) \\(c(w,d)\\) Weighted Average \\(k(d)c(w,d)\\) \\(\\dfrac{\\sum_w c(w,d) \\frac{q(w,d+1)}{c(w,d)}}{\\sum_w c(w,d)}\\) \\(c(w,d)^2\\) Least Square \\(k(d)\\) \\(\\dfrac{\\sum_w c(w,d)^2 \\frac{q(w,d+1)}{c(w,d)}}{\\sum_w c(w,d)^2}\\) \\(c(w,\\infty) = F(d)c(w,d)\\) \\(F(d) = \\prod_{s \\geq d} (1 + f(s))\\) Recall \\(\\dfrac{q(w,d+1)}{c(w,d)}\\) are the empirical LDF - 1 Definition 5.1 (Chainladder Parameter definition) \\[\\mathrm{E}[q(w,d+1)] = f(d)c(w,d)\\] \\(n = \\sum \\limits_{i=1}^{m-1} i = \\dfrac{m(m-1)}{2}\\) = predicted data point? \\(p=m-1\\) since we don’t predict the first column \\(m =\\) dimension "],
["testable-implications.html", "5.2 6 Testable Implications", " 5.2 6 Testable Implications 6 Testable Implications Statistical significance of \\(f(d)\\) Is there a better estimate for \\(q\\) than \\(f \\times c\\) Check residuals against \\(c(w,d)\\) Test for Mack assumption 1 (4.1)? Stability of \\(f(d)\\) down the column Test for Mack assumption 1 (4.1)? No correlation among columns Test for Mack assumption 1 (4.1) No particularly high or low diagonals Test for Mack assumption 2 (4.2) 5.2.1 Goodness of Fit Measurement Compare different fit of the models based on adjusted \\(SSE\\) (actual vs projected excluding 1st column) Adjusted SSE \\[\\begin{equation} \\dfrac{SSE}{(n-p)^2} \\tag{5.1} \\end{equation}\\] Akaike Information Criterion \\[\\begin{equation} AIC \\approx SSE \\times e^{2p/n} \\tag{5.2} \\end{equation}\\] Bayesian Information Criterion \\[\\begin{equation} BIC \\approx SSE \\times n^{p/n} \\tag{5.3} \\end{equation}\\] Remark. \\(n =\\) # of predicted data points EXCLUDING 1st column Exclude because when we do reserving we don’t predict anything from the first column So usually equals number of cells in the triangle excluding first column \\(p =\\) # of parameters \\(SSE = \\sum (A - E)^2\\) Here you exclude the first column when calculating the difference Venter use the adjusted SSE as the AIC can be too permissive of over parameterization for large data sets SSE calculation can be done with the table features on TI-30XS Plug in each the actual and projected triangle into L1 and L2 (make sure the cells match) Calculate L3 = (L1 - L2)2 Calculate ∑L3 "],
["venter-imp-1.html", "5.3 Implication 1: Significance of Factors", " 5.3 Implication 1: Significance of Factors Check if the parameter coefficient is \\(&gt; 2 \\sigma\\) for 95% sure that the parameters are \\(\\neq 0\\) Can do \\(1.65 \\sigma\\) for 90% confidence Remember \\(f(d)\\) is LDF - 1 LDFs tend to fail towards the tail "],
["venter-imp-2.html", "5.4 Implication 2: Superiority of Alternative Emergence Patterns", " 5.4 Implication 2: Superiority of Alternative Emergence Patterns If an alternative emergence pattern provides a better explanation of the triangle, maybe it should be used Calculate \\(q(w,d)\\) under various emergence patterns (See Table 5.3) Calculate the Adjusted SSE (5.1) (based on every cell except the age 0 column) 5.4.1 Parameters: Alternative Emergence Pattern Table 5.3: Alternative emergence pattern on a \\(m \\times m\\) triangle Emergence Patterns # of Parameters \\(p\\) Comments \\(\\mathrm{E}[q(w,d+1) \\mid \\text{Data to }w+d] = f(d) c(w,d)\\) \\(m - 1\\) e.g. Chainladder \\(\\mathrm{E}[q(w,d+1) \\mid \\text{Data to }w+d] = f(d) c(w,d) + g(d)\\) \\(2m - 2\\) e.g. Least Squares \\(\\mathrm{E}[q(w,d) \\mid \\text{Data to }w+d] = f(d)h(w)\\) \\(2m-2\\) e.g. BF \\(\\mathrm{E}[q(w,d) \\mid \\text{Data to }w+d] = f(d)h\\) \\(m-1\\) e.g. Cape Cod Remark. \\(f(d) c(w,d) + g(d)\\) Often significant in forecasting age 1 Remark. \\(f(d)h(w)\\) Here \\(f(d)\\) is related to the % of losses emerged in period \\(d\\) Incremental % emergence, NOT LDF - 1 \\(h(w)\\) can be think of as an estimate of ultimate losses for AY \\(w\\) Like an a-priori The -2 for the BF is due to \\(f(0)\\) and constant (degrees of freedom?) If BF is better \\(\\Rightarrow\\) Loss emergence is more accurately represented as fluctuating around a proportion of expected ultimate losses (rather than proportion of reported losses) Cape Cod works when the loss ratio is stable (stable book of business) Use \\(h(w) = h \\times Premium(w)\\), so we only need stable ELR Cape Cod works out to an additive model \\(q(w,d) = h \\times f(d)\\) Can further reduce parameters by combining some row and column parameters Might be intuitively appealing to sum up the recent and tail years of the \\(h(w)\\) since there’s little empirical data to support different estimates We’ll be mostly focusing on this form 5.4.2 Variance Assumptions: Alternative Emergence Pattern Consider \\(\\mathrm{E}[q(w,d) \\mid \\text{Data to }w+d] = f(d)h(w)\\) from Table 5.3 We need to minimize the sum of squared residuals to get the optimal \\(f(d)\\) and \\(h(w)\\): \\[\\sum_{w,d} \\varepsilon^2(w,d) = \\sum_{w,d} [q(w,d) - \\underbrace{f(d)h(w)}_{\\mathrm{E}[q(w,d)]}]^2\\] Remark. Since this is a non-linear model, we need an iterative method to minimize to SSE We can use weighted least squares if the variances of the residuals are not constant over the triangle We need to minimize the variance of each residual \\(\\varepsilon(w,d)\\) \\[\\mathrm{Var}(\\varepsilon(w,d)) \\propto f(d)^p h(w)^q\\] Remark. \\(p\\) &amp; \\(q\\) typically \\(\\in [0,1,2]\\) And regression weights (applied to each \\(f(d)\\) or \\(h(w)\\)) will be \\(\\dfrac{1}{f(d)^p h(w)^q}\\) (inversely proportional to variance, similar to Mack 1994) Since \\(\\mathrm{E}[q(w,d)] = f(d)h(w)\\) \\(f(d) = \\dfrac{\\mathrm{E}[q(w,d)]}{h(w)}\\), and \\(h(w) = \\dfrac{\\mathrm{E}[q(w,d)]}{f(d)}\\) So the actual \\(\\dfrac{q(w,d)}{h(w)}\\) is an estimate of \\(f(d)\\) and vice versa And we can estimate \\(f(d)\\) based on a weighted average of each observation Different variance assumption for \\(\\varepsilon\\) \\(\\Rightarrow\\) different parameters (weight) similar to the Chainladder method in table 5.2 Table 5.4: Variance and parameters for various form of \\(\\mathrm{E}[q(w,d) \\mid \\text{Data to }w+d] = f(d)h(w)\\) Method \\(\\mathrm{Var}(\\varepsilon(w,d)) \\propto f(d)^p h(w)^q\\) \\(\\mathbf{f(d)}\\): Col Parameters \\(\\mathbf{h(w)}\\): Row Parameters BF1 \\(p=q=0\\) \\(f(d) = \\dfrac{\\sum_w h^2 \\frac{q}{h}}{\\sum_w h^2}\\) \\(h(w) = \\dfrac{\\sum_d f^2 \\frac{q}{f}}{\\sum_d f^2}\\) Cape Cod2 \\(p=q=0\\) \\(f(d) = \\dfrac{\\sum_w h^2 \\frac{q}{h}}{\\sum_w h^2}\\) \\(h = \\dfrac{\\sum_\\Delta f^2 \\frac{q}{f}}{\\sum_\\Delta f^2}\\) BF (Var \\(\\propto\\) \\(fh\\))3 \\(p=q=1\\) \\(f^2(d) = \\dfrac{\\sum_w h \\left( \\frac{q}{h} \\right) ^2}{\\sum_w h}\\) \\(h^2(w) = \\dfrac{\\sum_d f \\left( \\frac{q}{f} \\right)^2}{\\sum_d f}\\) BF: assumes each \\(q(w,d)\\) have constant variance (least square, standard regression) For \\(h(w)\\) weight is \\(f(d)^2\\) and cells with higher expectes loss get higher weight For \\(f(d)\\) weight is expected ultimate losses squared, years with higher expected losses get higher weight Cape Cod: also assumes constant variance Variance of the cell is proportional to the expected loss of the cell Recall from Mack 1994, we need to look at the residual graphs to determine which variance assumption is appropriate 5.4.3 Iteration Process Need to seed one of them and iterate until convergence e.g. for constant variance BF: \\(f(d)\\) \\(\\sum \\downarrow\\); \\(h(w)\\) \\(\\sum \\rightarrow\\) Use the above to estimate the parameters and then calculate the unpaid When combining parameters, don’t count the \\(f(0)\\) and always subtract 1 Step 1) Start iteration with the \\(f(d)\\) from Chainladder For age greater than 0, these are the \\(\\dfrac{ATA - 1}{ATU}\\) For age 0, subtract the sum of the other factors from unity Step 2) Find \\(h(w)\\)’s that minimize the \\(SSE\\) One regression for each \\(w\\) Depending your variance assumption (Table 5.4), see figures below Step 3) Find \\(f(d)\\)’s that minimize the \\(SSE\\) based on the \\(h(w)\\) from step 2 Step 4) Repeat step 2 &amp; 3 till convergence Step 5) Use the \\(f(d)\\) and \\(h(w)\\) calculated to get the estiamted triangle to calculate \\(SSE\\) or do projection Figure 5.1: BF with constant variance Figure 5.2: BF with variance proportional to \\(fh\\) Figure 5.3: Cape Cod 5.4.4 Counting n and p for SSE For n For reserving: number of cell in the triangle minus the first column \\(\\dfrac{m(m-1)}{2}\\) (Since we don’t need to forecast the first column to calculate the unpaid) For pricing: number of cell in the triangle For p (Just a walk through of how we get the \\(p\\) in Table 5.3) BF: Start with \\(2m\\) parameters all the \\(f(d)\\)’s and \\(h(w)\\)’s We don’t need \\(f(0)\\) for reserves again since we don’t need to forcast the first column Less one more due to degree of freedom (Since you can fix any one of the parameters and still get the same results) So we have \\(p = 2m-2\\) Similarly for Cape Cod but we only start with \\(m + 1\\) so we ended up with \\(m-1\\) (i.e. taking out the \\(f(d)\\) and the degree of freedom) Grouped BF is similar as well if you don’t group the \\(f(0)\\) Note: If you group the \\(f(0)\\) you can’t subtract one for that as you’ll be using the parameter Chainladder you have \\(m\\) for all the \\(f(d)\\)’s to start with and less the \\(f(0)\\) "],
["venter-imp-3.html", "5.5 Implication 3: Linearity", " 5.5 Implication 3: Linearity The forecast incremental losses might not be a linear function (e.g. \\(q(w,d) = a\\sqrt{c(w,d)}\\)) Test for linearity by making sure the residuals are not a sequence of positive then negative and vice versa Do this for each regression (i.e. every \\(d\\)) Look at residuals as function of \\(c(w,d-1)\\) Similar to residual test from mack "],
["venter-imp-4.html", "5.6 Implication 4: Stability", " 5.6 Implication 4: Stability Look at empirical LDFs \\(f(d)\\) down a column Use the entire history if factors are stable Take more recent average if unstable or follow a trend Can compare rolling 5 year averages as well "],
["venter-imp-5.html", "5.7 Implication 5: No Correlation on Columns (Test for independence)", " 5.7 Implication 5: No Correlation on Columns (Test for independence) Calculate Pearson correlation for every pair of columns with at least 3 LDFs This is a test on the LDFs (Mack assumption 1 (prop. 4.1), LDFs are not correlated) Test with only 2 LDFs will either be 1 or -1 Not just adjacent LDF pairs like in Mack-1994 \\(\\therefore\\) For a \\(m \\times m\\) triangle we have \\({7 \\choose 2}\\) pairs (7 because only 9 columns of LDFs and we take out the 2 columns with less than 3 LDFs) Correlation: \\[\\begin{equation} r = \\dfrac{\\sum {\\tilde{x}} {\\tilde{y}} }{\\sqrt{\\sum {\\tilde{x}}^2 \\sum {\\tilde{y}}^2}} \\tag{5.4} \\end{equation}\\] Where \\(\\tilde{x} = x - \\bar{x}\\) and \\(\\tilde{y} = y - \\bar{y}\\) \\(x\\)’s and \\(y\\)’s are incremental LDFs - 1 But actually not necessary since -1 doesn’t affect the correlation Use calculator data table for \\(r\\) once you have \\(\\tilde{x}\\) and \\(\\tilde{y}\\) Test statistics for significance is \\(T \\sim t_{n-2}\\) \\[\\begin{equation} T = r \\sqrt{ \\dfrac{n-2}{1-r^2} } \\tag{5.5} \\end{equation}\\] Look up the t-value from table for 90% \\(n\\) is the number of LDF pairs If \\(|T| &lt;\\) table value \\(\\Rightarrow\\) Not correlated Perform test for all columns We deem the triangle have significant correlations if the number of correlated pairs are more than \\[\\begin{equation} 0.1 x + \\sqrt{x} \\:\\:\\: : \\:\\:\\: x = {m - 3 \\choose 2} \\tag{5.6} \\end{equation}\\] x = # of pair tested for a \\(m \\times m\\) triangle "],
["venter-imp-6.html", "5.8 Implication 6: No High of Low Diagonals (Test for independence)", " 5.8 Implication 6: No High of Low Diagonals (Test for independence) Similar to Mack’s CY Test This is a test on Mack assumption 2 (prop. 4.2), no AY correlation Key Idea: Run regression on the triangle with a dummy variable for each diagonal Each \\(q(w,d)\\) is regressed against the prior cumulative losses + dummy variable for which diagonal it is in \\[\\begin{equation} q(w,d) \\sim c(w,d-1) + dummy_{CY} \\tag{5.7} \\end{equation}\\] If losses are significantly higher or lower in a diagonal \\(\\Rightarrow\\) The coefficient of the dummy variable would be statistically significant (i.e. coefficient is double the \\(\\sigma\\)) Only includes diagonals that forcast at least 2 elements Caveat: Diagonal effect is additive More likely to see multiplicative impact. e.g. from inflation This can be implement with a regression on the logarithm of the losses 5.8.1 Diagonal Trend as Inflation Consider CY trend as inflation and model \\(q(w,d)\\) with a diagonal parameter \\(g(w+d)\\), where \\(w+d\\) is the diagonal \\[\\mathrm{E}[q(w,d)] = f(d)h(w)g(w+d)\\] This will have parameters for each row, column, and diagonal Can be reduce similar to the grouped BF We can model a constant CY trend to reduce the parameters e.g. \\(g(w+d) = (1+j)^{w+d}\\) "],
["past-exam-questions-4.html", "5.9 Past Exam Questions", " 5.9 Past Exam Questions Concepts 2011 #4: Implication 1 2012 #5: Mack assumptions Implication Tests \\(\\star\\) 2014 #4: correlation tests 2015 #5: correlation tests \\(\\star\\) 2016 #6: Correlation test combine with Mack TIA 2: correlation test TIA 3: correlation test TIA 4: statistical significance test on the correlation TIA 5: statistical significance test on the correlation Alternative Emergence \\(\\star\\) TIA 1: BF with constant variance \\(3 \\times 3\\) triangle Can simplify the calculation if one of the parameter is constant \\(\\star\\) TIA 6: BF with variance proportional to \\(fh\\) \\(\\star\\) TIA 7: \\(SSE\\) calc for \\(4 \\times 4\\) triangle Calculate Adj \\(SSE\\), AIC and BIC Pick method that is better \\(\\star\\) TIA 8: Full blown calc with BF \\(\\propto \\: fh\\) \\(\\star\\) TIA 9: Calculating \\(n\\) and \\(p\\) and all the goodness of fit metrics for various methods and pick the best 5.9.1 Question Highlights n/a -->"],
["curve-fit-clark.html", "Chapter 6 LDF Curve-Fitting and Stochastic Reserving: A Maximum Likelihood Approach - D. Clark", " Chapter 6 LDF Curve-Fitting and Stochastic Reserving: A Maximum Likelihood Approach - D. Clark \\(\\star\\) Paper definition/standards: We use the average age of the period here (so minus 6 month) Ratio of variance to mean is constant, \\(\\sigma^2\\), for the whole triangle (6.3) Variance of incremental loss is proportional to the mean of incremental loss and assume this proportion is known and same for the whole triangle Reserve estimate \\(\\star \\star\\) Given \\(G(x)\\) distribution Loglogistic (6.1), or Weibull (6.2) Might have to estimate parameters 2 Reserving method \\(\\star\\) LDF Method \\(\\star\\) Cape Cod Method Typically better because it has additional information on exposure Test for truncation by looking at age twice the triangle size Different truncation method Reserve process variance \\(\\star \\star\\) Need Estimation of \\(\\sigma^2\\) (6.4) Note the \\(n\\) and \\(p\\) for the 2 methods We’re looking at incremental losses Distribution Process variance ODP for incremental loss (6.5) Parameter variance Estimate using MLE 6.1 Estimate for \\(ULT_{AY}\\) based on MLE Estimate for \\(ELR\\) based on MLE 3 key assumptions \\(\\star \\star\\) Test \\(iid\\) assumpation with residual plots Other use of the model Estimate variance for next prospective u/w year Estimate CY development Variability in discounted reserve Counting average accident date "],
["expected-loss-emergence.html", "6.1 Expected Loss Emergence", " 6.1 Expected Loss Emergence Definition 6.1 \\(x =\\) average age of AY (e.g. 6mo for the most recent instead of 12mo) \\(G(x \\mid \\omega, \\theta)\\) = % paid to date growth function 2 forms of the growth function below The curves move smoothly from 0 to 1 Loglogistic: \\[\\begin{equation} G(x \\mid \\omega, \\theta) = \\dfrac{x^{\\omega}}{x^{\\omega} + \\theta^{\\omega}} \\tag{6.1} \\end{equation}\\] Weibull: \\[\\begin{equation} G(x \\mid \\omega, \\theta) = 1- \\mathrm{exp}\\left\\{ { - \\left( \\dfrac{x}{ \\theta } \\right)^{\\omega}} \\right \\} \\tag{6.2} \\end{equation}\\] Remark. % Emergence in period \\(x\\) to \\(x+12\\) = \\(G(x+12) - G(x)\\) Equivalent to the \\(f(d)\\) in Venter Table 5.3 for the alternative pattern \\(f(d)h(w)\\) Given \\(d = x / 12 - 1\\) Advantages (over Venter): Uses only 2 column parameters: \\(\\theta\\) for mean; \\(\\omega\\) for s.d. Can use data @ different age Not the same maturity as prior years Only last few CYs Output is a smooth curve \\(\\Rightarrow\\) Can interpolate between ages and extrapolate a tail Motivation for using a curve is to recognize that adjacent LDFs are related Doesn’t work when there is expected negative loss development 6.1.1 Expected Ultimate Loss Methods Method 1: Cape Cod \\[Premium_{AY} \\times ELR\\] Remark. A single row parameter \\(h\\) for the entire triangle \\(h\\) here is the \\(ELR\\) Similar parameters in Venter (5.3) This method is preferred: Only need 3 parameters Includes extra information, exposure base Estimate Future Emergence: \\[[G(y \\mid \\omega, \\theta) - G(x \\mid \\omega, \\theta)] \\times [Premium_{AY} \\times ELR]\\] Method 2: LDF \\[ULT_{AY}\\] Remark. \\(h(w)\\) for each row (i.e. parameter for each AY) \\(h(w)\\) here represents the ultimate loss for each \\(w\\) Similar parameters in Venter (5.3) Estimate Future Emergence: \\[[G(y \\mid \\omega, \\theta) - G(x \\mid \\omega, \\theta)] \\times ULT_{AY}\\] "],
["distribution-of-actual-loss-emergence-and-maximum-likelihood.html", "6.2 Distribution of Actual Loss Emergence and Maximum Likelihood", " 6.2 Distribution of Actual Loss Emergence and Maximum Likelihood Estimate parameters with MLE 6.2.1 Process Variance Key assumption: \\[\\begin{equation} \\mathrm{Var}(c_i) \\propto \\mathrm{E}[c_i] = \\sigma^2 \\mathrm{E}[c_i] \\tag{6.3} \\end{equation}\\] Assume ratio of the variance to mean is constant for each cell in the triangle \\(c_i\\) is the incremental loss here \\(\\sigma^2\\) is the same for the entire triangle Remark. Compare variance assumption with Mack and Venter Mack-1994: Proposition 4.3: \\(\\mathrm{Var}\\left (c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ) = \\alpha_k^2 \\: c_{i,k}\\) Constant is same for each column \\(k\\) (development period) n = includes all here? Venter Factors: 2 version of BF method 5.4 Variance is either constant or varied by \\(\\propto f(d)h(w)\\) n = predicted? Estimate \\(\\mathbf{\\sigma^2}\\) based on the entire triangle: \\[\\begin{equation} \\dfrac{Variance}{Mean} = \\sigma^2 = \\dfrac{1}{n-p}\\sum\\limits_{i \\in \\Delta}^n\\dfrac{(c_i - \\mu_i)^2}{\\mu_i} \\tag{6.4} \\end{equation}\\] \\(n =\\) # of data points in triangle \\(p =\\) # of parameters Cape Cod: \\(p=3\\) (\\(\\omega, \\theta, ELR\\)) LDF: \\(p=2 +\\) # of AYs (\\(\\omega, \\theta,\\) row parameters) \\(c_i =\\) actual incremental loss emergence \\(\\mu_i =\\) expected incremental loss emergence \\(\\sigma^2\\) for \\(LDF\\) will tend to be higher due to more parameters used This calculation is similar to Shapland’s dispersion factor Assume incremental loss follows ODP Poisson \\[\\begin{equation} C_i \\sim ODP(\\lambda_i, \\sigma^2) \\tag{6.5} \\end{equation}\\] Use ODP so that variance \\(\\neq\\) mean \\(C_i = \\sigma^2 X_i\\) where \\(X_i \\sim Poi(\\lambda_i)\\) \\(\\mathrm{E}[C_i] = \\sigma^2 \\lambda_i = \\mu_i\\) \\(\\mathrm{Var}(C_i) = \\sigma^2 \\mu_i\\) Here \\(C_i\\) means the r.v. while \\(c_i\\) is the observation Caveat: Potential issue with ODP is that some granularity is lost since reserves are estimated in multiple of \\(\\sigma^2\\) However \\(\\sigma^2\\) is generally small so little precision is lost 6.2.2 MLE for Best Parameters Not super testable Given a set of observed incremental losses \\(\\{c_i\\}\\), we want to find the \\(\\omega\\), \\(\\theta\\), and \\(ELR\\) that best fit the actual losses Proposition 6.1 Maximum likelihood function \\[l = \\sum \\limits_{i \\in \\Delta} c_i \\mathrm{ln}(\\mu_i) - \\mu_i\\] Maximize each \\(l\\) across each parameters \\(\\omega\\), \\(\\theta\\) and \\(ELR\\) by taking the derivative of \\(l\\) w.r.t. each of the parameters and setting the equation to zero Proof. For each cell \\(i\\) we have incremental losses \\(C_i\\) with mean: \\[\\mu_i = \\sigma^2 \\lambda_i\\] The likelihood function is: \\[\\prod \\limits_{i \\in \\Delta} \\Pr(C_i = c_i) = \\prod \\limits_{i \\in \\Delta} \\dfrac{\\lambda_i ^{c_i / \\sigma^2} e^{-\\lambda_i}}{(c_i / \\sigma^2)!} = \\prod \\limits_{i \\in \\Delta} \\dfrac{(\\mu_1 / \\sigma^2) ^{c_i / \\sigma^2} e^{-\\lambda_i}}{(c_i / \\sigma^2)!}\\] Take the log of the above \\[\\sum_{i \\in \\Delta} \\dfrac{c_i}{\\sigma^2} \\ln \\left( \\dfrac{\\mu_i}{\\sigma^2} \\right) - \\dfrac{\\mu_i}{\\sigma^2} - \\ln \\left[ \\left(\\dfrac{c_i}{\\sigma^2} \\right)! \\right]\\] If we assume the \\(\\sigma^2\\) is known and constant then we have function in proposition 6.1 We can remove \\(\\sigma^2\\) in the first 2 terms as it is a constant The 3rd term goes away as the whole thing is now a constant 6.2.2.1 MLE for Method 1 When maximize the MLE for \\(ELR\\) we get: \\[ELR = \\dfrac{\\sum_{i \\in \\Delta} c_i}{\\sum_{i \\in \\Delta} P_i \\times [G(y) - G(x)]}\\] This is the sum of all incremental losses in the triangle \\(\\div\\) Premium \\(\\times\\) Expected portion of claims paid Which is the Cape Cod \\(ELR\\) from Hurlimann 6.2.2.2 MLE for Method 2 When we maximize the MLE for \\(ULT_{AY}\\) we get: \\[ULT_{AY} = \\dfrac{\\sum_{i \\in AY} c_i}{\\sum_{i \\in AY}[G(y) - G(x)]}\\] Sum of claims reported to date \\(\\div\\) % expected reported to date in a row The is the LDF method of estimating ultimate 6.2.3 Parameter Variance Not super testable Information matrix \\(I\\): (2nd derivative matrix \\(l\\) vs each parameter) \\[\\begin{equation} \\begin{bmatrix} \\dfrac{\\partial^2 l}{\\partial^2 ELR} &amp; \\dfrac{\\partial^2 l}{\\partial ELR \\: \\partial \\omega} &amp; \\dfrac{\\partial^2 l}{\\partial ELR \\: \\partial \\theta}\\\\ \\dfrac{\\partial^2 l}{\\partial \\omega \\: \\partial ELR} &amp; \\dfrac{\\partial^2 l}{\\partial^2 \\omega} &amp; \\dfrac{\\partial^2 l}{\\partial \\omega \\: \\partial \\theta}\\\\ \\dfrac{\\partial^2 l}{\\partial \\theta \\: \\partial ELR} &amp; \\dfrac{\\partial^2 l}{\\partial \\theta \\: \\partial \\omega} &amp; \\dfrac{\\partial^2 l}{\\partial^2 \\theta}\\\\ \\end{bmatrix} \\tag{6.6} \\end{equation}\\] Covariance matrix: \\[\\begin{equation} \\mathbf{\\Sigma} = -\\sigma^2 \\times I^{-1} \\tag{6.7} \\end{equation}\\] \\(3 \\times 3\\) matrix for Cape Cod \\((n+2) \\times (n+2)\\) for LDF Method 6.2.4 Variance of the Reserves Process Variance of \\(R\\) \\[\\sigma^2 \\sum_i \\mu_i\\] Technically testable if given \\(\\sigma^2\\) Process variance \\(\\propto\\) to the mean with proportion \\(\\sigma^2\\) Parameter Variance of \\(R\\) \\[\\mathrm{Var}(\\mathrm{E}[R]) = (\\partial R)&#39;\\mathbf{\\Sigma} (\\partial R)\\] \\(\\mathbf{\\Sigma}\\) is from equation (6.7) above \\(\\partial R\\) = vector that is the derivative of the reserve by each parameter Calculation heavy, not testable "],
["clark-key-ass.html", "6.3 3 Key Assumptions of Model", " 6.3 3 Key Assumptions of Model Incremental losses \\(iid\\) Test this using residual analysis \\(\\frac{Variance}{Mean}\\) scale parameter \\(\\sigma^2\\) is fixed and known Technically this should be estimated with the other parameters but will makes things intractable Variance estimates are based on the approximation to the Rao-Cramer lower bound Variance based on information matrix \\(I\\) (6.6) \\(I\\) is exact only when using linear functions In our case this is simply a lower bound We are using approximated parameters The above temper the volatility in the model, actual results can be more variable Model only works for positive expected incremental losses, but a negative loss here or there is fine as well "],
["clark-ldf-method.html", "6.4 LDF Method (Method 2)", " 6.4 LDF Method (Method 2) Can use either Loglogistic (6.1) or Weibull (6.2) for \\(G(x)\\) Estimate \\(\\theta\\), \\(\\omega\\), and \\(ULT_{AY}\\) with MLE Recall \\(ULT_{AY}\\) estimate from prior section Then estimate \\(\\mu\\) for the whole triangle \\[\\mu = ULT_{AY} \\times [G(y) - G(x)]\\] Calculate the log likelihood for the sum of the whole triangle Calculate \\(\\sigma^2\\) for residual or reserve process variance \\[\\sigma^2 = \\dfrac{1}{n-p}\\sum\\limits_{i \\in \\Delta} \\underbrace{\\dfrac{(c_i - \\mu_i)^2}{\\mu_i}}_{\\chi^2 \\text{ term}}\\] n = all data points (not just the predicted like in Venter) p = 2 + 10 AYs This is for incremental losses 6.4.1 Residual Review Normalized residuals: \\[r_i = \\dfrac{c_i - \\mu_i}{\\sigma \\sqrt{\\mu_i}}\\] Divide by the square root of the variance \\(\\sigma^2 \\mu\\) for ODP Same as \\(\\dfrac{\\sqrt{\\chi^2 \\text{ term}}}{\\sigma}\\) Plot residuals (should be randomly scattered around 0): age \\(x\\) vs \\(r_{i,x}\\) expected loss \\(\\mu_i\\) vs \\(r_{i,x}\\) Test of the constant \\(\\frac{Variance}{Mean}\\) assumption If fail, can try alternative variance assumptions (e.g. \\(\\mathrm{Var} \\propto \\mu^2\\)) AY, CY, etc vs \\(r_{i,x}\\) 6.4.2 Reserve Estimate Untruncated Get \\(G(x)\\): % paid(reported) to date Ultimate = Paid to date (reported to date) \\(\\div\\) \\(G(x)\\) Truncated @ age \\(x_t\\) To cut of the tail at some point and stop the development Remember we’re \\(x\\) is in mid year (so year times 12 and minus 6 months) Use \\(G&#39;(x) = \\dfrac{G(x)}{G(x_t)}\\) instead just like above Process Variance: \\[\\begin{equation} \\sigma^2 \\sum_i \\mu_i = \\sigma^2 \\times \\text{Unpaid} \\tag{6.8} \\end{equation}\\] Parameter variance is huge and computational intensive as it requires inverting a big matrix We expect the Cape Cod to have parameter variance (as we only estimate 3 parameters) Total variance can be calculated by summing the process and parameter variance "],
["clark-cape-cod-method.html", "6.5 Cape Cod Method (Method 1)", " 6.5 Cape Cod Method (Method 1) Requires exposure base: e.g. on-level and trended EP, original loss projection, trended # of vehicles, claim counts, etc We want an exposure base that allow us to assume a constant ELR across AYs Estimate \\(\\theta\\), \\(\\omega\\) and \\(ELR\\) with MLE Recall \\(ELR\\) estimate from prior section \\[ELR = \\sum_{AY} \\dfrac{\\text{Losses Paid to Date}}{\\underbrace{G(x)}_{\\text{Expected portion paid}} \\times Premium}\\] Do not truncate when calculating \\(ELR\\) Calculate \\(\\mu\\) \\[\\mu = Premium_{AY} \\times ELR \\times [G(y) - G(x)]\\] Calculate the log likelihood for the sum of the whole triangle Calculate \\(\\sigma^2\\) for residual or reserve process variance \\[\\sigma^2= \\dfrac{1}{n-p}\\sum\\limits_{i \\in \\Delta}^n\\dfrac{(c_i - \\mu_i)^2}{\\mu_i}\\] \\(n =\\) all data points (Not just the predicted like in Venter) \\(p = 3\\) Check if the assumption of one expected LR is reasonable by looking for any upward or downward trends in the ultimate LR Since we’re assuming a single \\(ELR\\) 6.5.1 Reserve Estimate Untruncated Reserve = On-level Premium \\(\\times \\: ELR \\: \\times [1 - G(x)]\\) Truncated @ age \\(x_t\\) Reserve = On-level Premium \\(\\times \\: ELR \\: \\times [(G(x_t) - G(x)]\\) Or, Reserve = On-level Premium \\(\\times \\: ELR \\: \\times G(x_t) \\: \\times \\:[(1 - G&#39;(x)]\\) Similar story for the process variance and parameter variance as the LDF method The Covariance matrix \\(\\Sigma\\) is smaller just \\(3 \\times 3\\) Parameter variance is smaller than the LDF method since we have more information (exposure) in the Cape Cod method "],
["other-use-of-model.html", "6.6 Other Use of Model", " 6.6 Other Use of Model 6.6.1 Variance of Prospective Losses Estimate variance for the next u/w year Need to be given the \\(\\mathrm{Var}(ELR)\\) Process Variance: \\[\\sigma^2 \\times \\: ELR \\: \\times Premium\\] Parameter Variance: \\[\\left(\\sqrt{\\mathrm{Var}(ELR)}\\times Premium \\right)^2\\] Total Variance: Process Variance + Parameter Variance CoV: \\[\\dfrac{\\sqrt{Total \\: Variance}}{ELR \\times Premium}\\] 6.6.2 Calendar Year Development Estimated paid losses over the next 12 months: \\[[G(x+12) - G(x)] \\times a \\: priori \\:Ultimate\\] No truncation here a priori from the LDF method here Sum for all AYs and compare with actual calendar year emergence Can calculate the s.d. to see if it’s in range (process var is still \\(\\sigma^2 \\times\\) estimate) 6.6.3 Variability in the Discounted Reserves Similar to the above, but CoV will be smaller since the tail with the most variability gets discounted the most "],
["comments-and-conclusion.html", "6.7 Comments and Conclusion", " 6.7 Comments and Conclusion Can use data in table format (AY, From and To Age, Incremental Paid, Exposure) Can we Use the CoV from the model even if select a different reserve? No: since you don’t trust the reserve estimate Yes: since the s.d. is a selection and the CoV from this model is a reasonable basis Curve was selected because: The move smoothly from 0 to 1 Closely match empirical data 1st and 2nd derivative are calculable Others can be used as well MLE is useful for estimating both the expected emergence pattern and the variance ODP is convenient link to LDF and Cape Cod estimates "],
["avg-age-claim.html", "6.8 Average Age of Claims", " 6.8 Average Age of Claims From paper appendix, to be included later…(Not crucial) Not complete \\(t\\) is the number of months since inception Accident year \\[x = \\begin{cases} \\dfrac{t}{2} &amp;: \\: t &lt; 12 \\\\ t - 6 &amp;: \\: t \\geq 12 \\\\ \\end{cases}\\] Accident Quarter \\[x = \\begin{cases} \\dfrac{t}{2} &amp;: \\: t &lt; 3 \\\\ t - 1.5 &amp;: \\: t \\geq 3 \\\\ \\end{cases}\\] Policy year \\[x = \\begin{cases} \\dfrac{t}{3} &amp;: \\: t \\leq 12 \\\\ t - 12 &amp;: \\: t \\geq 24 \\\\ \\end{cases}\\] The in between 12 and 24 is some complicated formula… "],
["past-exam-questions-5.html", "6.9 Past Exam Questions", " 6.9 Past Exam Questions Reserve/variance Calculation \\(\\star \\star\\) 2011 #2 (6.1): Cape Cod truncated and reserve CoV 2012 #2: Cape Cod method and reserve CoV \\(\\star \\star\\) 2013 #3 (6.2): LDF method, need to calc \\(\\sigma^2\\) and normalized residuals 2014 #3: \\(\\sigma^2\\) calc 2014 #5: plug and play LDF and Cape Cod method with Benktander \\(\\star\\) 2015 #2 (6.3): LDF Method and concept where if we switch to Cape Cod the CoV should go down as it incorporates additional information \\(\\star\\) 2016 #3: stdv for cape cod and LDF and why the \\(\\sigma^2\\) is higher for LDF TIA 1: Reserve with truncated loglogistic and given parameters Cape Cod LDF method TIA 2: Weibull Cape Cod no truncation \\(\\star\\) Process standard deviation Variance of incremental loss TIA 3: Weibull LDF method reserve Loss reserve total standard deviation given parameter variance Note that we can not model triangle with negative development Other calculation TIA 4: Loglogistic with LDF method \\(\\star\\)Log likelihood to test which parameters are better Probably too long for exam, but good for checking my understanding Total reserve \\(\\star\\) TIA 7: given all parameters for cape cod; everything is mostly given but good to see how the calculation works CY paid Variance for the next CY paid Prospective AY expected loss and variance TIA 8: Setup data for clark’s calculation TIA 9: loglikelihood for cape cod compare (similar to TIA 4) \\(\\star\\) TIA 10: process variance, long calc for \\(\\sigma\\), LDF method and Weibull \\(\\star\\) TIA 11: count average dates for quarterly TIA 12: count average date for policy year TIA 13: count average date for policy year with stuff in between 12 and 24 Concepts \\(\\star \\star\\) 2016 #4b: appropriatness of using Cape Cod TIA 5: Assumptions made that lead to the cape cod and LDF method List the formula for the 2 methods TIA 6: Advantages of Clark’s growth function method 6.9.1 Question Highlights Figure 6.1: 2011 Question 2 Figure 6.1: 2011 Question 2 Figure 6.2: 2013 Question 3 Figure 6.2: 2013 Question 3 Figure 6.3: 2015 Question 2 Figure 6.3: 2015 Question 2 -->"],
["a-model-for-reserving-workers-compensation-high-deductibles-j-siewert.html", "Chapter 7 A Model for Reserving Workers Compensation High Deductibles - J. Siewert", " Chapter 7 A Model for Reserving Workers Compensation High Deductibles - J. Siewert 6 methods and their pros and cons: Loss Ratio Method Per occ (7.1) and aggregate (7.2) formula Implied Development Direct Development See the 2 methods, (7.3) and (7.4), for calculating the \\(^{XS}LDF^L_t\\) (7.4) is basically from method 5 proposition 7.3 Credibility Weight Method Formula (7.5) weighting 1 and 3 Development Method Severity needs to be trended Claim counts are developed separately ground up \\(\\star \\star\\) Relativities 7.1, severity LDF formulas, know them well to manipulate and know what formula requires what Proposition 7.1 and 7.2 are simlar one for limited one for XS Might need to break out the \\(LDF_t\\) into it’s components Proposition 7.3 combines the above Note the 3 proposition above works with LDF if we sub out the ultimate relativities Proposition 7.4 and 7.5 are for incremental LDFs Distribution Method Use Weibull for \\(R_t^L\\) Finally there’s the method for accounting for aggregate charge with collective risk model or table M "],
["introduction.html", "7.1 Introduction", " 7.1 Introduction WC high deductible reserving with occurrence and/or aggregate deductible Reserving for layers XS of a per occ limit and/or aggregate limit The rest of this section is not important for exam but just background information on high deductible WC program High deductible WC became popular in the 90’s and actuarial efforts focused on pricing issues Program was developed to provide both insurer and insured many advantages: Achieving pricing flexibility while passing additional risk to larger insureds in what was considered at the time an unprofitable LoB Ameliorating onerous residual market charges and premium taxes in some states Realizing cash flow advantages similar to paid loss retro providing insureds with another vehicle to control losses while protecting them against random large losses Allowing self-insurance without submitting insureds to sometimes demanding state requirements After the program matures, the focus shifts to issues on the liability side How to estimate these liabilities when losses are not expected to emerge above deductible limits for many years How to construct development factors in the absence of long-term histories under a deductible program How to determine development patterns that reflect the diversity of deductible size and mix How to determine consistent development factors between limited and XS values What is a reasonable approach for the indexing of deductible limits over time How to estimate the liability associated with aggregate loss limits Is there a sound way to determine the proper asset value for associated service revenue Similar to loss conversion factor in retro rating, loss multipliers are applied to deductible losses to capture expenses that vary with loss "],
["siewert-lr-method.html", "7.2 Method 1) Loss Ratio Method", " 7.2 Method 1) Loss Ratio Method Expected Ultimate lossXS Per Occ limit \\[\\begin{equation} P \\cdot E \\cdot \\chi \\tag{7.1} \\end{equation}\\] \\(P\\) = Premium \\(E\\) = Expected ground up loss ratio \\(\\chi\\) = Occurrence charge = % of losses above deductible Expected Ultimate Loss XS Aggregate Limits \\[\\begin{equation} P \\cdot E \\cdot (1-\\chi) \\cdot \\varphi \\tag{7.2} \\end{equation}\\] \\(\\varphi\\) = Aggregate charge = % of losses in deductible layer that exceed the aggregate Remark. \\(\\chi\\) and \\(\\varphi\\) are from industry tables Specific to the deductible, aggregate, and size of expected losses E.g. From NCCI Table M These are ultimate loss estimate Advantages Useful when little data is available Ties to pricing Can include industry experience Disadvantages Ignores actual emergence May not properly reflect account characteristics "],
["siewert-implied-dev.html", "7.3 Method 2) Implied Development", " 7.3 Method 2) Implied Development Ultimate excess loss = ultimate unlimited loss - ultimate limited loss Need to make sure LDFs are consistent with the different layers (especially for the tail) \\(LDF^L \\leq LDF\\) Don’t develop limited losses at a layer above limited losses at a higher layer Need to adjust the deductible at different exposure year for inflation when selecting limited LDFs Keeps the proportion of deductible/XS losses consistent over time Otherwise historical losses take too long to hit the deductible limit and distorts the LDF Advantages Get estimate for early period with no losses Limited factors are more stable Disadvantages Does not directly estimate the XS loss "],
["siewert-direct-dev.html", "7.4 Method 3) Direct Development", " 7.4 Method 3) Direct Development Directly applying XS LDF to XS loss to date Different ways to get \\(^{XS}LDF^L_t\\): Formula 1: Given unlimited &amp; limited LDFs and \\(\\chi\\) \\[\\begin{equation} ^{XS}LDF_t^{L} = \\dfrac{^{XS}Ult}{^{XS}Loss_t} = \\dfrac{Ult \\cdot \\chi}{\\frac{Ult}{LDF_t} - \\frac{Ult\\cdot(1-\\chi)}{LDF_t^L}} = \\dfrac{\\chi}{\\frac{1}{LDF_t} - \\frac{(1-\\chi)}{LDF_t^L}} \\tag{7.3} \\end{equation}\\] Based on \\(\\underbrace{Loss_t \\cdot LDF_t}_{100\\%} = \\underbrace{Loss^L_t \\cdot LDF^L_t}_{100\\% - \\chi} + \\underbrace{^{XS}Loss_t^L \\cdot {^{XS}LDF}^L_t}_{\\chi}\\) Can’t use the actual losses to date for this as it’ll just be equal to the implied method mathematically Formula 2: Given unlimited &amp; limited LDFs and \\(R^L_t\\) \\[\\begin{equation} ^{XS}LDF^L_t = \\dfrac{LDF_t - R^L_t \\times LDF^L_t}{1-R^L_t} \\tag{7.4} \\end{equation}\\] Based on Question 21 from 2004 (7.1) \\(R^L_t = \\dfrac{Sev^L}{Sev}\\), or \\(R^L_t = \\dfrac{Ult^L \\div LDF^L}{Ult \\div LDF} = \\dfrac{\\text{Expected Limited Reported}}{\\text{Expected Unlimited Reported}}\\) Disadvantages LDFs large and volatile, not recommend this method "],
["siewert-cred-weight-model.html", "7.5 Method 4) Credibility Weighting Techniques / Bornhuetter-Ferguson", " 7.5 Method 4) Credibility Weighting Techniques / Bornhuetter-Ferguson Credibility weighting between Method 1 and Method 3 \\[\\begin{equation} L = Z \\times ({^{XS}Loss}_t^L \\cdot {^{XS}LDF}_t^{L}) + (1 - Z) \\times E \\tag{7.5} \\end{equation}\\] If \\(Z = \\frac{1}{^{XS}LDF_t^L}\\) then we have the BF method \\(E\\) is from method 1 "],
["siewert-dev-method.html", "7.6 Method 5) Development Method", " 7.6 Method 5) Development Method Method overview: Select trend to adjust historical layers Develop ground up claim count Calculate severity \\(^{XS}LDF^L_t\\) based on \\(R^L_t\\) Data Required Ground up WC claims level data (Allows individual losses to ber capped at different amounts) Includes indemnity, medical and ALAE together Can split the data by account, injury, and state as a future step 7.6.1 Severity Trend Need to account for loss trend when capping losses Apply different limits to the historical data by AYs (or else there will be more and more losses piercing the limiting layer \\(\\therefore\\) Distorting the LDFs) Adjusting for loss trend Select a trend by fitting an exponential curve to the average severity of unlimited losses by AY (e.g. \\(y = ce^{bx}\\)) Optional: Use different trend for different years Optional: Adjust for large losses Cap the historical losses at lower amount to compensate for the loss trend by detrending the historical layer 7.6.2 Claim Count Development Split claim count development from severity development For claim count development we focusing on ground up claims Advantages: Most claims are reported not too deep in the tail even if the full severity in not yet known If you only count claims once they pierce the deductible layer you have to deal with uncertain claim count development deep into the tail If your claim counts depend on the limits, every time you update the severity trend assumptions the claim counts will change 7.6.3 Severity Development Fit LDFs for each layer to the power curve Power curve needs a cutoff Using the same cut off for all layers might introduce a bias as lower layers will cease development earlier Different layer LDFs need to respect the relationships lay out below (Proposition 7.1, 7.2 and 7.3) 7.6.3.1 Relativities Definition 7.1 Relativity \\[R_t^L = \\dfrac{\\text{Limited Sev @ }t}{\\text{Unlimited Sev @ t}} = \\dfrac{S^L_t}{S_t}\\] \\[R^L = \\dfrac{\\text{Limited Sev @ Ult}}{\\text{Unlimited Sev @ Ult}} = \\dfrac{S^L}{S}\\] Remark. Relativity (for different limits) over time Relativity starts close to 1.00, and drops over time before reaching the ultimate relativity The losses limited at higher limits start with a very high relativity (close to 1.00), and take longer to come down Here they define \\(LDF_t = \\dfrac{S}{S_t}\\) (So severity LDFs, not sure what’s the implication) Proposition 7.1 \\[LDF_t^L = LDF_t \\dfrac{R^L}{R_t^L}\\] Proof. \\(LDF_t^L = \\dfrac{\\text{Ultimate Loss}}{\\text{Losses @ t}} = \\dfrac{C \\cdot S^L}{C_t \\cdot S^L_t} = \\dfrac{C \\cdot S \\cdot R^L}{C_t \\cdot S_t \\cdot R^L_t} = \\underbrace{\\dfrac{C \\cdot S}{C_t \\cdot S_t }}_{LDF_t} \\times \\dfrac{R^L}{R^L_t} = LDF_t \\dfrac{R^L}{R_t^L}\\) Remark. \\(C\\) is claim count and \\(S\\) is severity Proposition 7.2 \\[^{XS}LDF_t^L = LDF_t \\dfrac{(1-R^L)}{(1-R_t^L)}\\] Remark. Analogous relationship for XS LDFs Proposition 7.3 \\[LDF_t = R^L_t \\cdot LDF^L_t + (1 - R^L_t) \\cdot {^{XS}LDF}^L_t\\] Remark. This follows from Proposition 7.2 and 7.3 This formula doesn’t require \\(R^L\\) Formula only work once claim reporting is finished Relativity based on average from historical losses All the above can be adjusted to work with incremental LDFs if we don’t use the ultimate relativities 7.6.3.2 Incremental LDFs and Change in Relativities Proposition 7.4 \\[\\dfrac{_{inc}LDF^L_t}{_{inc}LDF_t} = \\Delta R^L_t = \\dfrac{R^L_{t+1}}{R^L_t}\\] Remark. Difference of limited and unlimited incremental LDFs is driven by the change in relativity With \\(R_t\\) and \\(R_{t+1}\\) we can get the limited or unlimited LDF given one or the other Proposition 7.5 \\[\\dfrac{_{inc}^{XS}LDF^L_t}{_{inc}LDF_t} = \\Delta (1 - R^L_t) = \\dfrac{1 - R^L_{t+1}}{1 - R^L_t}\\] "],
["siewert-dist-model.html", "7.7 Method 6) Distribution Model", " 7.7 Method 6) Distribution Model Use a modeled severity distribution (e.g. Weibull) for each age Parameters vary @ different ages; make sure they are consistent e.g. Parameters for Weibull, \\((\\theta, \\omega)_t\\), are different depending on age \\(R^L_t = \\dfrac{\\mu^L_t}{\\mu_t}\\) Mean of Weibull: \\(\\mu^L_t = \\theta \\cdot \\Gamma \\left(1 + \\frac{1}{\\omega}\\right)\\) Advantages Maintain the relationships of the limited and unlimited over time Can easily interpolate among limits and years Then we can use the distribution and then applies all the relationships discussed in Method 5 "],
["siewert-agg.html", "7.8 Aggregate Limits", " 7.8 Aggregate Limits Use collective risk model to model individual losses Poisson (frequency) and Weibull (severity) Build out a table with XS loss for each deductible and aggregate Need a different model for each age? Need 4 inputs: Expected unlimited Age Deductible Aggregate limit Reserve (use BF): Expected aggregate loss \\(\\times\\) % unreported Caveat: doesn’t take into account if the aggregate is about to be pieced of not 7.8.1 Table M Appendix discuss calculation with Table M From paper appendix, to be included later…(Not crucial) Need adjusted losses to determine ELG \\[\\text{Adj Losses} = \\underbrace{\\dfrac{1 + 0.8 \\cdot \\chi}{1 - \\chi}}_{\\text{Adj Factor}} \\times \\mathrm{E}[\\text{Unlimited Loss}]\\] Entry ratio: \\[\\dfrac{\\text{Agg Limit}}{\\text{Expected Losses in Layer}}\\] Use entry ratio to lookup insurance charge and apply to the expected loss in layer to get the expected loss XS the aggregate "],
["past-exam-questions-6.html", "7.9 Past Exam Questions", " 7.9 Past Exam Questions Concepts 1998 - #41: Discuss methods and pros/cons 2000 - #4: Relativities relationship 2001 - #11: Development method process 2002 - #28 b: Pros/cons implied development 2005 - #19 b c: Inflation index limit, implied dev advantages 2006 - #6: LR approach advantage 2008 - #12 b: LR method pros and cons 2011 - #6 c: BF pros (more stable and can be tied to pricing estimates) Simple Plug and Play Calculations 2001 - #23: Loss ratio method for aggregate loss charge 2002 - #28 a c: XS deductible IBNR reserve and service revenue 2003 - #8: Loss ratio method for losses XS aggregate limit \\(\\star\\) 2004 - #21 (7.1): IBNR using LR, implied, direct, BF 2005 - #19 a: Implied LDF method 2008 - #12 a: LR method both occ and aggregate 2010 - #3: Back out limited LDF \\(\\star\\) 2010 - #9 (7.3): XS limited LDF based on relativity 2012 - #6: Get unlimited with limited and XS and relativity with the formula \\(\\star\\) 2014 - #6 (7.4): Use incremental formula and also the combined formula 2015 - #6 a b: LR and implied dev method TIA 1 &amp; 2: Plug and play with relativities TIA 3: plug and play with incremental rel Full Calcualation \\(\\star\\) 2007 - #41 (7.2): XS and limited implied LDF calc from ground up with aggregate limit 2009 - #7: implied and direct development from triangle of unlimited and XS 2011 - #6 a b: Direct development and BF (slightly erroneous question) \\(\\star \\star\\) 2015 - #6 c (7.5): Calculate layer between \\(\\star\\) 2016 - #9: Use industry and company XS development then the pros and cons of the 2 methods \\(\\star\\) TIA 4: reserve for policy with deductible and agg limit Lots of arithemtics \\(\\star\\) TIA 5: XS dev and Table M 7.9.1 Question Highlights Figure 7.1: 2004 Question 21 Figure 7.1: 2004 Question 21 Figure 7.2: 2007 Question 41 Figure 7.2: 2007 Question 41 Figure 7.3: 2010 Question 9 Figure 7.3: 2010 Question 9 Figure 7.4: 2014 Question 6 Figure 7.4: 2014 Question 6 Figure 7.5: 2015 Question 6 Figure 7.5: 2015 Question 6 -->"],
["claims-development-by-layer-r-sahasrabuddhe.html", "Chapter 8 Claims Development by Layer - R. Sahasrabuddhe", " Chapter 8 Claims Development by Layer - R. Sahasrabuddhe \\(\\star \\star\\) Setup base triangle Set AY CY trend triangle Unlimited mean loss table with (8.1) detrended LEV triangle @ \\(L\\) and LEV for the last row @ \\(B\\) with (8.2) Use (8.3) to get base layer \\(\\star \\star\\) Convert the base LDFs to any layer Convert base layer LDFs (8.4) Convert to LDFs for XS Layers: (8.5) Practical adjustments Formula when we don’t have severity distribution for every age (8.6) and how to estimate (8.7) Know the assumptions used: Claim size model "],
["introduction-1.html", "8.1 Introduction", " 8.1 Introduction 2 Key Parts: Convert a triangle to one level of trend and layer of losses This is then used to determine LDFs at this base layer Convert LDFs at a Base layer to LDFs at any other layer Claim size models (distribution for individual claims) Exponential (in this paper) Different distribution for each column in the triangle Requires a distribution of losses at each age, which can be difficult (Last section try to address this) Definition 8.1 Sahasrabuddhe notations \\(B\\): Base layer, LDFs are determined at this layer \\(X\\): Layer of interest, ultimately we want LDFs for this layer \\(\\Phi_{ij}\\): Cumulative loss distn in row \\(i\\) age \\(j\\) \\(LEV(X; \\Phi_{ij})\\): Limited Expected Value, average loss with distn \\(\\Phi_{ij}\\) capped at \\(X\\) \\(F_{ij}^X\\): LDF to ultimate for cell \\(ij\\) with losses capped at \\(X\\) \\(C^L_{ij}\\): Cumulative paid to date for AY \\(i\\), age \\(j\\) in layer \\(L\\) 8.1.1 Claim Size Model Distribution of the individual losses (this underpins calculations we do below) Properties of \\(\\Phi_{ij}\\) Have parameters to adjust for inflation (e.g. Lognormal, Weibull, Exponential, Pareto) Assumes that the only difference between the distribution of cells in the same column is trend (\\(T_{ij}\\)) \\(\\therefore\\) We have \\(\\Phi_{ij} \\sim f(\\Phi_{nj},T_{ij},T_{nj})\\) i.e. Distribution of \\(C_{ij}\\) only depends on the things above This allows us to choose one distribution for each column Specifically the distribution is selected for row n in each column and we use the trend factors to select the distribution for any cell in the same column Mean (and limited means) can be calculated with reasonable effort Definition 8.2 (Increased Limits Factors) \\[S_{ij}(L_a, L_b) = \\dfrac{\\text{Expected Losses for Layer }L_a}{\\text{Expected Losses for Layer }L_b} = \\dfrac{\\mathrm{E}\\left[ C^{L_a}_{ij} \\right]}{\\mathrm{E}\\left[ C^{L_b}_{ij} \\right]} = \\dfrac{LEV(L_a; \\Phi_{ij})}{LEV(L_b; \\Phi_{ij})}\\] Both for cell \\(C_{ij}\\) \\(L_a\\): Layer \\(a\\), deductible \\(d_a\\) and policy limit \\(p_a\\) \\(L_b\\): Layer \\(b\\), deductible \\(d_b\\) and policy limit \\(p_b\\) "],
["base-tri.html", "8.2 Part 1) Setup Base Layer Triangle", " 8.2 Part 1) Setup Base Layer Triangle Setup a consistent base layer triangle for LDFs selection We only need to go up to calculating LEV if we already have the base LDFs \\(F^B_{nj}\\) 8.2.1 Setup the trend triangle Total Trend = AY Trend \\(\\times\\) CY Trend No adjustment needed if only AY trend is present Since AY trend increases losses proportionally down each row CY trend should be on incremental losses These are ground up trend, which is consistent if taken from external sources Trend to the last row of the triangle Don’t stop just at the diagonal We typically starts trending from the top left corner (but it doesn’t have to) i.e. Have the 1.000 at the top left corner Get the AY and CY trend triangle separately and the multiply them together Caveat: The paper applies the trend to cumulative loss which is problematic Cumulative trend depends on the incremental trend and the emergence pattern Output: Trend triangle 8.2.2 Calculate unlimited mean We need the unlimited mean for each cell in triangle (Avg sev paid to date) Based on mean for the latest AY (last row \\(n\\)) \\[\\begin{equation} \\begin{array}{c} C_{nj} \\sim \\Phi_{nj} = Exp(\\theta_j) \\\\ \\mathrm{E}[C_{nj}] = \\theta_j \\\\ \\tag{8.1} \\end{array} \\end{equation}\\] Detrend the mean back up from the bottom row to fill the whole square Practically, we usually just need to calculate this for 4 cells for the LDF conversion formula (8.4) (if we have the LDF already) Output: Unlimited mean loss table adjusted for trend 8.2.3 Calculate LEV We need the LEV for each cell in: For calculating \\(B\\) triangle: Triangle @ \\(L\\) and last row @ \\(B\\) For converting LDFs: Diagonal and ultimate column @ \\(L\\) and last row @ \\(B\\) \\[\\begin{equation} LEV(X; \\Phi \\sim Exp(\\theta)) = \\theta \\: \\left[ 1 - \\operatorname{exp}\\left\\{-\\left(\\dfrac{x}{\\theta}\\right)\\right\\} \\right] \\tag{8.2} \\end{equation}\\] Remark. Use the table of means (\\(\\theta\\)’s) calculated from Step 2 Similarly, we only need 4 points for the calculates in part 2 if we don’t need the LDFs This gives us everything for equation (8.4) if we already have the Base level LDF Output: \\(LEV\\) for each cell in triangle for layer \\(L\\) and the last row for layer \\(B\\) 8.2.4 Calculate the Base Layer Triangle and LDF Convert triangle of actual losses by dividing it’s LEV at layer \\(L\\) then times it’s LEV at layer \\(B\\) \\[\\begin{equation} C_{ij}&#39; = C_{ij}^L \\times \\underbrace{\\dfrac{LEV(B;\\Phi_{nj})}{LEV(L;\\Phi_{ij})}}_{\\text{ILF w/ on-level to }nj} \\tag{8.3} \\end{equation}\\] Remark. See definition 8.2 on the ILF component This takes losses from layer \\(L\\) at trend level \\(ij\\) to layer \\(B\\) at trend level \\(nj\\) Once we have the triangle we can select the base layer LDFs \\(F^B_{nj}\\) "],
["convert-ldf.html", "8.3 Part 2) Convert LDFs from Base Layer", " 8.3 Part 2) Convert LDFs from Base Layer Convert LDFs from base layer to LDFs at any other layer At this step we should already have \\(F_{nj}^B\\) selected from the base triangle created above \\[\\begin{equation} F_{ij}^X = F_{nj}^B \\times \\frac{LEV(X;\\Phi_{i\\infty})\\div LEV(B;\\Phi_{n\\infty})}{LEV(X;\\Phi_{ij}) \\div LEV(B;\\Phi_{nj})} \\tag{8.4} \\end{equation}\\] Without trend, you can basically just take the middle part as the LDFs e.g. \\(F^B_{nj} = \\dfrac{LEV(B; \\Phi_{n \\infty})}{LEV(B; \\Phi_{nj})}\\) then it all just cancels out However, we trust the LDF from the analysis of losses limited to \\(B\\) more and less so with the distribution functions \\(\\Phi\\) \\(\\therefore\\) We take the LDF from the limited analysis and scaling from layer \\(B\\) to \\(X\\) Figure 8.1: Converting LDFs from base layer to any other layer 8.3.1 LDFs for Layers from Basic Limits to Policy Limits \\[\\begin{equation} F_{ij} = F_{nj}^B \\times \\frac{ \\left[{\\color{blue}{LEV(Y;\\Phi_{i\\infty}) - }} LEV(X;\\Phi_{i\\infty})\\right]\\div LEV(B;\\Phi_{n\\infty})}{ \\left[ {\\color{blue}{LEV(Y;\\Phi_{ij}) - }} LEV(X;\\Phi_{ij}) \\right] \\div LEV(B;\\Phi_{nj})} \\tag{8.5} \\end{equation}\\] \\(Y\\) here is the policy limits and \\(X\\) is the basic limits "],
["other-prac-use-sah.html", "8.4 Other Practical Uses", " 8.4 Other Practical Uses If we don’t have the severity distribution by age (as required by the denominator of equation (8.4)), we can work with the severity at ultimate and estimate \\(R_j(X,B)\\), for lower layers \\(X &lt; B\\) \\[\\begin{equation} F_{ij}^X = F_{nj}^B \\times \\frac{LEV(X;\\Phi_{i\\infty})\\div LEV(B;\\Phi_{ {\\color{red}i} \\infty})}{R_j{(X,B)}} \\tag{8.6} \\end{equation}\\] Note the \\(i\\) in the second part of the numerator (I guess it’s to be consistent with formula below) See how this change pictorially (Compare to fig. 8.1) Ratio of limited losses at layer \\(X\\) to layer \\(B\\) at age \\(j\\) \\[\\begin{equation} R_{j}(X,B) = \\dfrac{LEV(X;\\Phi_{ij})}{LEV(B;\\Phi_{ij})} \\tag{8.7} \\end{equation}\\] The ratio is calculated on the diagonal This needs to be estimated Equation (8.7) assume: \\[\\dfrac{LEV(B; \\Phi_{n \\infty})}{LEV(B; \\Phi_{nj})} \\approx \\dfrac{LEV(B; \\Phi_{i \\infty})}{LEV(B; \\Phi_{ij})}\\] Assumes the ratio of losses at different cost layers is immaterial The expected LDF in different AYs (row \\(n\\) and \\(i\\)) are similar when losses are capped at \\(B\\) This is a reasonable assumption with low inflation Estimate \\(R_j(X,B)\\) (8.7) \\(R_j(X,B)\\) is bound by the following: \\[U_i \\leq R_j(X,B) \\leq U_i \\cdot F_{ij}^{\\infty} \\leq 1\\] Where: \\[U_i = R_{i\\infty}(X,B) = \\dfrac{LEV(X;\\Phi_{i\\infty})}{LEV(B;\\Phi_{i \\infty})}\\] Remark. For a given \\(j\\), \\(R_{ij}(X,B)\\) decreases as we move across the age \\(i\\) since more losses pierce through the layer We expect \\(U_i\\) to \\(\\downarrow\\) for more recent AYs due to loss trend; Larger % of losses pierce the lower layer Larger losses are capped in the numerator but not as much in the denominator \\(U_i\\) increases as we move to more mature years (??) \\(U_i\\) is the ratio at ultimate of limited means; same as \\(R_{i \\infty}\\) Empirical example from text: Select a decay from 1.000 and approaches \\(U_i\\) as maturity increase Overlay with empirical \\(R_j(X,B)\\) along the diagonal Overlay with the \\(U_i\\) ultimate ratios as well to serve as a floor Estimate doesn’t work when: There is expected negative development XS layer develops more quickly than a working layer Upper bound: Get upper bound for \\(R_j\\) by using unlimited losses for \\(B\\) and the loss development factors from unlimited losses \\(B\\infty\\) "],
["issues-with-assumptions.html", "8.5 Issues with Assumptions", " 8.5 Issues with Assumptions Three assumptions that are likely reasonable Must select a basic limit \\(B\\) Needs an ultimate claim size model Or use ILFs Just have to accurate around the basic limit and policy limit since that’s where we calculate expectations Need to create a triangle of losses at basic limit and one cost level We do this in the step converting the base triangle Two assumptions that are more tenuous Requires robust claim size model at each age Ultimately we need the ratio of limited losses at different layers, less import to get the absolute values correct Trend Trend should be applied to the incremental losses Not clear how to apply trend to reported losses (by day of reserve or payment?) "],
["past-exam-questions-7.html", "8.6 Past Exam Questions", " 8.6 Past Exam Questions Full calculation 2013 #5: set up base triangle diagonal with trend Can start trend from bottom left cell \\(\\star\\) TIA 1: full blown calc of ultimate given base LDFs and latest \\(\\theta\\) TIA 2: Convert LDF to layer \\(\\star\\) TIA 3: Convert to base triangle TIA 4: Get base layer triangle and LDFs \\(\\star\\) TIA 5: Use the practical formula Concepts \\(\\star\\) 2016 #8: 8.6.1 Question Highlights n/a -->"],
["using-the-odp-bootstrap-model-a-practitioners-guide-shapland.html", "Chapter 9 Using the ODP Bootstrap Model: A Practitioner’s Guide - Shapland", " Chapter 9 Using the ODP Bootstrap Model: A Practitioner’s Guide - Shapland Notations are similar to Venter See Table 9.1 for sample triangle layout Model Parameters: Mean (9.1) Variance (9.3) and dispersion factor Need residual: unscaled (9.8), scaled (9.9), standardized (9.10) Parameterize with GLM or simplified (ODP) (Note the condition required for ODP) \\(\\star\\) Simulation procedure Parameter variance (9.14) Process variance (9.15) Other variation of the bootstrap (e.g. incurred, BF, Cape Cod) Pros and Cons of GLM GLM vs ODP bootstrap \\(\\star\\) Practical issues: \\(\\star\\) Negative Incremental Values During fitting: (9.16) and (9.17) During simulation: (9.21) Non‐Zero Sum of Residuals Using an L-year Weighted Average Missing Values Outliers \\(\\star\\) Heteroscedasticity Stratified sampling Hetero adjustment to residuals Non-constant scale parameter Heteroecthesious Data Exposure Adjustment Tail Factors Fitting a Distribution to ODP Bootstrap Residuals 5 diagnostics \\(\\star\\) Residual graphs Normality test Outliers Parameter adjustment \\(\\star\\) Model results Multiple models Model testing "],
["introduction-2.html", "9.1 Introduction", " 9.1 Introduction Paper focus on over-dispersed Poisson (ODP) bootstrap Incremental losses are modeled as ODP random variable Goal is to generate a distribution of possible outcomes Just FYI, not important for exam Other papers on bootstrap Statistics: Bradley Efron (1979) Actuarial: England &amp; Verrall (1999; 2002), Pinheiro, et al. (2003), Kirschner, et al. (2008) Practical motivation for modeling loss distribution Definition of actuarial estimate in ASOP 43 can be based on a first moment from a distribution While ASOP 36 (SAO) focus on deterministic point estimates SEC is looking for more information on reserving risk in the 10-K Rating agencies are building dynamic risk models and welcome actuary input Companies that use dynamic risk models for internal risk management need unpaid claim distributions SII and IFAS are moving towards unpaid claim distribution Advantages of bootstrap Generates a distribution of the estimate of unpaid claims Can be tailored to statistical features of our data Reflects that loss distn are usually skewed to the right Disadvantages of bootstrap Takes more time to create, but okay once set up 9.1.1 Stochastic vs Static Model ODP bootstrap is a specific form of GLM Benefit of GLM: It can be specifically tailored to the statistical features found in the data Contrast with algorithms that force the data to be fit to a static model (fig. 9.1) Figure 9.1: Stochastic vs Static Model Diagram Just FYI, not important for exam Some authors define a model as having a defined structure and error distribution, so under this more restrictive definition bootstrapping would be considered to be a method or algorithm However, using a less restrictive definition of a model as an algorithm that produces a distribution, bootstrapping would be defined as a model "],
["shapland-notations.html", "9.2 Notations", " 9.2 Notations Definition 9.1 Same notations as Venter for \\(n \\times n\\) triangle \\(w\\): Accident (exposure) year \\(d\\): Development age \\(q(w,d)\\): incremental loss for AY \\(w\\) from age \\(d-1\\) to \\(d\\) \\(c(w,d)\\): cumulative loss \\(F(d)\\): Incremental LDF from age \\(d\\) to \\(d+1\\) \\(f(d)\\): \\(F(d) - 1\\), for forcasting incremental losses \\(G(w)\\): Factor relating to accdient (exposure) year \\(w\\); ultimate gross level \\(h(k)\\): Factor relating to the diagonal \\(k\\) along which \\(w+d\\) is constant Table 9.1: Incremental triangle with corresponding row and column parameters - \\(\\beta_2\\) \\(\\beta_3\\) \\(\\alpha_1\\) \\(q(1,1)\\) \\(q(1,2)\\) \\(q(1,3)\\) \\(\\alpha_2\\) \\(q(2,1)\\) \\(q(2,2)\\) \\(\\alpha_3\\) \\(q(3,1)\\) Chainladder assumptions: LDFs are the same for each row \\(F(w,d) = F(d)\\) Each AY has a parameter representing it’s level e.g. CL project based on level of losses to date "],
["bootstrap-model.html", "9.3 Bootstrap Model", " 9.3 Bootstrap Model Benefits of the bootstrap model: Allows us to estimate the distribution with very little data We don’t have to make any assumptions about the underlying distribution (non-parametric) The ODP part is the error distribution ODP bootstrap models: Incremental claims directly as the response With the same linear predictor as Kremer (1982) Using a GLM with log-link function and an ODP Poisson error Where a specific form of this model is identical to the volume weighted chain ladder Using bootstrap (sampling residuals with replacement) to estimate the distribution of point estimates (Instead of simulating from a multivariate normal for a GLM) 9.3.1 GLM Parameters Mean and variance for each \\(q(w,d)\\) in the triangle (per table 9.1) 9.3.1.1 Mean and log-mean for \\(q(w,d)\\) \\[\\begin{equation} \\mathrm{E}[q(w,d)] = m_{wd} = \\exp \\left [\\alpha_w + \\sum_{i=2}^d \\beta_i \\right] \\:\\: : \\: \\: w \\in [2, n] \\tag{9.1} \\end{equation}\\] \\[\\begin{equation} \\ln \\left( \\mathrm{E}[q(w,d)] \\right) = \\ln(m_{w,d}) = \\eta_{w,d} = \\alpha_w + \\sum_{i=2}^d \\beta_i \\:\\: : \\: \\: w \\in [2, n] \\tag{9.2} \\end{equation}\\] Remark. \\(\\alpha\\)’s are the individual level parameters \\(\\beta\\)’s adjust for the development trends after the first development period We don’t use \\(\\beta_1\\) which effectively means \\(\\beta_1 = 0\\) \\(\\alpha_i\\) and \\(\\beta_j\\) are selected to minimize error between \\(\\ln(actual) - \\ln(forecast)\\) Equivalence for using Venter notation: \\(h(w) = e^{\\alpha}\\) \\(f(d) = e^{\\sum \\beta}\\) 9.3.1.2 Variance for \\(q(w,d)\\) \\[\\begin{equation} \\mathrm{Var}[q(w,d)] = \\phi m_{wd}^z \\tag{9.3} \\end{equation}\\] \\(\\phi\\): Dispersion factor Scale factor estimated as part of the fitting procedure while setting the variance proportional to the mean Estimated from the residuals \\(z\\): Error distribution Paper focus on \\(z = 1\\) for Over Dispersed Poisson (ODP) Specifies the whole mean-variance relationship (not only the first 2 moments) Table 9.2: Distribution with corresponding \\(z\\) \\(z\\) Distribution 0 Normal 1 Poisson 2 Gamma 3 Inverse Gaussian 9.3.2 Fitted Triangle We can fit the \\(\\alpha\\)’s and \\(\\beta\\)’s defined above using the GLM framework, or the simplified GLM method 9.3.2.1 Parameterize with GLM Framework Start with a \\(3 \\times 3\\) incremental triangle Table 9.3: \\(3\\times 3\\) incremental triangle: w/d 1 2 3 1 \\(q(1,1)\\) \\(q(1,2)\\) \\(q(1,3)\\) 2 \\(q(2,1)\\) \\(q(2,2)\\) 3 \\(q(3,1)\\) Log transform of the triangle Table 9.4: \\(3\\times 3\\) log incremental triangle: w/d 1 2 3 1 \\(\\ln[q(1,1)]\\) \\(\\ln[q(1,2)]\\) \\(\\ln[q(1,3)]\\) 2 \\(\\ln[q(2,1)]\\) \\(\\ln[q(2,2)]\\) 3 \\(\\ln[q(3,1)]\\) Create a system of equations based on equation (9.2) \\[\\begin{equation} \\begin{split} \\ln[q(1,1)] &amp;= 1\\alpha_1 + 0\\alpha_2 + 0\\alpha_3 + 0\\beta_2 + 0\\beta_3 \\\\ \\ln[q(2,1)] &amp;= 0\\alpha_1 + 1\\alpha_2 + 0\\alpha_3 + 0\\beta_2 + 0\\beta_3 \\\\ \\ln[q(3,1)] &amp;= 0\\alpha_1 + 0\\alpha_2 + 1\\alpha_3 + 0\\beta_2 + 0\\beta_3 \\\\ \\ln[q(1,2)] &amp;= 1\\alpha_1 + 0\\alpha_2 + 0\\alpha_3 + 1\\beta_2 + 0\\beta_3 \\\\ \\ln[q(2,2)] &amp;= 0\\alpha_1 + 1\\alpha_2 + 0\\alpha_3 + 1\\beta_2 + 0\\beta_3 \\\\ \\ln[q(1,3)] &amp;= 0\\alpha_1 + 0\\alpha_2 + 1\\alpha_3 + 1\\beta_2 + 1\\beta_3 \\\\ \\end{split} \\tag{9.4} \\end{equation}\\] Express the above in matrix form \\[\\begin{equation} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\alpha_2 \\:\\:\\: \\alpha_3 \\:\\:\\: \\beta_2 \\:\\:\\: \\beta_3 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - &amp; - &amp; - \\\\ - &amp; 1 &amp; - &amp; - &amp; - \\\\ - &amp; - &amp; 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 &amp; - \\\\ - &amp; 1 &amp; - &amp; 1 &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 &amp; 1 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\end{bmatrix} \\end{array} \\tag{9.5} \\end{equation}\\] Remark. \\(\\mathbf{X}\\) is the design matrix that defines the parameters used to estimate the losses in each cell Use iteratively weighted least squares or MLE1 to solve for the parameters in the in \\(\\mathbf{A}\\) that minimize the squared difference between \\(\\mathbf{Y}\\) and \\(\\mathbf{S}\\), the solution matrix \\[\\begin{equation} \\mathbf{S} = \\begin{bmatrix} ln[m_{1,1}] \\\\ ln[m_{2,1}] \\\\ ln[m_{3,1}] \\\\ ln[m_{2,1}] \\\\ ln[m_{2,2}] \\\\ ln[m_{1,3}] \\\\ \\end{bmatrix} \\tag{9.6} \\end{equation}\\] After solving the system of equations we will have: \\[\\begin{equation} \\begin{split} \\ln[m_{1,1}] &amp;= \\eta_{1,1} &amp;= \\alpha_1 \\\\ \\ln[m_{2,1}] &amp;= \\eta_{2,1} &amp;= \\alpha_2 \\\\ \\ln[m_{3,1}] &amp;= \\eta_{3,1} &amp;= \\alpha_3 \\\\ \\ln[m_{1,2}] &amp;= \\eta_{1,2} &amp;= \\alpha_1 + \\beta_2\\\\ \\ln[m_{2,2}] &amp;= \\eta_{2,2} &amp;= \\alpha_2 + \\beta_2\\\\ \\ln[m_{1,3}] &amp;= \\eta_{1,3} &amp;= \\alpha_1 + \\beta_2 + \\beta_3\\\\ \\end{split} \\tag{9.7} \\end{equation}\\] The above solution shown as a triangle below Table 9.5: \\(3\\times 3\\) GLM fitted log incremental triangle: w/d 1 2 3 1 \\(\\ln[m_{1,1}]\\) \\(\\ln[m_{1,2}]\\) \\(\\ln[m_{1,3}]\\) 2 \\(\\ln[m_{2,1}]\\) \\(\\ln[m_{2,2}]\\) 3 \\(\\ln[m_{3,1}]\\) Exponentiate the triangle above to get our fitted (or expected) incremental results of the GLM model Table 9.6: \\(3\\times 3\\) GLM fitted incremental triangle: w/d 1 2 3 1 \\(m_{1,1}\\) \\(m_{1,2}\\) \\(m_{1,3}\\) 2 \\(m_{2,1}\\) \\(m_{2,2}\\) 3 \\(m_{3,1}\\) 9.3.2.2 Simplified GLM GLM model = Chainladder w/ volume-weighted averages when: Variance \\(\\propto\\) Mean \\(\\varepsilon(w,d) \\sim\\) Poisson A parameter for each row and column (except 1st column) Benefits: Replace GLM fitting with much simpler calculation LDFs are easier to explain Still works even when there are negative incremental values Procedure for fitting incremental triangle: Select LDFs based on vol-wtd Start from the last cumulative diagonal and divide backwards by each incremental LDFs to get the cumulative fitted triangle Subtracting out the cumulative diagonals to get your incremental fitted triangle 9.3.3 Residuals Unscaled Pearson residuals \\[\\begin{equation} \\begin{split} r_{w,d} &amp; = &amp; \\dfrac{A - E}{\\sqrt{\\mathrm{Var}(E)}} &amp;\\\\ &amp; = &amp; \\dfrac{q(w,d) - m_{wd}}{\\sqrt{m^z_{wd}}} &amp;\\\\ &amp; = &amp; \\dfrac{q(w,d) - m_{wd}}{\\sqrt{m_{wd}}} &amp; \\:\\:\\:\\: \\text{Recall }z = 1\\text{ for ODP Poisson}\\\\ \\end{split} \\tag{9.8} \\end{equation}\\] Mean and variance as defined above Residual for the right and bottom corners of the triangle are going to be 0 Because a unique parameter is used for those 2 cells Alternatively we can use Anscombe residual We prefer Pearson because its calculation is consistent with the scale parameter \\(\\phi\\) Scaled Pearson residuals (England &amp; Verrall) \\[\\begin{equation} r^S_{w,d} = r_{w,d} \\times \\underbrace{\\sqrt{\\dfrac{N}{N-p}}}_{f^{DoF}} \\tag{9.9} \\end{equation}\\] Degrees of freedom adjustment, to effectively allow for over dispersion of the residuals in the sampling process and add process variance to approximate a distribution of possible outcomes Increase the variability of the pseudo triangle Standardized residuals (Pinheiro et al.) \\[\\begin{equation} r^H_{w,d} = r_{w,d} \\times \\underbrace{\\sqrt{\\dfrac{1}{1-H_{i,i}}}}_{f^H_{w,d}} \\tag{9.10} \\end{equation}\\] \\[\\begin{equation} \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{WX})^{-1}\\mathbf{X}^T\\mathbf{W} \\tag{9.11} \\end{equation}\\] \\[\\begin{equation} \\mathbf{W} = \\begin{bmatrix} m_{1,1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; m_{2,1} &amp; 0 &amp; 0 \\\\ \\vdots &amp; 0 &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; \\cdots &amp; m_{1,n}\\\\ \\end{bmatrix} \\tag{9.12} \\end{equation}\\] Hat matrix adjustment factor \\(f^H_{w,d}\\) is based on the diagonal on the hat matrix \\(\\mathbf{H}\\) (Going down the column of the triangle from left to right) \\(\\mathbf{W}\\) is a \\(2n \\times 2n\\) matrix \\(\\mathbf{X}\\) is the design matrix from (9.5) Benefits: \\(f^H_{w,d}\\) account for the exclusion of zero-value residuals Or the zero-value residuals will have some variance but we just don’t know what it is yet so we should sample from the remaining residuals but not the zeros \\(f^H_{w,d}\\) is an improvement on \\(f^{DoF}\\) 9.3.4 Dispersion Factor Dispersion factor \\[\\begin{equation} \\phi = \\dfrac{\\sum r_{wd}^2}{N-p} \\tag{9.13} \\end{equation}\\] \\[N = \\dfrac{n (n+1)}{2}\\] \\[p = 2n-1\\] \\(N =\\) # of data points (including first column unlike Ventor) \\(N\\) can be less than indicated above if the tail incremental developments are all 0’s \\(p =\\) # of parameters One for each row, one for each column minus first column \\(p\\) can be less than \\(2n-1\\) if the later incremental values are all 0’s and therefore not needed for fitting This calculation is similar to Clark’s \\(\\sigma^2\\) (6.4) Alternate method for \\(\\phi\\) \\[\\phi \\sim \\phi^H = \\dfrac{\\sum (r^H_{w,d})^2}{N}\\] We can still use the same dispersion factor even with the scaled and standardized residuals, this just give us another method to estimate \\(\\phi\\) You can also use other methods such as orthogonal decomposition or Newton-Raphson to solve for the parameters↩ "],
["odp-boot-sim.html", "9.4 Bootstrap Simulation Procedure", " 9.4 Bootstrap Simulation Procedure Bootstrap simulation procedure, repeat steps 1 - 5 at least 10,000 times Model our losses, determine mean and residual for each cell This can be based on GLM Framework of simplified GLM Create a sampled \\(triangle^*\\) from the residuals and the means Sample with replacement on the Pearson residuals (9.8) from our original triangle from Step 0. (Since data needs to be \\(iid\\) for bootstrap) (Note the distribution of the residual will be purely empirical) Simulated loss: \\[\\begin{equation} q^*(w,d) = m_{wd} + r_p^* \\sqrt{m_{wd}^z} \\tag{9.14} \\end{equation}\\] (\\(r^*_p\\) is the realized sample from i.) Estimate dispersion factor \\(\\phi\\) for Step 3 as this is based on the original triangle Determine parameters from \\(triangle^*\\): For GLM Framework, calculate the \\(\\alpha_w\\)’s, \\(\\beta_d\\)’s For Simplified GLM calculate the weighted average LDFs and Ultimate Loss Calculate mean and variance2 for the future cells (unpaid): \\((m^*_{wd}, \\phi m^*_{wd})\\) Mean: \\(m^*_{wd}\\) GLM Framework: \\(m^*_{wd} = \\mathrm{exp} \\left [\\alpha_w + \\sum \\limits_{i=2}^d \\beta_i \\right]\\) Simplified GLM Back out the \\(c^*(w,d)\\) by \\(\\dfrac{Ult_w}{CDF_d}\\) then get the \\(m^*_{wd}\\) (Need to back out from ultimate because we need to complete the cells for unpaid too) Variance: \\(\\phi m_{wd}^{*}\\) (for ODP \\(z=1\\) on the \\(m^{*z}_{wd}\\)) Add process variance: draw losses3 from the following Gamma distribution for the future cells (unpaid): \\[\\begin{equation} Gamma(m_{wd}^*,\\phi m_{wd}^*) \\tag{9.15} \\end{equation}\\] Simulate loss from the gamma distribution for each future cells Use \\(u \\sim U[0,1]\\) and \\(F^{-1}_{gamma}(u)\\) Calculate simulated unpaid: sum the bottom half of triangle Step 3 and 3 were added in England &amp; Verrall (2002), in their 1999 paper it doesn’t have this step (stopped at 2) and instead just suggest you to add process variance by multiply the results by \\(f^{RoF}\\)↩ Poisson distribution can be used to remain more consistent with the underlying theory of GLM framework, but it is considerably slower to simulate, so gamma is a close substitute that performs much faster in simulation although it can be more skewed than the Poisson. Indeed other distributions could be used as well to better approximate the observed “skewness” of the residuals from the diagnostics↩ "],
["bootstrap-variation-shap.html", "9.5 Variations on the ODP Bootstrap", " 9.5 Variations on the ODP Bootstrap Reason to use paid data: For insurance risk it is best to focus on the claim payment stream: It measures the variability of the actual cash flows that directly affect the bottom line Case reserves temper the volatility Changes in case reserves and IBNR reserves will also impact the bottom line, but to a considerable extent the changes in IBNR are intended to counter the impact of the changes in case reserves To some degree, then, the total reserve movements can act to mask the underlying changes due to cash flows Reason to include case reserves: Case reserves contain valuable information about potential future payments 9.5.1 Bootstrapping the Incurred Loss Triangle 2 approaches to model the unpaid loss distribution using incurred loss triangle Method 1: Modeling the incurred data and convert the ultimate values to a payment pattern Run the paid and incurred data model in parallel For each iteration and each AY individually: Use the payment pattern (from paid model) to convert the ultimate values (from incurred model) to a payment stream Method 1 Advantages: We improve the ultimate estimates by incorporating the case reserves while still focusing on the payment stream for measuring risk Which effectively allows a distribution of IBNR to become a distribution of IBNR and case reserves Can make it more sophisticated by correlating some part of the paid and incurred models (e.g. the residual sampling and/or process variance portions) So that if we have large payment @ an older age, the incurred should be large as well Method 2: Applying the ODP bootstrap to the Munich chain ladder model See Liu and Verrall (2010) Method 2 Advantages: Don’t have to model the paid loss twice Explicitly measuring and imposing a framework around the correlation of the paid and outstanding losses 9.5.2 Bootstrapping the BF and Cape Cod Method ODP issue: Distribution for the most recent AYs can produce results with more variance than you would expect when compared to earlier AYs in the actual data Due More LDFs are used to extrapolate the sampled values for the most recent accident years and the random samples of incremental values Similar to the leverage effect of the deterministic chainladder Solution: Incorporate BF or Cape Cod Have the a-priori be stochastic e.g. draw the BF a-priori from a distribution or apply Cape Cod to each simulated triangle More complicated approach is to modify the underlying assumptions of the GLM framework which would results in a completely different set of residuals (this is beyond scope) "],
["GLM-bootstrap.html", "9.6 GLM Bootstrap Model", " 9.6 GLM Bootstrap Model Limitations of ODP Bootstrap carry over from chainladder Does not adjust for CY trend May over fit the data from using too many parameters We can solve the above by going back to the GLM framework instead of using the Simplified GLM when we’re at Step 2 of the simulation GLM benefits Not forced to use a specific number of parameters (e.g. GLM Variation 1 and 2) Allows for CY trend Can work with shapes that are non triangles (e.g. data with only the last \\(x\\) diagonals) We can forecast past the end of the triangle (e.g. have the \\(\\beta\\)’s continue the decay) Also see section on practical issues GLM drawbacks Solving GLM at each iteration can slow down the process Model not explainable using development factors Below subsections are just examples of the variations discussed above 9.6.1 GLM Variation 1: Reduce Row Parameters Use only 1 AY parameter \\(\\alpha_1\\) Similar to Venter Cap Cod method 5.3 with \\(h(w) = h\\) Moves away from the Chainladder assumption that each AY has its own level Also note that the residual in cell (3,1) will no longer be zero since it shares the \\(\\alpha_1\\) with all the other rows Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\beta_2 \\:\\:\\: \\beta_3 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; - \\\\ 1 &amp; 1 &amp; - \\\\ 1 &amp; 1 &amp; - \\\\ 1 &amp; 1 &amp; 1 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] 9.6.2 GLM Variation 2: Reduce Column Parameters Use only 1 development year parameter \\(\\beta_2\\) This just assumes the losses decay by \\(e^{\\beta_2}\\) for all ages Also note that the residual in cell (1,3) will no longer be zero since it shares the \\(\\beta_2\\) with all the other columns Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\alpha_2 \\:\\:\\: \\alpha_3 \\:\\:\\: \\beta_2 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - &amp; - \\\\ - &amp; 1 &amp; - &amp; - \\\\ - &amp; - &amp; 1 &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 \\\\ - &amp; 1 &amp; - &amp; 1 \\\\ 1 &amp; - &amp; - &amp; 2 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\beta_2 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] 9.6.3 GLM Variation 3: Reduce Row and Column Parameters Further reduce the parameters to just 1 row and 1 column parameter \\(\\alpha_1\\) and \\(\\beta_2\\) Flexibility of the GLM Bootstrap so that we’re not always stuck with \\(p = 2n-1\\) as stated earlier This will gives us 6 residuals to sample from (the corners will not longer be 0’s) Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) See diagnostics section on how to determine which parameters are statistically significant \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\beta_2 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - \\\\ 1 &amp; - \\\\ 1 &amp; - \\\\ 1 &amp; 1 \\\\ 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\beta_2 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] 9.6.4 GLM Variation 4: Calendar Year Parameter We can also add calendar year trend \\(\\gamma_k\\) where \\(k\\) is the CY \\(\\gamma_2\\) is the 2nd diagonal and etc \\(\\gamma_k\\)’s are incremental decay similar to the \\(\\beta_d\\)’s \\(\\therefore\\) The total impact on the 3rd diagonal is \\(e^{\\gamma_2 + \\gamma_3}\\) Note that the model here have 7 parameters and 6 values \\(\\therefore\\) It has no unique solution Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\alpha_2 \\:\\:\\: \\alpha_3 \\:\\:\\: \\beta_2 \\:\\:\\: \\beta_3 \\:\\:\\: \\gamma_2 \\:\\:\\: \\gamma_3 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -\\\\ - &amp; 1 &amp; - &amp; - &amp; - &amp; 1 &amp; -\\\\ - &amp; - &amp; 1 &amp; - &amp; - &amp; 1 &amp; 1\\\\ 1 &amp; - &amp; - &amp; 1 &amp; - &amp; 1 &amp; -\\\\ - &amp; 1 &amp; - &amp; 1 &amp; - &amp; 1 &amp; 1\\\\ 1 &amp; - &amp; - &amp; 1 &amp; 1 &amp; 1 &amp; 1\\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\gamma_2 \\\\ \\gamma_3 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] 9.6.5 GLM Variation 5: One Parameter for Each Dimension Again we can simplify things by having only 1 parameters for each dimension: \\(\\alpha_1\\), \\(\\beta_2\\), and \\(\\gamma_2\\) Use this as a starting point then add or remove parameters as needed Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\beta_2 \\:\\:\\: \\gamma_2 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; 1 \\\\ 1 &amp; - &amp; 2 \\\\ 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 2 \\\\ 1 &amp; 2 &amp; 2 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\beta_2 \\\\ \\gamma_2 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] "],
["odp-vs-glm-shap.html", "9.7 ODP vs GLM Bootstrap Summary", " 9.7 ODP vs GLM Bootstrap Summary ODP Bootstrap is a specific case of the GLM model with the following parameters: Parameters for every AY Parameters for every development year minus the first Variance of the incremental losses \\(\\propto\\) mean Solution is the same as volume weighted Chainladder GLM Bootstrap is the general model Can have as few as 1 row and/or column parameters Can include CY trend Variance of the incremental losses \\(\\propto\\) \\(m^z_{wd}\\) (where \\(z=1\\) for this paper) "],
["odp-prac-issues.html", "9.8 Practical Issues", " 9.8 Practical Issues Practical issues we might run into with ODP bootstrap Negative Incremental Values Non‐Zero Sum of Residuals Using an L-year Weighted Average Missing Values Outliers Heteroscedasticity Heteroecthesious Data Exposure Adjustment Tail Factors Fitting a Distribution to ODP Bootstrap Residuals 9.8.1 Negative Incremental Values GLM doesn’t work with negative incremental values because of \\(\\ln[q(w,d)]\\) Need to work around this in: Model fitting (e.g. Step 0 and 1 of the Bootstrap process) Simulating for process variance with negative means (e.g. Step 4 of the Bootstrap process) Also additional work around on extreme outcomes from negative values 9.8.1.1 Model Fitting Method 1: Use \\(-ln(abs\\{q(w,d)\\})\\) \\[\\begin{equation} Cell_{w,d} = \\begin{cases} \\ln[q(w,d)] &amp; \\text{if } q(w,d) &gt; 0 \\\\ 0 &amp; \\text{if } q(w,d) = 0 \\\\ -\\ln[abs \\{ q(w,d) \\}] &amp; \\text{if } q(w,d) &lt; 0 \\\\ \\end{cases} \\tag{9.16} \\end{equation}\\] Remark. Doesn’t work when the column sum to a negative value This is done when setting the design matrix (9.5) Method 2: Subtract a negative constant \\(\\Psi\\) \\[\\begin{equation} q^+(w,d) = q(w,d) - \\Psi \\\\ \\ln[q^+(w,d)] \\text{ for all } Cell_{w,d} \\tag{9.17} \\end{equation}\\] Pick \\(\\Psi =\\) largest negative value in the column Apply (9.17) before solving the GLM system of equations (e.g. (9.2) and (9.4)) Then adjust the fitted values by adding back \\(\\Phi\\) to reduce each fitted incremental value \\[\\begin{equation} m_{w,d} = m^+_{w,d} + \\Psi \\tag{9.18} \\end{equation}\\] Can use this method combined with method 1 to take care of the extra large negative ones Need to make use the absolute value for the residual and re-sampling formula, modify (9.8) and (9.14) with below: \\[\\begin{equation} r_{wd} = \\dfrac{q(w,d)-m_{wd}}{\\sqrt{abs\\{m^z_{wd}\\}}} \\tag{9.19} \\end{equation}\\] \\[\\begin{equation} q^*(w,d) = m_{wd} + r^*_p \\sqrt{abs\\{m^z_{wd}\\}} \\tag{9.20} \\end{equation}\\] Method 3: Use simplified GLM Use ODP bootstrap (i.e. Chainladder with volume weighted average LDFs) This will yield different estimate than using the GLM framework with adjustment 1 or 2 9.8.1.2 Simulating Negative Values From above, we might have the fitted \\(m_{wd}\\) that are negative, which will be an issue when used in Step 4 of the bootstrap simulation, when we need to model the process variance with \\(Gamma(m_{wd},\\phi m_{wd})\\) Since \\(Gamma\\) only takes positive parameters Adjustment to the Gamma Distribution with negative \\(m_{wd}\\) \\[\\begin{equation} Gamma(abs\\{m_{wd}\\}, \\phi abs\\{m_{wd}\\}) + 2m_{wd} \\tag{9.21} \\end{equation}\\] This will maintain the right skew of Gamma while having the mean of \\(m_{wd}\\) Alternatively if we use \\(-Gamma(abs\\{m_{wd}\\}, \\phi abs\\{m_{wd}\\})\\) it’ll flip the curve to skew left 9.8.1.3 Extreme Outcomes from Negative Values Column with negative mean in the early ages can results in vary large LDFs (and lead to simulated outcomes that are 1,000 times greater than our mean) Negative mean causes one column of cumulative values to sum close to 0 and the next to sum to a much larger number resulting in extremely large LDF and there for projection that are extremely large Need to address this as it’ll throw off the mean even if you don’t care about the high percentiles 3 options to address this: Remove the extreme iterations Beware of understating the the likelihood of extreme outcomes Recalibrate the Model First need to identify the source of the negative losses Review data used and parameter selection e.g. remove the AYs that might not represent current behavior e.g. if due to S&amp;S then you can just model them separately and then correlate them during simulation Limit Incremental Losses to 0 Either with the simulated mean (Step 2) or the process var step (Step 4) Replace with negatives with 0s Can just do it in certain columns 9.8.2 Non-Zero Sum of Residuals Residuals are supposed to be iid with mean zero and constant variance \\(\\therefore\\) Sum of our residuals from the triangle should be 0 Not necessarily the case since this is just a sample Consequence: Simulated outcomes will be higher than the mean if sum of residuals are positive (and vice versa) 2 options to address this: Keep it if we believe this to be characteristics of the data set Add a constant to each non-zero residual so that it sums to 0 Then sample from the adjusted residuals If residuals are significantly different from zero then the fit of the model should be questioned 9.8.3 Using L-year Weighted Average Select LDFs based on the latest \\(L\\) years GLM Bootstrap Only use \\(L+1\\) diagonals of data to get \\(L\\) diagonals of LDFs Excluded diagonals are given zero weight and we’ll have less CY trend parameter (if we’re using it) In the simulation we’ll only sample residuals for the trapezoid used to parameterize the model (since that’s all we’ll need to estimate parameters) Simplified GLM Get L-year weighted average LDFs Will only have residuals (to sample from) for the most recent L + 1 diagonals In the simulation we’ll create the entire resampled triangle (Since we need the cumulative losses for each row) For projection using the resampled triangle we’ll still only use the L-year average LDFs The 2 methods will results in different results GLM Bootstrap: Models the incremental losses in the trapezoid Simplified GLM: Models the same losses but in relation to the cumulative losses, which include the non-modeled losses in the diagonals excluded 9.8.4 Missing Value ODP Bootstrap: Missing data impact: LDFs Fitted triangle (if missing value lies on the most recent diagonal) Residuals Degree of freedom Solutions: Impute from surrounding values Modify LDFs to exclude missing value Similar to the L-year weighted average: Missing value will be resampled so the cumulative losses can be calculated Projection from the resampled triangle will exclude the missing cell for resampled LDF selection GLM Bootstrap Impact on the is limited, we’ll just have less observations 9.8.5 Outliers Remove outliers if they are not representative of the variability of the losses, below are the options: Remove the entire row (easy if it’s the 1st row of the triangle) Remove the values and treat them as missing values Not use the residual but do create a sampled value in that cell Significant number of outliers might indicate bad model fit GLM Bootstrap Pick new parameters (grouping parameters) Change the error term distribution from \\(z=1\\) ODP Bootstrap Use L-year weighted average Heteroscedasticity may exist See adjustment next sub section and diagnostics Since we dont’ make a distribution assumption, the number of outliers could mean the data is quite skewed and it’s appropriate that is showing up in the simulation 9.8.6 Heteroskedasticity Issue of non constant variance ODP bootstrap assumes residuals are \\(iid\\) with constant variance No longer possible to sample the residuals from the whole triangle with heteroskedasticity GLM Bootstrap has the additional flexibility of choosing parameters to alleviate heteroscedasticity For ODP Bootstrap: 3 ways to deal with heteroscedasticity below They also work for GLM Bootstrap 9.8.6.1 Stratified Sampling Stratified Sampling Split the triangle into groups with similar variance Only sample residuals from the same group Cons Each group may not be that large, which limits the amount of variability in the possible outcomes 9.8.6.2 Hetero-Adjustment to the Residuals Calculate a hetero-adjustment factor to scale the residuals to the same level: Group the residuals with similar then calculate the \\(\\sigma\\) of the residuals in each group \\(i\\) Hetero-adjustment factor: \\(h^i\\) i.e. The largest \\(\\sigma\\) \\(\\div\\) each group’s \\(\\sigma\\) \\[\\begin{equation} h_i = \\dfrac{\\sigma \\left( \\bigcup_1^j r^H_{wd} \\right)}{\\sigma \\left( \\bigcup_i r^H_{wd} \\right)} \\:\\: : \\:\\: \\text{for each } 1 \\leq i \\leq j \\tag{9.22} \\end{equation}\\] Scale up the residuals: Residual (9.8) \\(\\times\\) Hat Matrix Factor (9.10) \\(\\times\\) Hetero Factor (9.22) \\[\\begin{equation} r_{wd}^{iH} = r_{wd} \\times f_{wd}^H \\times h^i \\tag{9.23} \\end{equation}\\] \\(h^i\\) here is based on the group we draw from Need to divide the sampled residual by \\(h^i\\) to reflect the variability of group \\(i\\) \\[\\begin{equation} q^{i*}(w,d) = m_{wd} + \\dfrac{r^{i*}}{h^i}\\sqrt{m_{wd}} \\tag{9.24} \\end{equation}\\] \\(h^i\\) here is based on the group we’re simulating for Adjust the variance for the process variance step in the simulation \\[\\begin{equation} Gamma(m_{wd}, \\dfrac{\\phi m_{wd}}{h^i}) \\tag{9.25} \\end{equation}\\] Remark. The hetero adjustment factors are new parameters and will impact degrees of freedom and will impact the scale parameter (9.13) and the degrees of freedom adjustment factor (9.9) 9.8.6.3 Non-constant Scale Parameters Adjust the dispersion factor \\(\\phi\\) as well as the residuals (similar to above) Calculate the hetero adjustment factor \\(h_i\\) using formula (9.27) below: \\[\\begin{equation} h_i = \\sqrt{\\dfrac{\\phi}{\\phi_i}} \\tag{9.26} \\end{equation}\\] Perform step 3 and 4 from the hetero adjustment method above (i.e. Equation (9.23) and (9.24)) Calculate \\(\\phi_i\\) for each homogenious residual group \\(i\\) (\\(n_i\\) = number of residuals in group): \\[\\begin{equation} \\phi_i = \\dfrac{N}{N-p}\\dfrac{\\sum_{w,d \\in \\{i\\}}r^2_{w,d}}{n_i} \\tag{9.27} \\end{equation}\\] Use \\(\\phi_i\\) for the process variance step Remark. The \\(\\phi_i\\) here also amount to new parameters that will impact the degrees of freedom adjustment factor (9.9) The hetero adjustment factor (9.26) is more theoretically sound but in practice very similar to (9.22) 9.8.7 Heteroecthesious Data ODP bootstrap requirements: Symmetrical shape (annual by annual, quarter by quarterly, etc triangles) Homoecthesious data (similar exposure) Heteroecthesious = Accident years have different level of exposures Here we are focusing on heteroecthesious due to interim evaluation dates: Partial first development period Partial latest calendar period 9.8.7.1 Partial First Development Period This means the entire first development period is shorter than the rest e.g. Annual data evaluated as of 6/30 with 1/1-12/31 AYs We’ll have a triangle with development periods @6, 18, 30, 42, etc Pearson residuals use the square root of the fitted value to make them all exposure independent (debatable…) \\(\\therefore\\) No impact to residuals Adjustment: Scale down the most recent AY projection to the appropriate exposure period (e.g. half the exposure based on example above), we have 2 options: Prorate the mean of the incremental cells for the latest AY between step 3 and 4 of the bootstrap process and then proceed to Step 4 for the process variance as usual Prorate the simulated incremental cells for the latest AY after the process variance step (Step 4) 9.8.7.2 Partial Latest Calendar Period This is where the latest diagonal is partial diagonal e.g. Evaluate in between typical data evaluation date Evaluation @6/30 for a 1/1-12/31 AYs and 12-24-36 triangle Similar problem as partial first development period + partial data in most recent diagonal ODP Bootstrap Select LDF by excluding latest diagonal or prorating the latest diagonal to full year Adjusted simulation process Calculate sampled triangle as usual (diagonal will be of full year) Calculate full year LDFs and Ultimate as usual Additional steps: De-annualize the diagonal Interpolate the full year LDFs to match the diagonal Forecast loss Scale down the latest AY similar to the partial AY adjustment No change GLM Bootstrap Should be something similar 9.8.8 Exposure Adjustment Adjustment for when exposure changed dramatically over the years (e.g. rapid growth or run off) ODP Bootstrap Divide losses by exposure (model loss cost) Need to multiply the simulated results by the exposure (after the process variance step) GLM Bootstrap Adjust losses by exposure similar to above Fit to the exposure adjusted losses should be exposure weighted (i.e. exposure adjusted losses with higher exposure are assumed to have lower variance, see Anderson et al. (2007)) This will need fewer AY parameters since the exposure adjustment should capture a lot of the difference between AYs 9.8.9 Parametric Bootstrapping ODP Bootstrap See CAS Tail Factor Working Party Report (2013) Add tail factor to the algorithm by assuming the factor follows a distribution (other considerations such as process variance, hetero-adj can all be extended to include the tail factors) Should be an extrapolation of the incremental tail factors (instead of a single tail factor to ultimate) Tail factors typically have \\(\\sigma &lt;\\) 50% of the tail factor - 1 (But should compare to the \\(\\sigma\\) of the AtA factors leading up to the tail in both the actual and simulated data) GLM Bootstrap Continue to use the last \\(\\beta_d\\) to estimate the tail by continuing to apply it (similarly for CY parameter) 9.8.10 Fitting a Distribution to ODP Bootstrap Residuals Data points from triangle may not be representative of the underlying distribution Whether the most extreme observation is a 1-in-100, 1-in-1000 event Alternative is to fit a distribution to the residuals and sample from the distribution instead i.e. parametric bootstrapping "],
["odp-diagnostics.html", "9.9 Diagnostics", " 9.9 Diagnostics Use diagnostics to judge the quality of the model: Test model assumptions Gauge quality of model fit Guide the adjustments of model parameters 5 diagnostics Residual graphs Normality test Outliers Parameter adjustment Model results 9.9.1 Residual Graphs Plot residuals versus CY, AY, Age, forecast loss (on x-axis) Want to see random variability around zero Bare in mind that we don’t have the same number of residuals at each point (helpful to plot the line for average as well) Test assumption of iid residuals across the entire triangle This helps with grouping for hetero adjustment Plot the relative \\(\\sigma(r_{wd})\\) for each group to further help with the groupings Do all the plots again after adjustment 9.9.2 Normality Test Normality is not required, only need this if we’re doing parametric bootstrap with normal distribution Plot residuals against the normal best fit based on the percentiles QQ-plot Statistical tests: Check if p-value &gt; 5% \\(R^2\\) AIC \\[2p + n \\left [ 1 + \\ln(2\\pi\\dfrac{RSS}{n})\\right]\\] BIC \\[n \\ln\\left( \\dfrac{RSS}{n}\\right) + p \\ln(n)\\] \\(RSS\\) = actual residual - expected residual from normal then squared and summed 9.9.3 Outlier Remove true outliers but do not remove points that are realistic extreme scenarios Use box &amp; whisker plot Box hows 25%-tile to 75%-tile Whiskers are 3 times the inter quartile range (both side total) Residuals outside the range are graphed 9.9.4 Parameter Adjustments Test model with different sets of parameters using GLM bootstrap Check parameter significance based on t-statistics (&gt;2) Parameter selection process: Start with all the AY and Age parameters (\\(\\alpha_w\\) and \\(\\beta_d\\)) and remove the insignificant ones until only significant parameters are left Add CY parameter (\\(\\gamma\\)) and check for significance After selecting parameters: Check the diagnostics discussed above (e.g. residuals and normality) Make hetero adjustment if necessary Compare implied development with ODP 9.9.5 Review Model Results Review outputs once we have decided on a model and run the bootstrap Mean, s.e., CoV, Min/Max, and percentiles by AYs Incremental fitted mean, s.e. and CoV for each cell in the triangle Check for reasonability and consistency Table 9.7: Model output review format AY Mean Unpaid Standard Error CoV Min Max 50%-tile 75%-tile 95%-tile 99%-tile (1) (2) (3) = (2) / (1) (4) (5) (6) (7) (8) (9) 1 - - - - - - - - - \\(\\vdots\\) - - - - - - - - - \\(w\\) - - - - - - - - - Total \\(\\sum\\) - - - - - - - - Remark. Standard Error: Col (2) Total s.e. should be greater than any individual year but less than the straight sum of each AY’s s.e. Expect s.e. to increase going down the column This is different from Mack? Where we expect the total s.e. to be greater than the simple sum due to correlation between AYs? (questionable statement here) Remark. Coefficient of Variation: Col (3) Total CoV should be less than any individual year (due to diversification of results across AYs) Except for the most recent AYs, CoV should decrease going down the column (due to larger based of unpaid losses for the more recent AYs) Higher CoV for the most recent AYs due to: More parameters used to forcase for the most recent AYs \\(\\therefore\\) parameter uncertainty \\(\\gg\\) process variance Model maybe overestimating the uncertainty \\(\\Rightarrow\\) Use BF of Cape Cod Remark. Min &amp; Max Col (4) - (5) Check for reasonability (e.g. extreme outcomes from negative values) Implausible results can affect the mean "],
["shapland-multi-mod.html", "9.10 Using Multiple Models", " 9.10 Using Multiple Models Use different methods (Paid/Inc’d Dev, BF, etc) and assigning weights by AYs Models should be reviewed and finalize individually before blending with weights Method 1: In the process variance step of bootstrap, use the same underlying \\(u \\sim U(0,1)\\) to draw from each model then weight the models by a set of deterministic %’s Use the same random variable or else we would reduce the variability of the outcomes Method 2: Run each model independently for each simulation (i.e. use different \\(u \\sim U(0,1)\\)) then for each AY use the weights to randomly select one of the modeled results Results will be a mixture of the various models Other considerations: Should consider both the mean and standard deviation (or CoV) in each model result when selecting weights Can also select the weights using Bayesian methods to account for the quality of each model’s forecast Perform the same model output review as in the above section for the best estimate Also review the IBNR by AYs to look for inconsistencies (e.g. negative IBNR) and compare to deterministic results 9.10.1 Additional Useful Output Using the best estimate total unpaid mean, s.e., and CoV from above to fit to Normal, LogNormal, and Gamma distribution. We can use these fitted distribution to: Assess quality of fit Parameterize a DFA model Smooth out extreme values 9.10.2 Estimated Cash Flow Results Since bootstrap generates simulation for each cell in the bottom half of the triangle we can use this to get cash flow forecasts by CY and their variability as well We can review the s.e. and CoV similar what we did in the diagnostics section 9.10.3 Estimated Ultimate Loss Ratio Results We can estimate mean and the variability of ultimate loss ratios by AYs Compile a similar table as before 9.7 but for loss ratio Useful for projecting pricing risk in a risk model 9.10.4 Estimated Unpaid Claims Runoff Results Project unpaid claims out by CY similar to the cash flow projection Useful for calculating risk margins using the cost of capital method 9.10.5 Distribution Graphs Plot the distribution of the simulated unpaid in a histogram Or smooth the histogram with a Kernel density function (for each point it takes a weighted average of the points around it, giving less weight to points further from it) For each point it takes a weighted average of the points around it; giving less weight to points further from it 9.10.6 Correlation Correlate the loss distribution over several LoB Multivariate distribution requires the same underlying distribution which doesn’t work here for ODP Method 1: Location Mapping When sampling the residuals, sample from the same place in the triangle for all the lines we want to correlate Disadvantages: Requires all LoB to have the same size triangle with no missing values or outliers Cannot stress the correlations among the LoBs (Can only use the historical correlations) Method 2: Re-Sorting Use Iman-Conover algorithms or Copulas (Not explained in paper) Advantages: Can accommodate different shapes and sizes Can make different correlation assumptions Can strengthen the correlation for extreme events (e.g. t Copula vs normal Copula) Calculate correlation matrix using Spearman’s Rank Order and re-sorting based on the ranks of unpaid claims by AYs Look at p-value for each correlation parameter to see that they’re significantly different from zero Additional comments: Using residuals to correlate LoBs (both location mapping&amp; re-sorting) are liable to create correlations close to zero Reserve Risk: Correlate total unpaid by correlating the incremental paid May or may not be a reasonable approximation Risk not modeled is contagion risk, where a single event results in claims in multiple lines of business (can change correlation assumptions to address this) Pricing Risk: Correlate loss ratios over time Not as likely to be close to zero Use different correlation assumption than for reserve risk "],
["shapland-testing.html", "9.11 Model Testing", " 9.11 Model Testing Important to test the model results against actual (see Meyers) Challenges: Don’t have underlying distribution to compare with Can’t wait for 10 years to see how it forecast General Insurance Reserving Oversight Committee test: GIROC created triangles that met the assumptions of Mack and ODP 30K 10 \\(\\times\\) 10 triangles were created Mack: losses were above the 99th percentile about 8-13% of the time \\(\\Rightarrow\\) Mack underestimates tail events ODP from England &amp; Verrall: losses were above the 99th percentile about 3% of the time This does not include the residual adjustments Future testing: Create datasets from claims transaction data and use them for model testing 9.11.1 Future Research Test ODP bootstrap on realistic data from CAS loss simulation model See how adjustments discussed here improve predictive power Expand ODP bootstrap with Munich Chainladder for incurred/paid Claim counts and average severity Different residuals (e.g. deviance or Anscombe residuals) Select weights using Bayesian methods by AYs Other risk analysis measures and use for ERM SII requirements Research in correlation matrix (difficult to estimate) "],
["past-exam-questions-8.html", "9.12 Past Exam Questions", " 9.12 Past Exam Questions TIA Exercise \\(\\star \\star\\) TIA 1: Use simplified GLM and then back out the GLM parameters TIA 2: Reduce parameters TIA 3: Minimize square error for GLM TIA 4: Benefit of simplified GLM \\(\\star\\) TIA 5: Residuals with england verrall adjustment TIA 6: Dispersion TIA 7: Dispersion with hat matrix adj TIA 8: Simulate loss variance distribution assumption &amp; simulate psudeo triangle (no process variance) TIA 9: Setup GLM with CY trend TIA 10: Negative values TIA 11: Simulate negative TIA 12: Partial triangle TIA 13: Stratified (includes “new” method in the paper) TIA 14: Dealing with correlation (location mapping) TIA 15: Outliers Practical Issues 2013 #7: negative values 2014 #7: List 4 practical issues and solutions 2015 #10: Heteroscedasticity, why important, adjustments description 2015 #11: Identify issues with given triangle: Negative values, outliers, exposure level 2016 #12: Negative values adjustment Diagnostics \\(\\star\\) 2014 #9: Evaluate the results given mean unpaid, s.e., CoV by AYs \\(\\star\\) 2016 #13: Residual plots, part c on rational Calculations \\(\\star \\star\\) 2016 14: Hetero adjustment 9.12.1 Question Highlights n/a -->"],
["obtaining-predictive-distributions-for-reserves-which-incorporate-expert-opinions-r-verrall.html", "Chapter 10 Obtaining Predictive Distributions for Reserves Which Incorporate Expert Opinions - R. Verrall", " Chapter 10 Obtaining Predictive Distributions for Reserves Which Incorporate Expert Opinions - R. Verrall Model is defined with (ODP or ODNB) and stochastic row parameters Moments for Mack: (10.1) &amp; (10.2) Moments for ODP: (10.3) Moments fod OD NB: (10.6) Chainladder intervention Variance of prior and it’s impact \\(\\star\\) Bayesian BF calculation: Gamma moments (10.10) Impact of \\(\\beta_i\\) on the Gamma variance \\(\\star\\) Credibility formula (10.11) and credibility weight (10.12) \\(\\beta_i\\) and \\(\\varphi\\) impact the credibility weight Use and calculate the stochastic column parameters \\(\\star\\) Use the parameters with (10.13), see fig. 10.1 \\(\\star\\) Calculate the parameters \\(\\gamma_i\\) pictorially, see fig. 10.2 "],
["Verall-intro.html", "10.1 Introduction", " 10.1 Introduction Uses Bayesian techniques to allow incorporation of expert opinion and also maintain the integrity of the prediction error i.e. how expert opinion from sources other than the specific data set under consideration can be incorporated into the predictive distributions of the reserves Take into account prior knowledge in setting reserves e.g. Adjusting data to reflect changes in benefits or claims handling, or select L-year average LDFs Calculating prediction error using prior knowledge Use Bayesian to take into account our a-priori and the strength of that a-priori (credibility) Paper focus on use of a-priori knowledge for LDF selection and BF Method Development of MCMC techniques has made Bayesian methods much easier Makes it easier to find the posterior distributions for future observations (defining the Bayesian model is always easy) MCMC breaks down the simulation process into a number of simulations that are easy to carry out \\(\\therefore\\) This solves the common Bayesian problem of difficulty in finding the posterior distribution (as they can be multidimensional) MCMC doesn’t simulate all the parameters at once, it use the conditional distribution of each parameter, given all the others \\(\\therefore\\) Reducing the simulation to a univariate distribution Markov chain is formed because each parameter is considered in turn, and it is a simulation-based method \\(\\therefore\\) MCMC 10.1.1 Notation For a \\(n \\times n\\) triangle Claims Incremental claims for AY \\(i\\) and age \\(j\\): \\(c_{ij}\\) \\[\\{ c_{ij} \\: : \\: j=1,...,n-i+1 \\: ; \\: i = 1,...,n \\}\\] Cumulative claims \\[D_{ij} = \\sum_{k=1}^j c_{ik}\\] Ultimate losses \\[D_{in} = \\sum_{k=1}^n c_{ik}\\] Remark. We only consider forecasting losses up to the latest development year \\(n\\) It is possible to extend this to allow a tail factor but not in this paper Row Parameters Expected ultimate losses for AY \\(i\\): \\(x_i\\) \\[x_i = \\mathrm{E}[D_{in}]\\] Column Parameters Expected % reported in each period: \\(y_j\\) \\[\\sum y_i = 1\\] Expected reported to date: \\(p_j\\) \\[p_j = \\sum_{k=1}^j y_k\\] Expected development from \\(D_{ij-1}\\) to \\(D_{ij}\\): \\[\\lambda_j = \\dfrac{p_j}{p_{j-1}}\\] \\[\\{\\lambda_j \\: : \\: j = 2 ,...,n\\}\\] Weighted average LDFs: \\(\\hat{\\lambda}_j\\) \\[\\hat{\\lambda}_j = \\dfrac{\\sum_{i=1}^{n-j+1} D_{ij}}{\\sum_{i=1}^{n-j+1} D_{ij-1}}\\] "],
["stochastic-models-for-the-chainladder.html", "10.2 Stochastic Models for the Chainladder", " 10.2 Stochastic Models for the Chainladder Stochastic models that have the same estimate of Unpaid losses as the Chainladder For each model, we can calculate the MSE of prediction and therefore a prediction interval Proposition 10.1 Prediction variance = process variance + estimation variance Proof. \\[\\begin{array} \\text{MSEP} &amp;= &amp;\\mathrm{E}[(y - \\hat{y})^2] \\\\ &amp;= &amp;\\mathrm{E}[((y-\\mathrm{E}[y]) - (\\hat{y} - \\mathrm{E}[y]))^2] \\\\ &amp;\\approx &amp;\\mathrm{E}[((y-\\mathrm{E}[y]) - (\\hat{y} - \\mathrm{E}[\\hat{y}]))^2] \\\\ &amp;= &amp;\\mathrm{E}[(y - \\mathrm{E}[y])^2] \\\\ &amp; &amp;-2\\mathrm{E}[(y-\\mathrm{E}[y])(\\hat{y} - \\mathrm{E}[\\hat{y}])] \\\\ &amp; &amp;+ \\mathrm{E}[(\\hat{y} - \\mathrm{E}[\\hat{y}])^2]\\\\ &amp;= &amp;\\underbrace{\\mathrm{E}[(y - \\mathrm{E}[y])^2]}_{\\text{Process Variance}}+ \\underbrace{\\mathrm{E}[(\\hat{y} - \\mathrm{E}[\\hat{y}])^2]}_{\\text{Estimation Variance}}\\\\ \\end{array}\\] Remark. Assume future observations are independent of past observations and the -2 term above goes to 0 S.e. = \\(\\sqrt{\\text{Estimation Variance}}\\) Prediction error includes both the error in estimating our parameters and the process variance (inherent variability in the data being forecast) Both bootstrap and MCMC allows us to calculate the prediction error 10.2.1 Mack-1993 (Non-parametric) Only the first 2 moments of cumulative claims are specified \\[\\begin{equation} \\mathrm{E}[D_{ij}] = \\lambda_j D_{ij-1} \\tag{10.1} \\end{equation}\\] \\[\\begin{equation} \\mathrm{Var}(D_{ij}) = \\sigma^2_j D_{ij-1} \\tag{10.2} \\end{equation}\\] Remark. Mean is the same as the Chainladder Variance is \\(\\propto\\) claims reported to date \\(D_{ij-1}\\) \\(\\sigma^2_j\\) has to be estimated separately from the development factors The simplicity of this allows the parameter estimate and prediction errors to be obtained from a spreadsheet Downside is that without specifying a distribution we don’t get a predictive distribution 10.2.2 Over-dispersed Poisson and Negative Binomial Separate stream of research that focused on the use of GLM Incremental claims distribution OD Poisson: \\[\\begin{equation} \\begin{split} &amp; c_{ij} \\mid c, \\alpha, \\beta, \\varphi \\sim ODP(m_{ij}, \\varphi m_{ij}) \\\\ &amp; \\mathrm{E}[c_{ij}] = m_{ij} \\\\ &amp; \\mathrm{Var}(c_{ij}) = \\varphi m_{ij} \\\\ &amp; \\ln(m_{ij}) = c + \\alpha_i + \\beta_j \\: ; \\: \\alpha_1 = \\beta_1 = 0 \\end{split} \\tag{10.3} \\end{equation}\\] Remark. Model result is the same as Chainladder \\(m_{ij}\\) mathematically the same as the one defined in Shapland, we can not include the \\(c\\) term and just use \\(\\alpha_i\\)’s as the individual level parameters instead of having \\(\\alpha_1 = 0\\) and use the \\(alpha_i\\)’s as adjustments to the level parameter constant \\(c\\) This allows for calculating prediction error Alternative way of writing ODP \\[\\begin{equation} \\begin{split} &amp; c_{ij} \\mid x,y, \\varphi \\sim ODP(x_i y_j, \\varphi x_i y_j) \\\\ &amp; \\sum_{k=1}^n y_k = 1 \\\\ &amp; x = \\{x_1, x_2,...,x_n\\} \\:\\: \\text{row parameters}\\\\ &amp;x_i = \\mathrm{E}[D_{in}] \\:\\: \\text{Expected ultimate cumulative losses up to }n\\\\ &amp; y = \\{y_1, y_2,...,y_n\\} \\:\\: \\text{col parameters}\\\\ &amp; y_i \\:\\: \\text{Proportions of ultimate losses that emerge in each development year} \\end{split} \\tag{10.4} \\end{equation}\\] Recall: \\[\\begin{equation} \\begin{split} &amp;X \\sim \\text{Poisson}(\\mu) \\\\ &amp;Y = \\varphi X \\sim ODP(\\varphi \\mu, \\varphi^2 \\mu)\\\\ \\end{split} \\tag{10.5} \\end{equation}\\] Where \\(\\varphi\\) is typically &gt; 1 \\(\\therefore\\) over-dispersed This can be extend to other distribution beyond Poisson e.g. see over-dispersed negative binomial below We can get the same predictive distribution (as ODP) with ODNB ODNB also make the connection between ODP and Chainladder more apparent Incremental claims distribution for OD Negative Binomial: \\[\\begin{equation} \\begin{split} &amp; c_{ij} \\mid D_{ij-1}, \\lambda_j, \\varphi \\sim ODNB \\\\ &amp; \\mathrm{E}[c_{ij}] = (\\lambda_j - 1)D_{ij-1} \\\\ &amp; \\mathrm{Var}(c_{ij}) = \\varphi \\lambda_j \\mathrm{E}[c_{ij}] \\\\ \\end{split} \\tag{10.6} \\end{equation}\\] Cumulative claims distribution for OD Negative Binomial: \\[\\begin{equation} \\begin{split} &amp; D_{ij} \\mid D_{ij-1}, \\lambda_j, \\varphi \\sim ODNB \\\\ &amp; \\mathrm{E}[c_{ij}] = \\lambda_j D_{ij-1} \\\\ &amp; \\mathrm{Var}(c_ij) = \\varphi (\\lambda_j - 1) \\mathrm{E}[c_{ij}] \\\\ \\end{split} \\tag{10.7} \\end{equation}\\] Remark. For both ODP and ODNB Use a quasi-likelihood approach so that the loss data are not restricted to the positive integers Reserve estimates are the same as Chainladder Both are subject to the positivity constraints It’s apparent in the ODNB formula that the column sums must be positive or else we’ll have development factor \\(\\lambda_j &lt; 1\\) \\(\\Rightarrow\\) Varaiance to be negative Advantages (over Kremer): Does not necessarily break down if there are negative incremental loss values In a strict sense, the model requires the incremental losses in a column to be positive otherwise it is more difficult to justify and interpret the inferences (but it doesn’t necessarily break the model) Gives the same reserve estimate as Chainladder More stable than the log-normal model of Kremer Verrall suggest that model can be specified for either incremental or cumulative loss Advantage of the NB: Form of the mean is the same as chainladder If we replace the NB and use normal instead we can deal with the problem of negative incremental claims (not discussed in paper) "],
["cl-intervention-verall.html", "10.3 Incorporating Expert Opinion about the Development Factors", " 10.3 Incorporating Expert Opinion about the Development Factors Intervene in the estimation of Chainladder factors Intervention in a development factor in a particular row How many years of data to use in the estimation Procedure Step 1. Use ODNB from (10.6) for our incremental claim distribution Step 2. Choose prior distribution Prior distributions (e.g. gamma, log-normal, etc) are chosen so that the numerical procedures in software (i.e.g winBUGS) work as well as possible We just choose if we want a strong or vague prior Vauge priors (i.e. large variances) Closer to Bayesian Chainladder Prediction error will be similar to CL or slightly larger Strong priors (i.e. small variances) We think prior means are appropriate Prediction error will decrease Examples below for a \\(n \\times n\\) triangle 10.3.1 Reproduce the Chainladder This is the form if we just want to have just a regular Chainladder \\[\\begin{array}. \\lambda_{i,j} = \\lambda_j &amp; \\text{for } &amp;i = 1,2,...,n-+1 ;\\\\ &amp; &amp;j = 2,3,...,n\\\\ \\end{array}\\] Vague prior distributions for \\[\\lambda_j \\:\\: ; \\:\\:(j=2,3,...,n)\\] Use vague prior so \\(\\lambda_j\\) is based on data 10.3.2 Intervention in a development factor in particular rows Example expert opinion: 2nd development factor (\\(\\lambda_3\\), from col 2 to 3) should be 1.5 for the recent 3 years (i.e. row 8,9,10) while the others being the same \\[\\begin{array}. \\lambda_{i,j} = \\lambda_j &amp; \\text{for } &amp;i = 1,2,...,n-+1 ;\\\\ &amp; &amp;j = 2,4,5,...,n\\\\ \\lambda_{i,3} = \\lambda_3 &amp; \\text{for } &amp;i = 1,2,..,7 \\\\ \\lambda_{8,3} = \\lambda_{9,3} = \\lambda_{10,3} \\\\ \\end{array}\\] Prior distribution mean and variance is chosen to reflect the expert opinion \\(\\lambda_{8,3}\\) has prior distribution with mean 1.5 and variance \\(W\\) \\(W\\) is selected to reflect the strength of the prior information \\(\\lambda_j\\) have prior distributions with large variances 10.3.3 Intervention in using L-years average Example expert opinion: Use 5 years weighted average for LDF selection We divide the data into 2 parts using the prior distributions: \\[\\begin{equation} \\begin{array}. \\lambda_{i,j} = \\lambda_j &amp; \\text{for } &amp;i = n-j-3, n-j-2, n-j-1,\\\\ &amp; &amp;n-j, n-j+1\\\\ \\lambda_{i,j} = \\lambda_j^* &amp; \\text{for } &amp;i = 1,2,...,n-j-4 \\\\ \\end{array} \\tag{10.8} \\end{equation}\\] Both \\(\\lambda_j\\) and \\(\\lambda_j^*\\) have prior distributions with large variance so they are estimated from the data The first part of the (10.8) is to adjust for later development years where there are less than 5 rows For those columns there is just one development parameters \\(\\lambda_j\\) "],
["bayesian-model-for-the-bornhuetter-ferguson-method.html", "10.4 Bayesian Model for the Bornhuetter-Ferguson Method", " 10.4 Bayesian Model for the Bornhuetter-Ferguson Method Use the ODP model from (10.4) For the BF there is expert opinion about the level of each row, so we’ll specify the prior distribution for it below: \\[\\begin{equation} x_i \\mid \\alpha_i, \\beta_i \\sim \\Gamma(\\alpha_i, \\beta_i) \\:\\: ; \\:\\: x_i \\perp\\!\\!\\!\\!\\perp x_j \\:\\: i \\neq j \\tag{10.9} \\end{equation}\\] Easiest to consider the mean and variance of the gamma distribution to parameterize \\[\\begin{equation} \\mathrm{E}[x_i] = \\dfrac{\\alpha_i}{\\beta_i} = M_i\\\\ \\mathrm{Var}(x_i) = \\dfrac{\\alpha_i}{\\beta_i^2} = \\dfrac{M_i}{\\beta_i}\\\\ \\tag{10.10} \\end{equation}\\] For a given choice of \\(M_i\\), the variance can be altered by changing the value of \\(\\beta_i\\) Larger \\(\\beta_i\\) means we are more sure about the value \\(M_i\\) and vice versa Now consider the effect of using these prior distributions on the model for the data, after some lengthy proof: \\[\\begin{equation} \\mathrm{E}[c_{ij}] = Z_{ij} \\times \\underbrace{(\\lambda_j -1)D_{ij-1}}_{\\text{Chainladder}} + (1 - Z_{ij}) \\times \\underbrace{M_i \\: y_i}_{\\text{BF}} \\tag{10.11} \\end{equation}\\] Remark. Result is a blend of Chainladder and BF (like Benktander) \\(y_j = \\dfrac{\\lambda_j -1}{\\lambda_j \\lambda_{j+1} \\cdots \\lambda_n}\\) Credibility factor \\(Z_{ij}\\) \\[\\begin{equation} Z_{ij} = \\dfrac{p_{j-1}}{\\beta_i \\varphi + p_{j-1}} \\tag{10.12} \\end{equation}\\] Remark. \\(p_{j-1} = \\sum_{k=1}^{j-1} y_k\\) (Mack notations), percentage paid at the prior age Large \\(p_{j-1}\\) means losses are more mature so more weight to the Chainladder method We can influence the balance of the weighting (\\(Z_{ij}\\)) through \\(\\beta_i\\) Larger \\(\\beta_i\\) the more weight goes to the BF method Prior with large variance will give results close to Chainladder Lowers the prediction error but not by much Prior with small variance will give results close to BF Will lower the prediction error due to low variance in the prior \\(\\varphi\\) the process variance on \\(c_{ij}\\) also have impact on the credibililty If there’s more variability in the \\(c_{ij}\\) then we trust the BF method more 10.4.1 Calculation Example Given Incremental paid triangle ODP mean and variance for every AYs Dispersion factor \\(\\varphi\\) Procedure Covert triangle to cumulative and select LDFs (Use all year vol. wtd. average) Calculate Cumulative LDFs and \\(p_j\\) and \\(y_j\\) Calculate Chainladder ultimate and use \\(y_i\\) to calculate the incremental means for each of the future cells Calculate a-priori mean based on the given ODP mean for each AY \\(\\times\\) \\(y_j\\) for the incremental means for each of the future cells Calculate \\(\\beta_i\\) for each AYs using \\(\\beta_i = \\dfrac{\\mathrm{E}[x_i]}{\\mathrm{Var}(x_i)}\\) Note that the \\(\\beta_i\\)’s are unit dependent so need to be consistent Calculate \\(Z_{ij}\\) with (10.12) for each future cells Finally credibility weight the Chainladder and BF estimates "],
["stoch-col-bf.html", "10.5 Stochastic Column Parameters for Bayesian BF", " 10.5 Stochastic Column Parameters for Bayesian BF BF above uses stochastic row parameters \\(x_i\\) and deterministic column parameters (Vol. Wtd. average LDFs) We can use stochastic process for both by first estimating the column parameters then the row parameters Define improper prior distribution for the column parameters first before applying prior distributions for the row parameters and estimating them This approach allows us to take into account the fact that the column parameters have been estimated when calculating the prediction errors, predictive distribution etc We don’t have to include any information about the column parameters so we use improper gamma distribution (wide variance) for the column parameters Result posterior distribution: \\[\\begin{equation} \\begin{array}{c} c_{ij} \\mid c_{1,j}, c_{2,j},..., c_{i-1,j}, x, \\varphi \\sim ODNB \\\\ \\mathrm{E}[c_{ij}] = (\\gamma_i - 1) \\sum \\limits_{m=1}^{i-1} c_{m,j} \\\\ \\end{array} \\tag{10.13} \\end{equation}\\] Note that it is similar to (10.6) but recursive down the column (over \\(i\\)) \\(\\sum \\downarrow\\) \\(\\gamma_i\\) is the new row parameters This tells you the level of losses in the row relative to the rows above Figure 10.1: Stochastic Column Parameters We now have a stochastic version of the BF method BF inserts values for the expected ultimate claims in each row, \\(x_i\\), in the form of the values \\(M_i\\) In Bayesian context, prior distributions will be defined for the parameters \\(x_i\\) as discussed above However, the model has been reparameterized with a new set of parameters \\(\\gamma_i\\) Therefore it is necessary to define the relationship between the new parameters \\(\\gamma_i\\) and the original \\(x_i\\) Section below shows how to find \\(\\gamma_i\\) from the values of \\(x_i\\) given in the prior distributions Stochastic BF summary: Column parameters (LDFs) are dealt with first using improper prior Their estimates will be those implied by the Chainladder Prior information can be defined in terms of distributions for the parameters \\(x_i\\), which can then be converted into values for the parameters \\(\\gamma_i\\) 10.5.1 Calculate Gamma Know how to calculate this pictorially (equation is complicated…) First, use Chainladder to get ultimate loss and % unpaid by AY \\[\\gamma_1 = 1.000\\] First calculate LDFs, then the % unpaid and a-priori for each AY \\(U_i\\) = a-priori ultimate based on LDF for AY \\(i\\) \\(q_i\\) = % unpaid for AY \\(i\\) Figure 10.2: Calculating gamma Note that in step 4 above if you don’t need the individual cells you can just take the \\(U_3 q_3\\) "],
["implementation.html", "10.6 Implementation", " 10.6 Implementation In a full Bayesian analysis, we should have a prior distribution for \\(\\varphi\\) as well But for ease of implementation, we’ll use a plug in estimate Value used is that obtained from the application of the ODP, estimating the row and column parameters using MLE The main thing is just picking how strong our prior is and it’ll be more like Chainladder or BF depending on our prior This section of the paper just goes through the methods we’ve discussed and comparing it to the no intervention Chainladder "],
["past-exam-questions-9.html", "10.7 Past Exam Questions", " 10.7 Past Exam Questions Concepts 2012 #8b: Explain model as trade off between standard CL and BF \\(\\star\\) 2013 #9: Model specification stuff on variance of the prior, BF Bayesian Bayesian vs Bootstrap: Provide expert opinion while maintaining the integrity of the variance estimates Bayesian vs Mack: Provides full distribution of unpaid losses and not just the first 2 moments \\(\\star\\) 2014 #10: Expert opinions in LDFs or row parameters; \\(\\beta\\) impact on the Bayesian model \\(\\star \\star\\) 2016 #11 10.4: Various distribution assumptions TIA 1: Chainladder intervention setup (LDF change) TIA 4: Chainladder intervention setup (LDF based on industry) \\(\\star\\) TIA 5: Model setup change in exposure (bayesian with stochastic row parameter) Full Calculation \\(\\star \\star\\) 2012 #8a 10.3: Bayesian model for BF method TIA 2: Using \\(\\gamma\\) for unpaid \\(\\star\\) TIA 3: BF unpaid estimate \\(\\star\\) TIA 6: Calculate \\(\\gamma\\) and forecast with both the row and column parameters \\(\\star\\) TIA 7: Calculate \\(\\gamma\\) 10.7.1 Question Highlights Figure 10.3: 2012 Question 8 Figure 10.3: 2012 Question 8 Figure 10.3: 2012 Question 8 Figure 10.4: 2016 Question 11 2016 Q11 Solution Part a Need the variance for the 3 distribution: ODP: \\(\\mathrm{Var}(c_{ij}) = \\varphi m_{ij}\\) (10.3) \\[m_{ij} = 75,000 - 50,000 = 25,000\\] \\[\\mathrm{Var}(c_{ij}) = 1.5 \\times 25,000 = 37,500\\] OD NB: \\(\\mathrm{Var}(c_{ij}) = \\varphi \\lambda_j \\mathrm{E}[c_{ij}]\\) (10.6) \\[m_{ij} = 75,000 - 50,000 = 25,000\\] \\[\\lambda_j = \\dfrac{75,000}{50,000} = 1.5\\] \\[\\mathrm{Var}(c_{ij}) = 1.25 \\times 1.5 \\times 25,000 = 46,875\\] Normal: \\(\\mathrm{Var}(D_{ij}) = \\sigma^2_j D_{ij-1}\\) (10.2) \\[\\mathrm{Var}(c_{ij}) = 1.75 \\times 50,000 = 87,500\\] Part b Negative binomial beause \\(\\lambda_j\\) is effectively a loss development factor similar to the chainladder Part c Normal because it’s the only one that can have negative output -->"],
["stochastic-loss-reserving-using-bayesian-mcmc-models-g-meyer.html", "Chapter 11 Stochastic Loss Reserving Using Bayesian MCMC Models - G. Meyer", " Chapter 11 Stochastic Loss Reserving Using Bayesian MCMC Models - G. Meyer 3 reasons model doesn’t predict well Interpretation of all the test: KS-test: (11.1), (11.2), and (11.3) \\(p-p\\) plot (fig. 11.1) Too light tailed: Shallow slope near corner and steep in the middle Too heavy tailed: Steep slope near corner and shallow in the middle Biased upwards: Bow down Freq vs Count plot (fig. 11.2) All the different model introduced (fig. 11.3) Bayesian Models (Cumulative): Lognormal (11.4) \\(\\beta = 0\\) when it’s done developing \\(\\sigma\\) constraint (11.6) Variations Leveled Chain-Ladder (LCL): Add variability to the row parameter with \\(\\alpha\\) Mean (11.5) Correlated Chain-Ladder (CCL): Add AY correlation with \\(\\rho\\) Mean (11.7) Changing Settlement Rate (CSR): LCL with speed up claims closure with \\(\\gamma\\) Mean (11.8) \\(\\gamma &gt;0\\) for increase payout speed Bayesian Models (Incremental): Distribution (11.11) Based on mixed lognormal distribution (11.10) (skewed log-normal (11.9) was not used) \\(\\sigma\\) constraint (11.13) is different Additional constraint on \\(\\beta\\) so that it is decreasing Constraint on CY trend \\(\\tau\\) Additional constraint on \\(\\sigma\\) so that it cannot increase drastically period to period Variation Correlated Incremental Trend (CIT): LIT with added AY correlation \\(\\rho\\) Mean (11.12) Leveled Incremental Trend (LIT): Use skewed distribution and CY trend \\(\\tau\\) Non-Bayesian Models: Mack: See Mack-1994 England &amp; Verall ODP: See Shapland, but doesn’t have the residual adjustments "],
["meyers-intro.html", "11.1 Introduction &amp; Synopsis", " 11.1 Introduction &amp; Synopsis Applying different estimate methods to 200 triangles and comparing the actual outcomes to the predicted distribution to see if the models accurately estimates the distribution of outcomes Models examined: Estimate from Mack and ODP do not have enough variability Incurred losses with variable row parameters and AY correlation \\(\\rho\\) is sufficient Paid losses requires variable row parameters and change in settlement rate parameter \\(\\gamma\\) Three reasons model doesn’t predict well: Insurance loss environment has experienced changes that are not observable at the valuation date i.e. There would be different “black swan” events that invalidate any attempt to model loss reserves There could be other models that better fit the existing data Data used to calibrate the model is missing crucial information needed to make a reliable prediction e.g. Changes in the way the underlying business is conducted, like changes in claim processes of changes in the direct/ceded/assumed reinsurance composition of the claims triangle If we can find better model and/or better data we can rule out 1) If we review many models and none of them validate it gives 1) credence but does not confirm "],
["data-set.html", "11.2 Data Set", " 11.2 Data Set 200 triangles from Sch P 1997 and reviewed 10 years later Paid and incurred 50 triangles from 4 LoB (Commercial Auto, Personal Auto, WC, Other Liab) Potential pitfalls in sample triangle selections Insurer with significant changes to their books over the exposure period would violate the assumptions of the model and should be excluded Solution: Use consisteny of NEP and Net:Gross Premium to establish stability in book Use CoV to establish consistency Pick triangles with CoV below a threshold Need to avoid selecting datasets that best suit the model e.g. removing “outliers” from the data Solution: Choose the company in an automated and well defined manner Losses are considered fully developed at 10 years so in practice the paid and incurred ultimate is slightly different "],
["meyers-test.html", "11.3 Testing Procedure", " 11.3 Testing Procedure We are testing the process of each method and not the results of any one distribution generated from the method Since there’s only 1 actual outcome for each triangle we test Focus on the process (the different methods) that gives us the distribution Compare the predicted percentiles (from the methods) against the expected percentile Typically we can compare distributions by comparing the density function, but we can not in this case because 1) We have a small data sets (50 points) 2) Each data point comes from a different distribution Testing Procedure Given \\(N\\) triangles and their actual outcome in 10 years Generate \\(N\\) sets of distribution (from the \\(N\\) triangles using one of the methods) and determine the predicted percentile \\(p_i\\) based on the predicted distribution i.e. see where the actual outcome lands on our predicted distribution The distribution of the \\(N\\) predicted percentiles \\(p_i\\) should follow a uniform distribution if the model is accurate, so we rank them to form \\(\\{p_i\\}\\) \\[\\{p_i\\} = \\{p_1,...,p_n\\}\\] The expected percentiles \\(\\{e_i\\}\\) should run from \\(\\frac{1}{n+1}\\) to \\(\\frac{n}{n+1}\\) \\[\\{e_i\\} = \\left\\{ \\dfrac{1}{n+1}, \\dfrac{2}{n+1},...,\\dfrac{n}{n+1} \\right\\}\\] 11.3.1 Kolmogorov-Smirnov Test For the KS test we’ll compare \\(\\{ p_i \\}\\) with \\(\\{ f_i \\}\\) \\[\\begin{equation} \\{f_i\\} = \\left\\{ \\dfrac{1}{n}, \\dfrac{2}{n},...,\\dfrac{n}{n} \\right\\} \\tag{11.1} \\end{equation}\\] \\(H_0\\): Distribution of \\(p_i\\) is uniform Test statistics for maximum difference between the predicted and expected percentiles \\[\\begin{equation} D = \\max \\limits_i \\mid p_i - e_i \\mid \\tag{11.2} \\end{equation}\\] Reject \\(H_0\\) @5% confidence level if: \\[\\begin{equation} D &gt; \\dfrac{136}{\\sqrt{n}}\\% \\tag{11.3} \\end{equation}\\] i.e. For \\(n = 50\\) \\(\\Rightarrow\\) 19.2%; \\(n=200\\) \\(\\Rightarrow\\) 9.6% Example Table 11.1: Kolmogorov-Smirnov test example \\(f_i\\) \\(p_i\\) \\(abs\\{ p_i - f_i \\}\\) (1) (2) (3) Col (1): \\(\\{f_i\\} = \\left\\{ \\dfrac{1}{n},...,\\dfrac{n}{n}\\right\\}\\) Col (2): \\(\\{p_i\\}\\) = \\(p_i\\) from each realization of the triangles sorted in ascending order Col(3) = Absolute difference between the first two columns \\(D\\) is the maximum value from column (3) Compare \\(D\\) with \\(\\dfrac{136}{\\sqrt{N}}\\) If \\(D\\) is less than the critical value we do not reject the \\(H_0\\) that \\(\\{ p_i \\}\\) is uniform Remark. Technically based on Klugman you test against both: \\[\\{f^+_i\\} = \\left\\{ \\dfrac{1}{n}, \\dfrac{2}{n},...,\\dfrac{n}{n} \\right\\}\\] and \\[\\{f^-_i\\} = \\left\\{ \\dfrac{0}{n}, \\dfrac{2}{1},...,\\dfrac{n-1}{n} \\right\\}\\] Alternatively, we can use Anderson-Darling test that focuses on the tail But it failed all the models therefore we do not use it as it does not help in model comparison 11.3.2 p-p Plot We plot the \\(p-p\\) plot with \\(e_i\\) vs \\(p_i\\) to diagnosis Dark blueline is what is expected from uniform distribution Light blueline is the critical value for a given \\(n\\) from the KS test above Figure 11.1: p-p plot Model is too light tailed: Shallow slope near corner and steep in the middle Model is too heavy tailed: Steep slope near corner and shallow in the middle Model is biased upwards: Bow down Biased upwards: predicted mean &gt; true mean while the s.d. is correct If we look at lognormal data fitted to normal, we’ll see a mix of light and heavy tailed model i.e. The right tail will be too light so we’ll see a shallow slope in the right and the left tail will be too light so we’ll see a steep slope at the left 11.3.3 Percentile Histogram We plot a (flipped) histogram y-axis being the predicted percentile x-axis is the frequency Bins with width 0.1 (so 10 bins) Vertical blue line represent the expected frequency based on uniform \\(\\{e_i\\}\\) Expected frequency = \\(\\dfrac{\\text{# of points}}{\\text{# of bins}}\\)= \\(\\dfrac{n}{10}\\) Figure 11.2: Percentile Histogram "],
["model-summary-meyers.html", "11.4 Models Overview", " 11.4 Models Overview Starting point for the Bayesian models were to relax some of the assumptions of the Mack method: LCL: Relaxes the first assumption where Mack treats loss to date as a fixed level parameters CCL: Builds on top of LCL and allows for AY correlations, which relaxes the 2nd assumption of Mack Prior distributions Paper uses diffuse prior for the most part since the author doesn’t have direct knowledge of the business Given more direct knowledge of the underlying business, we can specify more restrictive priors for \\(\\{\\alpha_w\\}\\) and \\(logelr\\) Bayesian Models: Leveled Chain-Ladder (LCL): Add variability to the row parameter Correlated Chain-Ladder (CCL): Add AY correlation \\(\\rho\\) Leveled Incremental Trend (LIT): Use skewed distribution and CY trend \\(\\tau\\) Correlated Incremental Trend (CIT): LIT with added AY correlation \\(\\rho\\) Changing Settlement Rate (CSR): LCL with speed up claims closure \\(\\gamma\\) Figure 11.3: Overview of models Non-Bayesian Models: Mack: See Mack-1994 England &amp; Verall ODP: See Shapland, but doesn’t have the residual adjustments Remark. Non-Bayesian Models Mack is the only one that does not have a base form of \\(\\mu_{wd} = \\alpha_w + \\beta_d\\) ODP is the England &amp; Verall Bootstrap "],
["non-bayesian-model.html", "11.5 Non-Bayesian Model", " 11.5 Non-Bayesian Model This paper doesn’t go into much details about them, they are mostly presented to demonstrate their shortcomings and as a motivation to develop the Bayesian methods 11.5.1 Mack Model See procedure from Mack-1994 on the log-normal CI Recall the Chainladder assumptions Incurred Light on both tail \\(\\Rightarrow\\) Does not have enough variability in it’s predicted distribution Paid Similar to ODP, biased high on personal auto and light left tail on WC 11.5.2 ODP Bootstrap England &amp; Verall ODP forecasts log incremental losses \\(\\Rightarrow\\) Only suitable for paid losses Can handle occasional negative losses as long as the \\(\\sum\\) column is positive Same procedure as Shapland paper but without all the residual adjustments Overall shows biased high "],
["bayesian-models-cumulative.html", "11.6 Bayesian Models (Cumulative)", " 11.6 Bayesian Models (Cumulative) Inputs Prior distribution is needed for each parameters (similar to Verall) Wide priors (diffuse) Narrow priors: Use expert knowledge in selecting mean and variance of the parameters Parameters: \\(\\alpha_w\\): row parameters \\(\\beta_d\\): column parameters \\(\\sigma_d\\): variance parameters (mostly constant across columns) \\(\\tau\\): trend \\(\\gamma\\): change in closure rate Data: Paid or incurred Triangle Output The posterior distribution of the parameters is expressed as simulated outputs (not closed form distribution) 11.6.1 Leveled Chain Ladder Data: Cumulative incurred Model Specification \\(C_{wd}\\) has a lognormal distribution with log means \\(\\mu_{wd}\\) and log standard deviation \\(\\sigma_d\\) \\[\\begin{equation} C_{wd} \\sim \\ln \\mathcal{N}(\\mu_{wd} , \\sigma_d) \\tag{11.4} \\end{equation}\\] \\[\\begin{equation} \\mu_{wd} = \\alpha_w + \\beta_d \\tag{11.5} \\end{equation}\\] Remark. \\(\\alpha_w\\) Is a random variable Not value on the diagonal (incurred to date) Model select an \\(\\alpha_w\\) for each instance of the simulation based on wide priors Main feature of this model for adding variability \\(e^{\\alpha}\\) is sort of the ultimate for the AY Remark. \\(\\beta_d\\) \\(\\beta_{10} = 0\\) so we have 100% at 10 years and not overdetermining the model \\(e^{\\beta_d} &lt; 100\\%\\) most of the time (i.e. \\(\\beta_d &lt; 0\\)), represents % paid (incurred) to date Remark. \\(\\sigma_d\\) Subject to the following constraints: \\[\\begin{equation} \\sigma_1 &gt; \\sigma_2 &gt; \\cdots &gt; \\sigma_{10} \\tag{11.6} \\end{equation}\\] Highest variability @ early ages Variance only varies by column (\\(d\\)) (not by AYs) Since as \\(d\\) increases there are fewer claims that are open and subject to random outcomes Priors for \\(\\{\\alpha_w\\}\\), \\(\\{\\sigma_d\\}\\), and \\(\\{\\beta_d\\}\\) Wide prior distribution (all you need to know for exam, below is just FYI) Each \\(\\alpha_w \\sim \\mathcal{N}(\\ln(Premium_w) + logelr, \\sqrt{10})\\) \\(logelr \\sim U(-1,0.5)\\) JAGS expression for a normal distribution uses a precision parameter equal to the reciprocal of the variance \\(\\Rightarrow\\) \\(\\sqrt{10}\\) corresponds to a low precision of 0.1 Each \\(\\beta_d \\sim U(-5,5)\\) for \\(d&lt;10\\) Each \\(\\sigma_d = \\sum_{i=d}^{10} a_i\\) where \\(a_i \\sim U(0,1)\\) Test Results Can compare the variability (s.d.) with Mack by plotting the log(s.d.) of the 2 models Model still does not capture the tail appropriately 11.6.2 Correlated Chain-Ladder Build upon the Leveled Chain-Ladder by adding \\(\\rho\\) to create correlation of losses in one AY and the previous AY Data: Cumulative incurred &amp; paid Model Specification \\(C_{wd}\\) follows a lognormal distribution similar to CCL (11.4), but with log means: \\[\\begin{equation} \\mu_{wd} = \\begin{cases} \\alpha_1 + \\beta_d &amp; \\text{if } w = 1\\\\ \\alpha_w + \\beta_d + \\rho \\cdot \\left[ \\mathrm{ln}\\left(C_{w-1, d}\\right) - \\mu_{w-1,d} \\right] &amp; \\text{if } w &gt; 1\\\\ \\end{cases} \\tag{11.7} \\end{equation}\\] Remark. \\(\\rho \\cdot \\left[ \\mathrm{ln}\\left(C_{w-1, d}\\right) - \\mu_{w-1,d} \\right]\\) If parameters \\(\\{\\alpha_w\\}\\), \\(\\{\\alpha_d\\}\\) and \\(\\rho\\) are given: \\(\\rho\\) is the correlation coefficient between \\(\\ln(C_{w-1,d})\\) and \\(\\ln(C_{wd})\\) \\(\\rho\\) is applied to the difference between the log of actual losses and the log mean of the expected loss from the prior AY Higher losses in one row \\(\\Rightarrow\\) higher expected losses in the following row The correlation \\(\\rho\\) here is what drives the additional variability Model reduces to LCL when \\(\\rho = 0\\) Priors for \\(\\{\\alpha_w\\}\\), \\(\\{\\sigma_d\\}\\), \\(\\{\\beta_d\\}\\) and \\(\\rho\\) Prior is still wide priors \\(\\{\\alpha_w\\}\\), \\(\\{\\sigma_d\\}\\), \\(\\{\\beta_d\\}\\) has the same distribution as in LCL \\(\\rho \\sim U(-1 ,1)\\), the full permissible range Test Results Incurred Results and K-S test show that this model is sufficient Paid Worst than ODP and Mack, biased high for all lines 11.6.2.1 Predictive Distribution Simulation Process Example This section is still work in progress, and it is not important for the exam Predictive distribution of outcomes is a mixed distribution Mixing is specified by the posterior distribution of parameters Below is a summary for the CCL R script provided by Meyers Predictive distribution for \\(\\sum_{w=1}^{10} C_{w,10}\\) (ultimate loss for all AYs @ age 10) is generated by a simulation For each of parameter set \\(\\{\\alpha_w\\}\\), \\(\\{\\sigma_d\\}\\), \\(\\{\\beta_d\\}\\) and \\(\\{\\rho\\}\\), start with the given \\(C_{1,10}\\) and calculate the mean \\(\\mu_{2,10}\\). Then simulate \\(C_{2,10}\\) from a lognormal distribution with log mean \\(\\mu_{2,10}\\) and log standard deviation \\(\\sigma_{10}\\) Similarly, use the result of this simulation to simulate \\(C_{2,10},...C_{10,10}\\) Then form the sum \\(C_{1,10} + \\sum_{w=2}^{10} C_{w,10}\\) 11.6.3 Changing Settlement Rate Based on LCL with \\(\\gamma\\) that allows for speed up in claim payments (likely from claims being reported and settled faster due to technology) Data: Cumulative paid Model Specification \\(C_{wd}\\) follows a lognormal distribution similar to CCL (11.4), but with log means: \\[\\begin{equation} \\mu_{wd} = \\alpha_w + \\left[ \\beta_d \\cdot (1-\\gamma)^{w-1}\\right] \\tag{11.8} \\end{equation}\\] Remark. \\(\\gamma\\) \\(\\gamma &gt;0\\) reflects increase in payment speed as \\((1-\\gamma)^{w-1} &lt; 1\\) A positive \\(\\gamma\\) will cause \\(\\beta_d \\cdot (1 - \\gamma)^{w-1}\\) to increase with \\(w\\) \\(\\Rightarrow\\) indicate speedup in claim settlement Negative \\(\\gamma\\) will indicate a slow down in claim settlement rate \\(\\gamma\\) has less impact further out in the tail as there are less payments happening out there Model fits one \\(\\gamma\\) for the whole triangle Priors for \\(\\{\\alpha_w\\}\\), \\(\\{\\sigma_d\\}\\), \\(\\{\\beta_d\\}\\) and \\(\\rho\\) Wide prior similar to LCL and CCL \\(\\gamma \\sim \\mathcal{N}(0, 0.025)\\) Test Results Overall fits well, slightly biased high on Personal Auto but is a big improvement over the other models "],
["skewed-distribution.html", "11.7 Skewed Distribution", " 11.7 Skewed Distribution Motivations: Incremental data has the following properties: Skewed right Occasionally negative 11.7.1 Skewed Normal Distribution From Frühwirth-Schnatter and Pyne (2000) \\[\\begin{equation} X = \\mu + (\\omega \\cdot Z) \\cdot \\delta + (\\omega \\cdot \\varepsilon) \\cdot \\sqrt{1 - \\delta^2} \\tag{11.9} \\end{equation}\\] \\(\\varepsilon \\sim \\mathcal{N}(0,1)\\) \\(Z \\sim Truncated \\: Normal_{[0,\\infty]} (0,1)\\) \\(\\mu\\) is the location parameter \\(\\omega\\) is the scale parameter, with \\(\\omega &gt;0\\) \\(\\delta\\) is the shape parameter, with \\(\\delta \\in (-1,1)\\) Also weight between \\(\\varepsilon\\) and \\(Z\\) Max skewness = 0.995 from the truncated normal \\(\\therefore\\) Not used by the Meyers as it is not skewed enough 11.7.2 Mixed Lognormal-Normal Mixed ln-n distribution with parameters \\(\\delta\\), \\(\\mu\\) and \\(\\sigma\\) \\[\\begin{equation} \\begin{split} X \\sim \\mathcal{N}(Z, \\delta) \\\\ Z \\sim \\ln \\mathcal{N}(\\mu, \\sigma) \\\\ \\end{split} \\tag{11.10} \\end{equation}\\] Pros Can create distribution more skew than the skewed normal Can also have negative values "],
["bayesian-models-incremental.html", "11.8 Bayesian Models (Incremental)", " 11.8 Bayesian Models (Incremental) Model applies CY trend and therefore uses incremental data \\(I_{wd}\\) Correlated Incremental Trend Model Level Incremental Trend Model 11.8.1 Correlated Incremental Trend \\(I_{wd}\\) has a mixed ln-n distribution (11.10) \\[\\begin{equation} \\begin{array}. I_{wd} \\sim \\begin{cases} \\mathcal{N}(Z_{1,d}, \\delta) &amp; \\text{if }w = 1 \\\\ \\mathcal{N}(Z_{w,d} + \\rho \\cdot (I_{w-1,d} - Z_{w-1,d}) \\cdot e^{\\tau}, \\delta) &amp; \\text{if } w &gt; 1\\\\ \\end{cases} \\\\ Z_{w,d} \\sim \\ln \\mathcal{N}(\\mu_{w,d}, \\sigma_d) \\\\ \\end{array} \\tag{11.11} \\end{equation}\\] With log-normal mean: \\[\\begin{equation} \\mu_{wd} = \\alpha_w + \\beta_d + \\tau \\cdot (w+d-1) \\\\ \\tag{11.12} \\end{equation}\\] Remark. \\(\\tau\\) CY trend parameter Note the \\(\\tau\\) is applied additively in the “log” space (11.12), therefore in the autocorrelation step (11.11) it is applied by \\(e^{\\tau}\\) Remark. \\(\\sigma_d\\) Subject to the following constaints different from LCL and CCL \\[\\begin{equation} \\sigma_1 &lt; \\sigma_2 &lt; \\cdots &lt; \\sigma_{10} \\tag{11.13} \\end{equation}\\] Since we’re looking at incremental data, smaller less volatile claims should be settled early Remark. \\(\\rho\\) Include correlation between AYs similar to CCL method \\(\\rho\\) is the coefficient of correlation between \\(I_{w-1,d}\\) and \\(I_{w,d}\\) Note \\(\\rho\\) is applied outside of the “log” space here to the incremental loss (not log) Recall for CCL we apply \\(\\rho\\) to the \\(\\ln(C_{w-1,d})\\) but we can’t do that here due to the chance of negative incremental losses Priors for \\(\\{\\alpha_w\\}\\), \\(\\{\\sigma_d\\}\\), \\(\\{\\beta_d\\}\\), \\(\\rho\\), and \\(\\tau\\) Disperse priors similar to LCL and CCL unless mentioned below Each \\(\\beta_d \\sim \\begin{cases} U(0, 10) &amp; \\text{if } d \\leq 4 \\\\ U(0, \\beta_{d-1}) &amp;\\text{if } d &gt; 4 \\\\ \\end{cases}\\) This assure \\(\\beta_d\\) decreases for \\(d&gt;4\\) \\(\\tau \\sim \\mathcal{N}(0,3.2\\%)\\), which correspond to a precision parameter by JAGs of 1000 Without restriction it was forecasting very negative trend which is offset by higher \\(\\alpha\\) and \\(\\beta\\) Meyers expected this to be predominantly negative since the other paid method we’ve tried so far were biased high Each \\(\\sigma_d \\sim \\begin{cases} U(0,0.5) &amp; \\text{if } d = 1 \\\\ U(\\sigma^2_{d-1},\\sigma^2_{d-1} +0.1) &amp; \\text{else} \\\\ \\end{cases}\\) Limit the speed \\(\\sigma_d\\) can increase, very high \\(\\sigma_d\\) can lead to unreasonably high simulate results \\(\\delta \\sim U(0, \\text{Avg Premium})\\) Steps for the method: Uncorrelated log mean of each cell with CY trend \\(\\mu_{wd} = \\alpha_w + \\beta_d + \\tau \\cdot(w+d-1)\\) Draw \\(Z_{wd} \\sim Lognormal(\\mu_{wd},\\sigma_d)\\) \\(\\sigma_1 &gt; \\sigma_2 &gt; \\cdots &gt; \\sigma_{10}\\) Smaller less volatile claims should be settled early \\(\\tilde{I}_{wd} \\sim Normal(Z_{wd},\\delta)\\) Add correlation between AYs for rows after the first \\(\\tilde{I}_{wd} \\sim Normal(Z_{wd} + \\rho \\cdot (\\tilde{I}_{w-1,d} - Z_{w-1,d})\\cdot e^{\\tau},\\delta)\\) Test Results Losses not much smaller than CCL while we would like it to be much smaller as CCL was biased high \\(\\rho\\) is lower than from CCL Strong negative correlation between trend \\(\\tau\\) and level parameters \\(\\alpha_w + \\beta_d\\) With small data set it is hard for the model to distinguish the AY level + development vs trend Based on scatter plot of \\(\\tau\\) vs \\(\\alpha_w + \\beta_d\\) for several \\(d\\) and \\(w=6\\) Average \\(\\tau\\) is negative Model showed no improvement over Mack or ODP 11.8.2 Leveled Incremental Trend Same as CIT but with \\(\\rho = 0\\) Results similar to CIT with lower standard deviation "],
["process-parameter-and-model-risk.html", "11.9 Process, Parameter, and Model Risk", " 11.9 Process, Parameter, and Model Risk Process &amp; Parameter Risk \\[\\underbrace{\\text{Variance}}_{\\mathrm{Var}(X)} = \\underbrace{\\mathrm{E}[\\text{Process Variance}]}_{\\mathrm{E}_{\\theta}[\\mathrm{Var}[X|\\theta]]}+\\underbrace{\\mathrm{Var}[\\text{Hypothetical Mean}]}_{\\mathrm{Var}_{\\theta}[\\mathrm{E}[X|\\theta]]}\\] Process risk: average variance of the outcomes from the expected result Parameter risk: variance due to the many possible parameters in the posterior distribution Similar to the idea of “range of reasonable estimates” Typically the parameter risk is much larger than process risk e.g. Total Risk = \\(\\mathrm{Var}\\left[ \\sum \\limits_{w=1}^{10} C_{w,10} \\right]\\) Parameter Risk = \\(\\mathrm{Var}_{\\theta}\\left[\\mathrm{E}\\left[\\sum \\limits_{w=1}^{10} C_{w,10}|\\theta\\right]\\right] = \\mathrm{Var}\\left[ \\sum \\limits_{w=1}^{10} e^{\\mu_{w,10} + \\frac{\\sigma^2_{10}}{2}} \\right]\\) Model Risk: Risk of not selecting the right model If possible models are “know unknowns”, we can turn the model risk into parameter risk Formulate a model as a weighted average of the candidate models with the weights as parameters If posterior distribution of the weights assigned to each model has significant variability, this indicates of model risk And in this case, just a special case of parameter risk If we have a very large dataset to run this model on, the parameter risk will shrink towards 0 and any remaining risk such as model risk will be interpreted as process risk This is mostly a academia thought experiment as most aggregated loss triangles are small datasets This serves to illustrate the theoretical difficulties that occur when we try to work with parameter/process/model classification of risk \\(\\therefore\\) Should focus on the total risk "],
["conclusion.html", "11.10 Conclusion", " 11.10 Conclusion Goal of the paper was to test the predictive accuracy of various models, both mean and distribution of outcomes Not on the reserve estimate for individual insurers Bayesian MCMC models can be developed to overcome shortcomings in existing models (e.g. how LCL and CCL loosens some of Mack Chainladder’s key assumptions) 11.10.1 Results Summary Incurred Data Mack understates variability as it assumes AYs are independent CCL introduces AY correlation and does relatively well Paid Data Mack and ODP were biased high as well as CCL There were change in environment that is not captured Calendar year trend: LIT and CIT still biased high CSR: significantly less bias than LIT and CIT (except for PA still failed) Mack and ODP did better than CCL, LIT and CIT 11.10.2 Final Comments Results were for specific annual statement year 1997 Possible the speed up of claims settlement was specific to the period \\(\\Rightarrow\\) CSR could potentially useless for another year Could use more narrow priors to incorporate knowledge of insurer’s business operation and obtain superior results "],
["appendix-b-intro-to-bayesian-mcmc-models.html", "11.11 Appendix B: Intro to Bayesian MCMC Models", " 11.11 Appendix B: Intro to Bayesian MCMC Models Just additional background information, not part of exam syllabus Definition 11.1 Markov Chain Random process where the transition to the next state depends only on its current state and not on prior states A Markov chain \\(X_t\\) for \\(t=1,2,...\\) is a sequence of vectors satisfying the property that \\[\\Pr(X_{t+1} = x \\mid X_1 = x_1, X_2 = x_2,...,X_t = x_t) = \\Pr(X_{t+1} \\mid X_t = x_t)\\] Key properties of Markov chain for Bayesian MCMC Ergodic class of Markov chain, for which vectors \\(\\{X_t\\}\\) approaches a limiting distribution As \\(T\\) increases, the distribution of \\(\\{X_t\\}\\) for all \\(t&gt;T\\) approaches a unique limiting distribution Markov chains used in Bayesian MCMC (e.g. Metropolis Hastings algorithm) are members of Ergodic class Let \\(x\\) be a vector of observations and let \\(y\\) be a vector of parameters in a model In Bayesian MCMC, the Markov chain is defined in terms of the prior distribution \\(p(y)\\) and the conditional distribution \\(f(x \\mid y)\\) The limiting distribution is the posterior distribution \\(f(y \\mid x)\\) If we let the chain run long enough, the chain will randomly visit all states with frequency that is proportional to their posterior probabilities The operative phrase above is long enough, which means in practice: Develop algorithm for obtaining a chain that is long enough as quickly as possible Develop criteria for being long enough 11.11.1 How Bayesian MCMC works in practice User specifies \\(p(y)\\) and \\(f(x \\mid y)\\) User selects a starting vector \\(x_1\\) Using computer simulation, runs the Markov chain through a sufficiently large number, \\(t_1\\), of iterations This first phase of the simulation is called the “adaptive” phase The algorithm is automatically modified to increase its efficiency See below on the Metropolis-Hasting alogrithm User runs an additional \\(t_2\\) iterations This is the “burn-in” phase \\(t_2\\) is selected to be high enough so that a sample taken from subsequent \\(t_3\\) periods represents the posterior distribution User then runs an additional \\(t_3\\) iterations and then takes a sample, \\(\\{ x_t \\}\\) from the \\((t_2 + 1)\\)th step to the \\((t_2 + t_3)\\)th step to represent the posterior distribution \\(f(y \\mid x)\\) From the sample, we can constructs various statistics of interest that are relevant to the problem addressed by the analysis 11.11.2 Metropolis-Hastings Algorithm Most common algorithms for generating Bayesian Markov chains are variants of the Metropolis-Hastings algorithm Given: \\(p(y)\\) and \\(f(x \\mid y)\\) The algorithm introduces a 3rd distribution \\(J(y_t \\mid y_{t-1})\\): “Proposal” or “jumping” distribution Given a parameter vector \\(y_{t-1}\\) the algorithm generates a Markov chain by the following steps: Select a candidate value \\(y^*\\), at random from the proposal distribution \\(J(y_t \\mid y_{t-1})\\) Computer the ratio \\[R \\equiv R_1 \\times R_2 = \\dfrac{f(x \\mid y^*) \\cdot p(y^*)}{f(x \\mid y_{t-1}) \\cdot p(y_{t-1})} \\times \\dfrac{J(y_{t-1} \\mid y^*)}{J(y^* \\mid y_{t-1})}\\] Select \\(U\\) at random from \\(U(0,1)\\) distribution If \\(U &lt; R\\) then set \\(y_t = y^*\\), else \\(y_t = y_{t-1}\\) Remark. \\(R_1\\) represents the ratio of the posterior probability of the proposal \\(y^*\\) to the posterior probability of \\(y_{t-1}\\) The higher the value of \\(R_1\\), the more likely will be accepted into the chain Regardless of how the proposal density distribution is chosen, the distribution of \\(y_t\\) can be regarded as a sample from the posterior distribution, after a suitable burn-in period Example We can look at trace plots to look at the value of the parameter as the chain progresses More on how it might break in the paper… The key is to be able to scale the proposal distribution to minimize auto correlation This is difficult with many parameters Minimizing autocorrelation in Metropolis-Hasting A good statistics to look at is the acceptance rate of \\(y^*\\) 50% is near optimal for a one parameter model and the rate decreases to about 25% as we increase the number of parameters in the model There are methods to automatically adjust the proposal density function in Metropolis-Hastins All these have been mechanized in software like JAGS and STAN The phase of generating the Markov chain where the proposal density function is optimized is called the “adaptive” state (as discussed above) As models become more complex, adaptive MCMC may not be good enough to eliminate the auto correlation Theory on Markov chain convergence will still hold but there is no guarantee on how fast it will converge If there is significance auto correlation after the best scaling effort, the next best practice is to increase \\(t_3\\) until there are sufficient number of ups and downs in the trace plot and then take a sample of the \\((t_1 + t_2 +1)\\)th to \\((t_1 + t_2 + t_3)\\)th iterations This process is known as “thinning” 11.11.3 Usecase Example for Actuaries Look at how the posterior distribution of \\(\\mu\\) might be of interest Consider a fitted lognormal distribution for a set of claims to determine the cost of an XS layer Given \\(\\mu\\) and \\(\\sigma\\) of the lognormal we can determine the cost of an XS layer (see Klugman, Panjer, and Willmot and the actuar package) As the posterior distribution of \\(\\mu\\) reflects the parameter risk in the model, it is also possible to reflect the parameter risk in the expected cost of a layer by calculating the expected cost of the layer for each \\(\\mu\\) in the simulated posterior distribution It is possible to simulate an actual outcome of a loss \\(X\\) in a layer given \\(\\mu\\) in the posterior distribution The distribution \\(X\\) calculated this way reflects both the parameter risk and process risk in the model 11.11.4 Bayesian interence Using Gibbs Sampling (BUGS) WinBUGS, JAGS are examples of software using Gibbs sampling Sample work process: Read data into R and call JAGS script with R package “runjags” Fetch the sample of the posterior back into R to calculate statistics of interest JAGS perform the Metropolis-Hasting algorithm we described above It also has a number of convergence diagnostics "],
["past-exam-questions-10.html", "11.12 Past Exam Questions", " 11.12 Past Exam Questions TIA Exercise \\(\\star\\) TIA 1: Perform KS test on given percentile TIA 2: read p-p plot and structure correlated AY model TIA 3: Skewed normal limitation TIA 4: Describe CIT model TIA 5: Read p-p plot \\(\\star\\) TIA 6: read p-p plot and how to address the issues Past Exam 2016 #15: KS Test with given percentile Reason why model can not predict How to improve on the Mack model 11.12.1 Question Highlights n/a -->"],
["a-framework-for-assessing-risk-margins-k-marshall-et-al-.html", "Chapter 12 A Framework for Assessing Risk Margins - K. Marshall et al.", " Chapter 12 A Framework for Assessing Risk Margins - K. Marshall et al. Importance of qualitative analysis Independent Risk Parameter and process variance model with stochastic model Internal Systemic Risk \\(\\star\\) Three components of internal systemic risk 12.4 CoV calculation: \\(\\star\\) Score against best practice (Table 12.2, 12.3 and 12.4) Calibrate to CoV for each component where CoV \\(\\in [5\\%, 25\\%]\\) 2 Hindsight analysis Check for internal consistency of the CoV Extnernal Systemic Risk \\(\\star\\) 7 Different risk categories Certain lines are impacted more than other by a given risk CoV: Use benchmark similar to internal but select CoV directly Correlation The 3 main risk sources are independent of each other and therefore can be sum with square root rule (12.2) Pros and cons quantitative method for correlation Independent (12.3): Assume independence across lines, weight by liabilities Internal (12.4): Base on correlation matrix \\(\\Sigma\\), again weighted by liabilities External (12.5) &amp; (12.6): Correlation between each valuation group and risk categories \\(\\Rightarrow\\) then roll up to the risk categories and assume they are independent of each other Risk Margin: (12.7) \\(\\alpha\\)-tile (12.8) \\(\\star\\) Addition analysis Sensitivity, scenario testing \\(\\star\\) Internal benchmarking, important to know the relationships and consistency, been heavily tested in the past External benchmarking Hindsight and mechanical hindsight Regularity of review "],
["introduction-3.html", "12.1 Introduction", " 12.1 Introduction Estimate risk margin for unpaid losses and premium liabilities An additional reserve held as a safety measure in case losses end up worse than expected Risk margin = 75th percentile - mean (an Australian regulatory standard) Based on CoV Goal is to create a framework for an actuary to proceed through to estimate the risk margin Data Segmentation Portfolio for the analysis should be split into reasonably homogeneous groups Might combine or further split groups from what was used for liability valuation Balance the practical benefits from large groups vs insight gained from smaller groups Might want to further split if a LoB has a portion that has significantly more uncertainty than another part (e.g. HO CAT/non CAT) "],
["three-sources-of-uncertainty.html", "12.2 Three Sources of Uncertainty", " 12.2 Three Sources of Uncertainty Coefficient of Variation: \\(\\phi = \\dfrac{\\sigma}{\\mu}\\) \\[\\begin{equation} \\phi^2 = \\underbrace{\\phi_{indep}^2}_{\\text{Independent Risk}} + \\underbrace{\\phi_{internal}^2}_{\\text{Internal Systemic Risk}} + \\underbrace{\\phi_{external}^2}_{\\text{External Systemic Risk}} \\tag{12.1} \\end{equation}\\] Definition 12.1 Systemic Risk Risks that can vary across valuation classes Remark. Qualitative approaches is recommended for systemic risk Need to make assumptions about the correlation of the different risks 12.2.1 Quantitative vs Qualitative Analysis Limitation of quantitative analysis Requires lots of data that we don’t have Only captures historical risk Does not capture risk that did not have an episode (of systemic risk) in the experience period \\(\\therefore\\) Need qualitative measures as well to examine the uncertainty Table 12.1: Quantitative method effectiveness Effective Not effective Independent risk Internal systemic risk Past episodes of external systemic risk External systemic risk that has not yet occurred Good stochastic model will fit away past systemic episodes (e.g. high inflation) while we still want to hold a margin for the future Outcome dependent significantly on actual episodes of risk, not all potential ones (for systemic risk that was not fitted away) Need to judge if episodes in data are representative going forward Model is unlikely to pick up internal systemic risk from the actuarial valuation process "],
["indep-risk-marshall.html", "12.3 Independent Risk", " 12.3 Independent Risk Definition 12.2 Independent Risk = Randomness inherent to the insurance process Parameter Risk: ability to select correct parameters and appropriate model Process Risk: randomness e.g. tossing a die Use stochastic modeling to analyze independent risk Focus on periods where episodes of systemic risk were non-existent or minimal Limit the impact of the historical external episodes and allow us to focus on the independent risk Supplement with internal and external benchmarking 12.3.1 CoV for Independent Risk Model the parameter and process risk together Mack, bootstrap, stochastic CL, GLM, Bayesian Ideally the model adequately model away the systemic risk so all that is left is the independent risk Residual should only have independent risk For small data set: difficult to model away the external systemic risk Solution: Do not model away past episodes of systemic risk Use results as starting point and then add a margin for external systemic risks not in data to have a measure of independent and external systemic risk Complexity of the model should be commensurate with the importance of the total risk margin Models for outstanding claims liabilities: GLM and bootstrapping are particularly useful They can isolate the independent risks Graphing residuals again AY, age, experience period can used to identify past systemic episodes Models for premium liabilities: GLM, bootstrap, Bayesian Can model frequency and severity CoV then combine Frequency: Remove past systemic episodes Severity: Adjust for inflation and seasonality "],
["int-sys-comp.html", "12.4 Internal Systemic Risk", " 12.4 Internal Systemic Risk Definition 12.3 Internal systemic risk = Uncertainty arising from the liability valuation process/actuarial valuation models From source anywhere along the chain of the valuation Examples: Data record/collection/organization (e.g. not collecting the right data; not using the right data) Analysis, actuarial judgement Reserve selection (e.g. management overrides actuary’s opinion) Definition 12.4 3 components of internal systemic risk: Specification Error: From not perfectly modeling the insurance process because it’s too complicated or just don’t have the data Parameter Selection Error: Difficulty in measuring all predictors and the trend in these predictors are particularly difficult to measure Data Error: Lack of data, lack of knowledge of the underlying pricing, u/w, and claim management process, inadequate knowledge of portfolio 12.4.1 CoV for Internal Systemic Risk Benchmarking technique: Need to define a list of risk indicators, score them against best practice, map the scores to a CoV Score against best practice Calibrate Score to CoV 12.4.1.1 Score Against Best Practice For each valuation group, assign a score (1-5) for each risk indicator Table 12.2: Specification Error Risk Indicators Risk Indicators Best Practice # of independent models used Each model should add value by considering a different dimension of claims experience Range of results produced by models Low variation in model results # and importance of subjective adjustments to factors Few subjective adjustments; adjustments regularly monitored and reviewed Ability to detect trends in key claim cost indicators Models have performed well at detecting trends in the past Table 12.3: Parameter Selection Error Risk Indicators Risk Indicators Best Practice Best predictors have been identified (but not necessarily used) Best predictors have been analyzed and identified (int or ext), and show a strong correlation with claims experience Best predictors are stable over time, or change due to process changes Predictors stable over time, stabilize quickly and respond well to process changes Value of predictors used Predictors are close to Best Predictors; lead (rather then lag) claims cost outcomes, modeled, rather than subjectively selected Table 12.4: Data Error Risk Indicators Risk Indicators Best Practice Knowledge of past processes affecting predictors Actuary has good and credible knowledge of past processes and change to processes Extent, timeliness, consistency and reliability of information from the business Regular, pro-active communication between the actuary and claims staff and the business Data is subject to reconciliations and quality control Reconciliation against financials, and prior studies; difference are well understood Frequency and severity of past misestimation due to revision of data No past instances of data revision Assign weight for each risk indicator (weight can vary by valuation group) Calculate weighted average the scores using the selected weights We score the 3 components for each valuation group (OCL and PL) and then roll up the score and assign the average grade for each valuation group to a CoV 12.4.1.2 Calibrate Score to CoV Significant amount of judgement supplement by quantitative analysis CoV \\(\\in [5\\%, 25\\%]\\) Analysis of past model performance should aid in estimating the potential variability Hindsight Analysis: Compare valuation of liabilities at prior point in time to the current view, to gain insight into how a better model can reduce volatility Mechanical Hindsight: Mechanically do various ex post analysis, and see how prediction error can be reduced; e.g. do a detailed vs crude and see the difference Additional comments: Improvement from poor to fair is greater than from fair to good Longer tail line will have higher CoV due to difficulty in estimating the parameters Larger liability will have smaller CoV when all else being equal OCL and PL might not necessarily be on the same scale PL may have additional uncertainty as it’s for future business For short tail lines ELR might be sufficient for PL but might not be the best practice for OCL Can always just add a load on top if justified "],
["ext-sys-risk.html", "12.5 External Systemic Risk", " 12.5 External Systemic Risk Definition 12.5 External systemic risk = systemic risk that are not internal Risks external to the liability valuation process Need to consider systemic risk not in the data set (i.e. Can’t only consider actual episodes of systemic risk in the data set) See risk categories in section below Remark. A handful of these risk categories will dominate the uncertainty for that valuation group Useful to rank the risk categories in order of impact on the uncertainty of a valuation group (This will give guidance on how to score them) Lots of the above should be something the valuation actuary already discussed with the business and with claims: Underwriting and risk selection Claims management Portfolio management process Expense management Emerging trends - portfolio and claims 12.5.1 CoV for External Systemic Risk Use bench marking technique similar to internal systemic risk Directly select the CoV Rank the risk in order of importance to help with the selection Quantitative approach can provide in insight But we need to also consider possible emerging and potential future sources of external systemic risk Should bare in mind the skewness (might not be relevant for 75th percentile if it is very skewed) Consider risk that affect u/w and risk selection, claims management, expense management, economic/legal environment 12.5.1.1 Risk Categories Economic and Social Risks Inflation; unemployment; GDP growth; interest rates; driving patterns; fuel prices; social trends For inflation we are concern with the systemic shifts not just randomness (randomness is in the independent risk) Some are more important for PL than OCL (e.g. driving conditions is more important for PL) Legislative, Political Risks, Claims Inflation Risks: Change in law, frequency of settlement vs suits to completion, loss trend (Long tail lines) All grouped together since each category needs to be uncorrelated with each other Long tail lines: more material to long tail LoBs since these changes could impact the entire portfolio of unpaid claims Considerations: Impact of recent legislative changes, change in court interpretation Potential for future legislative amendments with retrospective impacts Precedent setting in courts Changes to medical technology costs Change to legal costs Systemic shifts in large claim frequency or severity Short tail lines: can impact premium if there are sudden shifts in law or inflation Claim Management Process Change Risk: Change in process of managing claims e.g. case reserve practice Understand current philosophy and know any current or potential future process changes Discuss reporting patterns, payment patterns, finalization rates ,reopen rates, case estimate process More important to OCL, only impact PL when a change in process change the cost level of claims Expense Risk: Claim handling expense and policy maintenance expense CoV should be small Need to understand the drivers of policy maintenance and claim handling expenses Event claims can have a material impact on expenses, typically lower the ratio of expenses to indemnity paid amounts \\(\\therefore\\) consider these expense separately Event Risk: Natural or man-made CAT (Premium liabilities for property) Mostly premium risk Can model from past experience (with adjustment to portfolio size, geographical spread, inflation, policy terms and reinsurance), CAT modeling or input from reinsurers Latent Claim Risk: Claim from source not currently considered to be covered Unlikely for most LoB but can be severe Due to low probability, likely not worth to commit substantial resources to estimate the risk Discussion with reinsurers can guide in preparing a range of scenarios Recovery Risk: Recoveries from reinsurers or non-reinsurers S&amp;S for non reinsurer on LoB like auto Reinsurers, consider reinsurance contracts in place specifically for reinsurers where a large amount of premium is ceded "],
["correlation-aggregating-the-cov.html", "12.6 Correlation (Aggregating the CoV)", " 12.6 Correlation (Aggregating the CoV) Overall Assumes the 3 main risk components are independent of each other \\[\\begin{equation} \\phi = \\sqrt{\\phi_{indep}^2 + \\phi_{internal}^2 + \\phi_{external}^2} \\tag{12.2} \\end{equation}\\] Caveat of quantitative method to measure correlation: Complexity &gt; benefit Results heavily influenced by past correlations (While future correlation may differ) Difficult to separate past episodes of independent risk and systemic risk Internal systemic risk cannot be modeled using standard correlation modeling techniques Likely won’t aligned with our definitions of independent, internal/external systemic risks Practical guidance: Bucket into 0%, 25%, 50%, 75%, 100% (Any finer will likely lead to spurious accuracy) Introduce dummy variables and see their impact in each risk/valuation group pair (Inflation, unemployment, propensity to suit, freq of CAT, fraud) 12.6.1 Independent Risk Cov Correlation Assume independence across liabilities, where \\(i\\) is the different valuation groups \\[\\begin{equation} \\phi_{indep}^2 = \\sum_i (\\phi_i w_i)^2 = (\\vec{\\phi w})(\\vec{\\phi w})^T \\tag{12.3} \\end{equation}\\] \\(w_i\\) can be just claims liabilities or total liabilities (including premium liabilities) 12.6.2 Internal Systemic Risk Correlation Focus on correlation within internal systemic risk Can have correlation between the outstanding claim and premium liabilities for the same valuation group \\[\\begin{equation} \\phi_{internal}^2 = (\\vec{\\phi w}) \\times \\mathbf{\\Sigma} \\times (\\vec{\\phi w})^T \\tag{12.4} \\end{equation}\\] \\(\\mathbf{\\Sigma}\\) is the correlation matrix Again the \\(\\vec{w}\\) is the % of total liabilities 12.6.3 External Systemic Risk Correlation We measure the CoV for each risk category for each valuation group e.g. high inflation will be correlated across all valuation groups that have long tail or event risk across LoB \\(\\mathbf{\\Sigma}_c\\) is the correlation matrix between valuation groups for each risk category \\(c\\) For a given risk category \\(c\\), the CoV is: \\[\\begin{equation} \\phi_{external, c}^2 = (\\vec{\\phi_{c} w}) \\times \\mathbf{\\Sigma}_c \\times (\\vec{\\phi_{c} w})^T \\tag{12.5} \\end{equation}\\] Then assume independence between risk categories: \\[\\begin{equation} \\phi_{external}^2 = \\sum \\limits_{c \\: \\in risk \\: category} \\phi_{external, c}^2 \\tag{12.6} \\end{equation}\\] Important to pick risk categories that are likely to be independent of each other "],
["risk-margin.html", "12.7 Risk Margin", " 12.7 Risk Margin Risk Margin \\[\\begin{equation} \\mu \\times \\phi \\times Z_{\\alpha} \\tag{12.7} \\end{equation}\\] \\(\\mu\\): Expected loss \\(\\phi\\): CoV \\(Z_{\\alpha}\\): 0.67 for 75th percentile \\(\\alpha\\) %-ile \\[\\begin{equation} \\mu(1+\\phi Z_{\\alpha}) \\tag{12.8} \\end{equation}\\] "],
["marshall-add-analysis.html", "12.8 Additional Analysis", " 12.8 Additional Analysis Test the quality of our result Sensitivity Testing: Varying the different CoV and correlations Scenario Testing: Consider what assumptions need to change in our mid point to eat up the risk margin (similar to back testing) Internal Benchmarking: Compare CoVs within between OCL and PL and also with other valuation groups to look for consistency Independent risk: Large liability will have smaller CoV due to law of large numbers Short tail line will have a small CoV as well due to less volatility Therefore: Outstanding Claims Liability: \\(\\phi_{short \\: tail} &lt; \\phi_{long \\: tail} &lt; \\phi_{long \\: tail, \\: small \\: book}\\) Premium Liability - Long Tail: OCL &gt; PL \\(\\Rightarrow\\) \\(\\phi_{OCL} &lt; \\phi_{PL}\\) Because there are many AYs of claims in the reserves Premium Liability - Short Tail: OCL &lt; PL \\(\\Rightarrow\\) \\(\\phi_{OCL} &gt; \\phi_{PL}\\) Because OCL is small LoB with significant event risk will have different risk profiles for the PL and OCL Internal Systemic: If the methods to estimate liabilities is similar across valuation groups, we would expect the CoV to be similar for classes that have similar claim payment patterns External Systemic: Main sources of external systemic risk are higher for long tail lines; Event risk is higher for property; Liability for HO can also be significant External Benchmarking: Compare selected CoV with external sources Such as APRA 2008 Use just as a sanity check, not appropriate to simply take the risk margins from benchmarks Independent risk CoV depend on the size of liabilities \\(\\Rightarrow\\) Similar sized liability today would represent a small book due to inflation \\(\\Rightarrow\\) Adjust CoV upward Estimate PL with a scale up factor from OCL Hindsight Analysis: Compare the actual results with expected over multiple periods Blend of all 3 risks (independent, internal/external systemic), at least the actual episodes of those risk Useful as a guide if the CoVs are reasonable Works well for short-tailed LoB where the AY’s liabilities don’t move together too much Where for long tail lines you can change your mind significantly about the liabilities for multiple AYs in one reserve study Mechanical Hindsight: Value the liabilities today using a mechanical method and repeat with information at older evaluation date Measure independent risk over stable periods By using multiple methods you can measure the internal systemic risk By measuring over long periods of time, you can measure risk from all sources "],
["documentation-and-regularity.html", "12.9 Documentation and Regularity", " 12.9 Documentation and Regularity Level of documentation varies Full study to support CoV should be done every 3 years Key assumptions should be examined in the interim: Emerging trends Emerging systemic risks Change in valuation methods Should consider applying the key steps to any new portfolio "],
["past-exam-questions-11.html", "12.10 Past Exam Questions", " 12.10 Past Exam Questions Concepts \\(\\star \\star\\) 2013 #8 b f g h 12.1: Correlations, internal benchmarking \\(\\star\\) 2014 #8 12.2: internal systemic risk 3 components; risk indicator for each LoB for each risk component; Score the risk indicators \\(\\star\\) 2014 #11 12.3: External systemic risk and which lines impact the most \\(\\star\\) 2015 #8 12.4: Internal source, external source \\(\\star\\) 2015 #9 12.5: Internal benchmark \\(\\star\\) 2016 #10 12.6: int and ext sys risk and exampless, valuation class pick TIA 2: Method for estimating internal CoV \\(\\star\\) TIA 3: Classification of risk categories (int. and ext.) TIA 4: Pick the more important risk TIA 5: legislative risk TIA 8: How to model the risk Calculations \\(\\star\\) 2013 #8 a c d e 12.1: Correlations, risk margins 2015 #8 d: Independent CoV TIA 1: risk margin calculation TIA 6: Find CoV and risk margin \\(\\star\\) TIA 7: CoV calc Internal sys risk weighted by libailities amount Combine all risk sources Significance of a give risk source (based on weight) Calculate change Calculate change assuming full dependencies \\(\\star \\star\\) TIA 9: CoV Calculation and risk margin full calc 12.10.1 Question Highlights Figure 12.1: 2013 Question 8 Figure 12.1: 2013 Question 8 Figure 12.1: 2013 Question 8 Figure 12.1: 2013 Question 8 Figure 12.2: 2014 Question 8 Figure 12.2: 2014 Question 8 Figure 12.3: 2014 Question 11 Figure 12.3: 2014 Question 11 Figure 12.3: 2014 Question 11 Figure 12.4: 2015 Question 8 Figure 12.4: 2015 Question 8 Figure 12.4: 2015 Question 8 Figure 12.5: 2015 Question 9 Figure 12.5: 2015 Question 9 Figure 12.6: 2016 Question 10 Figure 12.6: 2016 Question 10 Figure 12.6: 2016 Question 10 -->"],
["reinsurance-loss-reserving-patrik.html", "Chapter 13 Reinsurance Loss Reserving - Patrik", " Chapter 13 Reinsurance Loss Reserving - Patrik \\(\\star\\) Know the 7 problems of reinsurance reserving Longer claims report lag Persistent upward development of most claims reserves Reporting pattern differ greatly Industry statistics not useful Reports received lack important information Data coding and IT system problems Reserve to surplus ratio is higher for reinsurer General theme: Heterogeneity, low frequency &amp; high severity, lack of detailed (exposure &amp; claims) information, longer lag Know the components of reinsurance reserve Four steps of setting reinsurance reserve: Partition \\(\\Rightarrow\\) Development Patterns \\(\\Rightarrow\\) Estimate \\(\\Rightarrow\\) Monitor and AvE \\(\\star\\) Partition priority Considerations for short vs medium vs long tail lines \\(\\star\\) SB method and how to adjust the premium Shortcut on getting all year IBNR \\(\\star\\) credibility weighted method Weight is \\(Z_k = p_k \\times CF\\) Weighting the reserve for \\(CL\\) and \\(SB\\) AvE "],
["prob-of-re.html", "13.1 7 Problems with Reinsurance Reserving", " 13.1 7 Problems with Reinsurance Reserving Problem 1: Longer claims report lag Claim must be perceived as report-able to the reinsurer by the cedant (e.g. half of the attachment point) and then flow through cedent’s system to reinsurer Cedant’s reporting system \\(\\Rightarrow\\) Cedant’s reinsurance accounting \\(\\Rightarrow\\) Intermediary (broker) \\(\\Rightarrow\\) Reinsurer books the claims \\(\\Rightarrow\\) Reinsurer’s claim system Cedant may undervalue the claim for a long time and thus not reported to the reinsurer See Problem 2 on the cedant reserving to the modal value Extreme delays in discovery or reporting for mass tort claims (e.g. asbestos) Problem 2: Persistent Upward Development of Most Claim Reserves Economic and social inflation Tendency to underestimate LAE Tendency of claims to reserve at the modal value Not initially clear which of a group of similar claims will become large so choose the same mostly likely value for all Modal value likely under attachment point of treaty so reinsurer does not learn about claim until it is known to be large, which may be several years after reported to primary insurer Problem 3: Reporting Pattern Differ Greatly Pattern differs by: LoB, contract type, contract terms, cedant, and intermediary Extremely heterogeneous exposures Extreme fluctuation in historical loss data due to low frequency and report lags Less information on the underlying exposure than primary carrier Problem 4: Industry Statistics Not Useful Due to heterogeneity exposures Sch P doesn’t break down the exposure fine enough and ISO not directly applicable Reporting lag grows with attachment point Problem 5: Reports Received Lack Important Information Proportional covers require only summary claims info Might only have UY or CY info (no AY or PY) Limitation of reinsurance premium by primary LoB as an exposure base Reinsurance premium is assigned to line according to a % breakdown estimate made at the beginning of the contract and based upon the distribution of subject premium by line To the degree that these % do not accurately reflect the reinsurer’s loss exposure by primary line, comparisons of premiums and losses by line maybe be distorted Mostly a problem for multi-line proportional treaties Reporting is typically done with a quarter lag so always missing a quarter of premium Problem 6: Data Coding and IT System Problems Again due to the heterogeneity in coverage Might have grown in size and complexity faster than data system could handle Problem 7: Reserve to Surplus Ratio is Higher for Reinsurer More of a management problem Management may underestimate level of reserves need (due to problem 1-6) until claims emerge. Difficulty with actuary convincing management to post the appropriate reserve Standard techniques requires: Homogeneous book, sufficient frequency, and detailed exposure info (Problems 3, 4, 5); Difficult to supplement with industry info Side note on tax impact, reinsurer liabilities are much longer tail than primary and there for the discounted liabilities are smaller and will incur greater income tax "],
["patrik-res-comp.html", "13.2 6 Components to a Reinsurance Loss Reserve", " 13.2 6 Components to a Reinsurance Loss Reserve 6 components of a reinsurer’s loss reserve Case reserve reported by cedent Can individual case reports (XS contracts) or reported in bulk (proportional contracts) Additional Case reserve from reinsurer (ACR) From reinsurer’s claim department’s review of individual case reserve reports May vary considerably by contract and cedant IBNER Pure IBNR Discount for future investment income For tabular discount as well as for tax purposes Tabular discount is on (WC permanent total, auto PIP annuity claims and medical PL) Risk Load For adverse deviation so uncertain income doesn’t flow into profits too quickly Can be implicitly built into the reserving assumptions or explicitly "],
["reinsurance-reserving-procedure.html", "13.3 Reinsurance Reserving Procedure", " 13.3 Reinsurance Reserving Procedure Partition \\(\\Rightarrow\\) Development Patterns \\(\\Rightarrow\\) Estimate \\(\\Rightarrow\\) Monitor and AvE Partition into homogeneous exposure groups that are consistent over time with respect to mix of business Analyze historical development; Ideally, consider case and emergence of IBNR claims separately Estimate future development; Ideally, estimate IBNER and pure IBNR separately Monitor and testing predictions (AvE) 13.3.1 Step (1): Portfolio Partition Data needs to be split into reasonably homogenous exposure groups that are relatively consistent over time with respect to mix of business (exposures) Group policies with similar report lags and development profile Variables to consider when partitioning data in approximately priority order: LoB Contract type (fac, treaty, finite) Types of Cover (QS, SS, XS per risk/occ, agg XS, CAT, LPT) Primary LoB (for cas) Attachment Point (for cas) Contract terms (flat-rated, retro rated, sunset clause, LAE share, CM, Occ) Type of cedant (Small, large, E&amp;S) Intermediary (brokers) Figure 13.1: Portfolio Partition Example: Start with the LoB or some other major categories Further breakdown into fac or treaty All significant XS exposure should be further broken down by type of retention (e.g. per-occ XS vs agg XS) Treaty casualty XS should be further broken down by attachment point range and primary LoB (e.g. AL, GL, PL, WC, etc) Treaty casualty proportional should be broken down by first dollar primary layers (group-up) vs higher excess layer (excess) Do we also breakdown by primary line? Facultative casualty: breakdown by automatic primary programs (pro rata share of group-up exposure) vs automatic nonprimary programs (excess) Certificate exposure should ideally be split by attachment point range Or at least buffer vs umbrella layer and then further by primary line For property and other exposure: split by pricing categories Need to balance homogeneity with credibility of data Review loss statistics and reporting pattern to see if data is still credible Leverage u/w-er, claim handlers and data processors to determine which variables are the most important 13.3.2 Step (2) &amp; (3): Analyze Historical Data and Projection Backward looking of historical data (Step 2): Analyze the historical development patterns. If possible, consider individual case reserve development and the emergence of IBNR claims separately Use long periods where practical and curve fit where practical Forward looking and projection (Step 3): Estimate the future development. If possible, estimate the bulk reserve for IBNER and pure IBNR separately 13.3.2.1 Claim Report and Payment Lags We want to find stable expected development pattern for homogeneous categories Definition 13.1 \\(y = Rlag(t) = p_k\\) (\\(p_k\\) is the Clark notation) Report lag \\(= \\dfrac{1}{ATU \\text{at time }t} =\\) % Reported to Date Remark. Benefits of the above definition: We can fit a parametric curve (see Clark) to smooth curves to compute the tail e.g. Gamma with \\(\\mu\\) = 2.5 years and \\(\\sigma\\) = 1.5 years (Figure 13.2) \\(Rlag(t)\\) can be interpret as probability that any particular claims dollar will be reported to the reinsure by time \\(t\\) We can compute statistics such as the expected value to compare one claim report pattern with another ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Figure 13.2: Claims report lag based on Gamma distribution 13.3.2.2 Short Tailed Lines Loss reporting and settlement is quick \\(\\therefore\\) Use method that provide reasonable accuracy for the least effort and cost Table 13.1: Reinsurance categories that are usually short-tailed Category I1 Category II Category III Property Treaty Proportional Property Treaty Cat Property Treaty Excess2 Property Facultative3 Fidelity Proportional For property, beware of recent cat events Exclude high layers for XS property Exclude construction International exposure can have significant reporting delay as well Estimation Methods Set IBNR = % of the last 12 months’ EP Reasonable for non-major-cat “filtered” claims Claims for major cat may not be fully reported and finalized for years even on proportional covers Book LR for new lines Allocate claims to AY for reserving (based on historical information) U/w year (or treat year) will have extra lag Treaty effective 1/1/16 will cover policies inception from 1/1/16 to 12/31/16 and therefore can cover accidents that happens in 12/31/17 \\(\\Rightarrow\\) 2 years after the inception of the treaty 13.3.2.3 Medium Tailed Lines Definition 13.2 Medium-taild exposure Average dollar lag of 1-2 years Almost completely settled within 5 years Over half the ultimate losses are reported within 2 years Table 13.2: Reinsurance categories that are usually medium-tailed Category I Category II Category III Category IV Property Treaty Excess High layers1 Construction2 Surety Fidelity Excess Ocean Marine Inland Marine Property International Non-casualty Excess Aggregate3 Ideally separate from working layers Ultimate value may not be known immediately and will take longer to penetrate higher per-risk XS attachment point Ideally separate from other property exposures Discovery period can extend years beyond the contract period Lags are longer then the underlying exposure For surety should estimate gross and recovery separately as the recovery has a longer tail Consider the ratio of salvage to gross loss for mature years Estimation Methods Chainladder works fine for paid and incurred with or without ACR 13.3.2.4 Long Tailed Lines Definition 13.3 Long-taild exposure Exposures which the average aggregate claims dollar report lag is over 2 years Claims are not settled for many years Table 13.2: Reinsurance categories that are usually long-tailed Category I Category II Category III Category IV Casualty Treaty Excess1 Casualty Treaty Proportional2 Casualty Facultative Casualty Excess Aggregate3 APH4 Longest lags except for APH Some of this exposure maybe medium-tailed Lag are longer than for the underlying exposure Asbestos, pollution, and other health hazard and mass tort claims Longest of the long tail Should be remove from data and analyzed separately Use methods specifically for APH Exclude claims from commuted contracts as they distort development patterns Need to separate the above into finer, more homogeneous categories (e.g. separate claims-made and occurrence coverage) 13.3.2.4.1 Long Tailed Lines: Estimation Methods Chainladder (CL): Don’t use, too much leverage Bornhuetter-Ferguson (BF): Better than CL, correlate future development with an exposure measure (reinsurance premium): Premium \\(\\times\\) ELR Caveat 1): Very dependent on selected LR LR for a given AY is strongly correlated with the underwriting cycle as well as the reported loss to date \\(\\therefore\\) need to consider the u/w cycle in ELR pick (May use LR from pricing work) Caveat 2): Estimate for each AY does not reflect the reported to date (unless the selected LR is chosen with that in mind) Stanard Buhlmann (SB or Cape Cod in Europe): \\[ELR = \\dfrac{\\sum C_k}{\\sum E_k p_k}\\] \\(C_k\\): reported to date \\(E_k\\): Adj Prem \\(p_k\\): expected % reported to date \\(k\\) is the exposure year Must on-level the historical premium and adjust to be pure premium (\\(E_k\\)) Remove commissions and brokerage and internal expenses (but might not worth the effort) Remove any suspected rate level differences (so each year have the same ELR) We need to do this because we are using a constant expected LR for all AYs Unclear how to adjust ELR to a prior estimate for each year (from 2000 #68) SB IBNR: \\[\\begin{align} IBNR_k &amp;= E_k(1-p_k) \\times ELR \\\\ &amp;= (E_k - p_k E_k) \\times ELR \\\\ \\end{align}\\] For all years \\(IBNR\\) we have \\[\\begin{align} IBNR &amp;= \\sum_k (E_k - p_k E_k) \\times ELR \\\\ &amp;= \\left( \\sum_k E_k - \\sum_k p_k E_k \\right) \\times ELR \\\\ &amp;= (\\text{Total Adj Prem} - \\text{Total Used Up Prem}) \\times ELR \\\\ &amp;= \\text{Premium Not Used} \\times ELR \\\\ \\end{align}\\] Remark. This estimate of the ELR use actual incurred loss Method is sensitive to the accurary of the on-level EP Total used up premium is something we’ll already have from calculating the \\(ELR\\) Credibility IBNR Estimates Weight the CL method with SB or BF Reserve (or IBNR) for year \\(k\\): \\[\\hat{R}_k = Z_k \\times R^{CL}_k + (1-Z_k)\\times R^{SB}_k\\] \\(Z_k = p_k \\times CF\\); Credibility Factor \\(CF \\in [0,1]\\) \\(CF = 0\\) \\(\\Rightarrow\\) BF \\(CF = 1\\) \\(\\Rightarrow\\) Benktander (GB) Can also weight together the IBNR estimate from paid loss method Case reserve are based on judgement of many people and can vary over time (lack consistency) \\(\\Rightarrow\\) Given long enough paid history, paid estimate might be more stable and credible Or weight with BF based on different a-priori (e.g. based on pricing ELR) Alternative Methods Fit losses reported to date to a curve (Clark) Practical problem: Negative incrementals (get around by grouping over different periods) Model claim counts and use stochastic model (Bootstrap) for the whole claims development process Caveat: difficult to explain to management Advantage: Can contemplate intuitively satisfying models for various lag distributions Time from loss to first report and lag from report to settlement Connect the lags with appropriate models for the dollar reserving and payments on individual claims up through settlement 13.3.3 Step (4): Monitoring and Testing Predictions Compare AvE over quarters and do this for several quarters to see if any trends become apparent Forecast IBNR runoff over the quarters Provides early warning if claims are emerging higher than expected Definition 13.4 Expected reported claims \\(\\dfrac{\\text{Expected % Reported in Period}}{\\text{Expected % Unreported}} \\times IBNR\\) Caveat: Can be difficult to tell if the AvE difference is large or small \\(\\therefore\\) Look for deviations over time and look for trends Deviation could be IBNR is too small, claims are paying faster than expected, or just random fluctuation "],
["past-exam-questions-12.html", "13.4 Past Exam Questions", " 13.4 Past Exam Questions Past question mostly focus on: The 7 problems of reinsurance loss reserving Using the SB and credibility IBNR method Concepts 1996 #10: Problems of reinsurance reserving 1999 #9: Partition consideration 1999 #10: Fidelity proportional is short tail 2000 #68 b c: SB incorporates reported loss in the ELR but difficult to adjust ELR to a prior estimate for each year 2001 #54: pros and cons of CL and BF 2002 #42: SB vs BF similarity and difference 2004 #49 c d: SB vs BF pros and cons 2006 - #45: Which method to use SB vs CL 2007 - #21: Discuss how SB is a special case of BF 59 \\(\\star\\) 2008 - #35: Reason of report lag and upward development due to modal value claim reserve \\(\\star \\star\\) 2008 - #37: Industry data vs internal Industry: thin internal data; volatile from rate changes; rate change history not credible; shift in underlying policies so historical no reliable Internal: Rate change not reflected in industry; industry composition different from reinsurer; Underlying policies not represented in Sch P data; believe data is credible and representative of the specific business involved \\(\\star \\star\\) 2009 - #32: How to partition data Fac vs treaty \\(\\Rightarrow\\) LoB; Don’t split by cedant as data too thin Diagnostics: Look at reporting pattern and amount of data for credibility; discuss with u/w and claims 2011 #10 b: SB pros and cons 2011 #11: report lag; upward development; standard method not work; industry data not work 2012 #10 b-d: SB vs CL 2012 #11: problems of reinsurance reserving 2013 #10 c: SB vs CL 2014 #13: problems of reinsurance reserving \\(\\star\\) 2015 #12: Discuss problems of reinsurance reserving based on data given Reporting lag (claims below reporting threshold) Persistent upward development (upward development of ALAE by looking at ALAE/Loss ratio) Heterogeneity patterns (each AYs have different retention) 2015 #13 b: Reason to adjust EP for SB (e.g on-level so LR for each AY is comparable, adjust for varying rate as SB assumes constant ELR) 2016 #17: 2 of the 7 problems with reinsurance reserving Describe the cause of each \\(\\star\\) TIA 2: LoBs groupings considerations Calculations 1997 #11: SB (Use adjusted on-level premium) 1998 #35: IBNR with CL, BF, SB 2000 #68 a: SB 2001 #53: SB 2003 #44: SB 2004 #49 a b: CL and SB 2005 #38: SB \\(\\star\\) 2007 - #3: Credibility weight SB and CL, remember to use \\(Z_k = p_k \\cdot CF\\) 2007 - #22 a b: CL and SB 2008 - #36: SB 2009 - #31: SB \\(\\star\\) 2011 #10 a c: SB and credibility weighted Shorthand on getting SB IBNR by \\(ELR \\times (\\text{total adjusted premium} - \\text{total used up premium})\\) \\(\\star \\star\\) 2012 #10 a: SB method, need to consider age of policy on risk attaching vs loss occurring basis, also take out expense from premium 2013 #10 a b: SB and credibility weighted SB and CL 2014 #14: SB 2015 #13 a: SB 2016 #16: SB Credibility weighting TIA 1: Standard Buhlmann and credibility IBRN $star$ Remember to use the on-leveled premium and remove commissions and brokerage and internal expenses (So pure premium) \\(\\star\\) credibility factor is applied to the \\(p_k\\) 13.4.1 Question Highlights -->"],
["estimating-the-premium-asset-on-retrospectively-rate-policies-m-teng-and-m-perkins.html", "Chapter 14 Estimating the Premium Asset on Retrospectively Rate Policies - M. Teng and M. Perkins", " Chapter 14 Estimating the Premium Asset on Retrospectively Rate Policies - M. Teng and M. Perkins Advantages of retro rating plan Retro formula: (14.1) Retro rating formula approach \\(\\star\\) PDLD formulas (14.4), (14.5), and (14.6) Basic premium factor vs charge (charge is after tax) Understand the shape and pattern of \\(PDLD\\) Empirical approach Assume premium lags (typically 9 month) Ratio selection \\(\\star \\star\\) Cumulative PDLD (14.7) and calculating it recursively (14.8) Practical application First adjustment period might cover more than one policy period Teng Perkins improvements and assumptions Understand pattern of the PDLD Feldblum’s adjustment to the empirical \\(CPDLD_1\\) (14.9) "],
["intro-teng-perk.html", "14.1 Introduction", " 14.1 Introduction Definition 14.1 Premium Asset (3 definitions) On retrospectively rate policies, premium that the insurer expects to collect based on the expected ultimate loss experience less the premium that the insurer has already booked, or Earned But Not Reported Premium (EBNR) ,or Asset supported by premium the insurer expect to collect from clients with retrospective policies as losses develops Remark. Admitted portion of the premium asset appears on the balance sheet as the “Asset for Accrued Retrospective Premiums” Background: Retrospective policies were popular in 1996: For insured: Returns premium to insured for good experience Cashflow: Premiums are due with short lag to losses, as opposed to upfront For insurer: Allows insurer to attracts good customers that expect to see favorable loss experience through loss control and loss management Allows an insurance company to shift a significant portion of the risk to the insured (Uncertainty due to inflation, rate regulations, uncertainty in claims compensation, increase utilization of the insurance benefits, growing attorney involvement) Estimating the premium asset was needed for many commercial lines insurers as this asset frequency exceeds 10% of surplus 14.1.1 Discussion of Berry and Fitzgibbon Method Method for calculating retro reserve [Premium deviation to date] - [Ultimate premium deviation] Definition 14.2 \\[\\dfrac{\\text{Ultimate Premium}}{\\text{Standard Premium}}\\] Ultimate Premiums Deviation: Amount by which the ultimate premium for a retro rated policy is expected to differ from the standard premium (manual premium w/ adj for experience) Premium deviation to date: Amount by which the currently booked premium differs from the standard premium Fitzgibbon Method: Analyze the historical relationship between the loss ratio and the premium deviation using statistical techniques Apply the relationship to the projected loss ratio to calculate the projected ultimate premium deviation Retro reserve = [Ultimate premium deviation] - [Premium deviation to date] Berry Method: Estimate ultimate premium with historical premium emergence pattern Subtract estimated ultimate premium by current premium Caveat: Lack intuitive appeal Methods do not consider how a retro rating formula actually works i.e. premium is a function of loss, with retro rating parameters such as the loss conversion factor, tax multiplier, retro min/max \\(\\therefore\\) Premium asset should be establish as a function of reported loss and the reserve for loss development Where the function is defined by the retro rating parameters "],
["teng-perkin-methods-overview.html", "14.2 Teng Perkin Methods Overview", " 14.2 Teng Perkin Methods Overview Key idea: Premium asset ~ Current losses + Expected future loss emergence + Retro rating parameters Look at how premiums develop as losses develop Ratio of premium development to loss development (PDLD ratio) Ratio can be based on retro rating parameters or historical development pattern (which leads to the 2 methods) The 2 methods discussed here estimate premium asset over a whole book of business Framework based on retro rating formula (requires knowledge of rating parameters for the entire book) Estimating parameters based on empirical data (historical development) Once the relationship is established, we can apply it to the expected future loss development to get the expected future premium development Sum of all future premium development is the premium asset "],
["formula-approach-for-pdld.html", "14.3 Formula Approach for PDLD", " 14.3 Formula Approach for PDLD First method, based on retro rating parameters Formula requires details about the policies written Reasonable for individual policies but difficult for an entire book 14.3.1 Retro Premium Formulas \\[\\begin{equation} P = \\left[BP + \\left(CL \\times LCF\\right)\\right] \\times TM \\tag{14.1} \\end{equation}\\] \\(P\\): Ultimate premium \\(BP\\): Basic Premium Covers minimum premium, commissions and expenses; premium when there are no losses \\(CL\\): Capped Losses Losses are capped from premium calculation, individually and in aggregate \\(LCF\\): Loss Conversion Factor Variable expense loading \\(TM\\): Tax Multiplier For premium tax The above (\\(P\\), \\(BP\\), \\(CL\\)) can be expressed in \\(\\propto\\) Standard Premium Standard Premium = Manual Rate \\(\\times\\) E-Mod \\(\\times\\) Sch-Mod = \\(\\dfrac{\\text{Expected Loss}}{\\text{ELR}}\\) 14.3.1.1 Premium Adjustment Polices can allow adjustments up to 6 years Typically 1st adjustment is based on losses 18 months after inception of the policy Further adjustments every 12 months Same formula (14.1) is used to calculate the cummulative premium due @ each adjustment but with the below: \\(P_n\\): Cumulative premium @ nth adjustment \\(CL_n\\): Capped losses @ nth adjustment (This is the loss to date with some capping on large losses) Remark. Disadvantage for the insurer since the client doesn’t finish paying their premium for several years No loss development is baked into formula (14.1) so the premium estimate at the first several valuations are likely too low 14.3.2 Calculating PDLD \\[\\begin{equation} PDLD = \\dfrac{\\Delta P}{\\Delta L} \\sim \\dfrac{d P}{d L} \\tag{14.2} \\end{equation}\\] Relationship between premium development and loss development Apply to expected loss development to determine the expected premium development \\(L\\) here is not capped \\[\\begin{equation} PDLD_n = \\dfrac{P_n - P_{n-1}}{L_n - L_{n-1}} \\tag{14.3} \\end{equation}\\] 14.3.2.1 Estimating First Period PDLD First adjustment includes Basic Premium (\\(BP\\)) \\(\\Rightarrow\\) Requires different treatment from the other adjustments \\[\\begin{equation} PDLD_1 =\\underbrace{\\left(\\frac{BP}{SP} \\right)}_{\\text{Basic Prem Factor}} \\frac{TM}{ELR \\cdot \\%Loss_1} + \\underbrace{\\left( \\frac{CL_1}{L_1}\\right)}_{\\text{Loss Capping Ratio}} \\cdot LCF \\cdot TM \\tag{14.4} \\end{equation}\\] Loss capping ratio is estimated base on the actual losses and the structure of the contract The formula above (14.4) is too responsive to actual losses in the first period, as only a portion of the formula is related to the \\(L_1\\) as shown below \\[\\begin{equation} P_1 = \\underbrace{\\left( \\frac{BP}{SP} \\right) \\frac{TM}{ELR \\cdot \\%Loss_1}}_{\\text{Not }\\propto\\text{ Loss}_1} \\times \\operatorname{E}[L_1] + \\underbrace{\\left( \\frac{CL_1}{L_1} \\right) \\cdot LCF \\cdot TM}_{\\propto \\text{ Loss}_1} \\times L_1 \\tag{14.5} \\end{equation}\\] So technically only the 2nd part of the formula should be applied to \\(L_1\\) while the first part is in relation to the \\(\\mathrm{E}[L_1]\\) Note that \\(SP = \\dfrac{\\text{Expected Loss}}{ELR}\\) Use this formula to address the issue of over responsive Remark. Basic premium factor vs charge Basic Premium Factor = \\(\\dfrac{BP}{SP}\\) Used by Teng Perkins Basic Premium Charge = \\(\\dfrac{BP}{SP} \\cdot TM\\) Used by Feldblum 14.3.2.2 Estimating Subsequent PDLD \\[\\begin{equation} \\begin{array}{cccl} PDLD_n &amp;= &amp;\\left(\\dfrac{CL_n - CL_{n-1}}{L_n - L_{n-1}}\\right) &amp;\\cdot \\: LCF \\cdot TM \\:\\:\\:\\:\\text{For }n&gt;1\\\\ &amp;= &amp;\\left(\\dfrac{\\Delta CL}{\\Delta L}\\right) &amp;\\cdot \\: LCF \\cdot TM\\\\ \\end{array} \\tag{14.6} \\end{equation}\\] \\(LCF\\) and \\(TM\\) are known at policy inception so we only have to estimate the loss capping ratio \\(\\dfrac{\\Delta CL}{\\Delta L}\\) \\(LCF\\) is smaller at older ages (since more large losses developed and capped) This equation is basically the 2nd part of (14.4) "],
["empirical-approach-for-pdld.html", "14.4 Empirical Approach for PDLD", " 14.4 Empirical Approach for PDLD Advantages: Easier to do for an entire book of business as not every client will have the same parameters (e.g. \\(\\frac{BP}{SP}\\), \\(LCF\\), \\(TM\\), \\(LCF\\)) Book should be separate into homogeneous groups Size of account and type of rating plan Need to assume a lag from the date used to value the losses and when the actual premium is collected Typically 3 to 9 months e.g. if the loss age is 18, we want to look at the premium at 27 if we assume a 9 months lag First evaluation for loss is typically at 18 months e.g. \\(PDLD_2 = \\dfrac{P_2 - P_1}{L_2 - L_1} = \\dfrac{^{39}P - {^{27}P}}{^{30}L - {^{18}L}}\\) 14.4.1 PDLD Ratios Selection Compile PDLD triangle for all effective date groupings and make a selection for each age If you observe increasing trends in the ratios for a give age: More losses are within the loss capping layer, can be due to: Higher agg max or lower agg min or higher per claim limits Improvement in loss experience (fewer large losses so larger portion is within the cap) Higher basic limits (for \\(PDLD_1\\)) PDLD can be negative if \\(\\Delta CL\\) is negative: Losses &gt; max increase while claims within retro limit have a reduction in reserves 14.4.2 Cumulative PDLD Ratios Cumulative PDLD based on a weighted average with expected % future report for each future periods \\[\\begin{equation} CPDLD_d = \\dfrac{\\sum_{i=d}^{\\infty} PDLD_i \\times \\text{% Reported in period i}}{\\sum_{i=d}^{\\infty} \\text{% Reported in period i}} \\tag{14.7} \\end{equation}\\] If you need the CPDLD for every PYs, it is best done starting with the oldest year and recursively up to the most recent year \\[\\begin{equation} CPDLD_d = \\dfrac{ CPDLD_{d-1} \\times \\text{% rpt&#39;d in period}_{d-1} + PDLD_d \\times \\text{% rpt&#39;d in period}_d }{\\text{% rpt&#39;d in period}_{d-1} + \\text{% rpt&#39;d in period}_d} \\tag{14.8} \\end{equation}\\] Or us calculator data table to calculate \\(\\dfrac{\\sum xy}{\\sum y}\\) adding one pair at a time 14.4.3 Application For each period: Use losses reported to date to estimate ultimate losses Subtract losses reported at prior retro adjustment date to get \\(\\Delta L\\) Multiply by CPDLD to get \\(\\Delta P\\) For each quarter Determine reported losses @ prior adjustment (given) Estimate ultimate losses \\(\\Delta L\\) = (2) - (1) \\(\\sum \\limits_{qtr \\in last \\: adj \\: age \\: i} \\Delta L_{qtr}\\) i.e. group the quarters by the age of next adjustments they’re waiting for For each year Expected premium emergence = \\(\\Delta P = CPDLD \\times \\Delta L_{yr}\\) Determine premiums booked through prior adjustment (given) Estimated total premium = (1) + (2) Premium asset = (3) - Premium booked as of current evaluation Remark. Premium booked as of current eval \\(\\neq\\) premium booked through prior adjustment due to: Timing of retro adjustments Minor premium adjustments Interim premium booking Negative premium assets means returning premium to client For the most recent periods need to adjust for only exposure period that is being earned out Pro rate your ultimate losses by % earned premium 14.4.4 Further Issues If loss plan includes ALAE, you should include ALAE in the losses for this as well \\(\\Delta\\) in mix of business will affect the sensitivity of premium to loss and thus the \\(PDLD\\) For premium that is not secured, a provision for bad debt should be held "],
["feldblums-discussion.html", "14.5 Feldblum’s Discussion", " 14.5 Feldblum’s Discussion We do not develop premium directly because: Estimate of ultimate inc’d losses can be obtained sooner Retro premium depends on incurred loss Both Fitzgibbon and PDLD are based on the retro rating formula (with some difference) Both methods estimate the parameter on the data, downside: Unable to look up retro parameters since this is done for the whole book of business Max/min will be different for account \\(TM\\) is different across state lines \\(\\therefore\\) Easier to do a regression using empirical data to estimate the parameters Fitzgibbons and Berry Also create a linear relationship of premium to losses, but they forecast ultimate premium directly, rather than premium development \\(\\Rightarrow\\) can lead to large deviations Premium asset \\(\\propto\\) expected unreported losses Caveat: Does not respond if the actual premium to loss relationship is different than estimated Berry’s solution is to give less weight to this method as the year matures and give more weight to his DR2 method 14.5.1 Improvements from Teng &amp; Perkins Slope of premium to loss changes (reduces) as the year matures Large losses that pierce the claim cap tend to have their development later No future development one aggregate max is hit Forecast only future premium development Projected premium asset is based on projected unreported losses, does not consider losses to date Errors in projecting premium to date are automatically corrected for since it projects premium development and not ultimate premium 14.5.2 Teng &amp; Perkins Assumptions Premium responsiveness \\(\\dfrac{\\Delta P}{\\Delta L}\\) in a period is \\(\\perp\\!\\!\\!\\!\\perp\\) of responsiveness at prior adjustments Premium responsiveness depends on the maturity, not the beginning loss ratio or beginning retro premium ratio This is superior to Fitzgibbons since it is projecting only future development 14.5.3 Enhancement \\(PDLD_1\\) should separate the basic premium component from the component \\(\\propto\\) losses Simple adjustment, subtract out the fixed component of \\(CPDLD_1\\) Fixed component of \\(CPDLD_1\\): \\[\\begin{equation} \\dfrac{P_{fixed}}{\\operatorname{E}[L]} = \\underbrace{\\dfrac{BP}{SP} \\cdot TM}_{\\text{Basic Prem Charge}} \\Big / ELR \\tag{14.9} \\end{equation}\\] Based on \\(P_{fixed} = BP \\cdot TM\\) and divided by \\(\\mathrm{E}[L]\\) We calculate \\(P\\) as the fixed component \\(\\times\\) \\(\\mathrm{E}[L]\\) and add the variable component \\(\\times\\) \\(L\\) Fixed component and variable components are what? This can help better understand changes in the \\(PDLD_1\\) ratio "],
["past-exam-questions-13.html", "14.6 Past Exam Questions", " 14.6 Past Exam Questions Concepts 1998 - #55: formula vs empirical, expected difference given a scenario 2001 - #39 b: PDLD ratio pattern First adjustment the basic premium is included in the retro premium computation; Small portion of loss is limited by the retro; LCF and TM also impacts it At later periods, more losses are capped by the limit 2003 - #9: Loss capping ratio properties Double check 2003 - #10: Feldblum comment on Teng Perkins: premium responsiveness decreases at higher LR for a book of business 2007 - #36: Retro premium and reported losses relationship on graph \\(\\star\\) 2014 #12 14.4: incremental losses vs premium relationships \\(\\star\\) 2015 #14 14.5: PDLD ratio TIA 2b: Difference in the first period Calculations \\(\\star\\) 1999 - #55 14.1: Calculating CPDLD and expected future premium Remember to apply \\(CPDLD_1\\) to all policies that have not have an adjustment 2001 - #39 a: CPDLP 2002 - #14: Premium asset = Expected Future Premium + Premiums booked from prior adjustment - premium booked as of current 2004 - #7: Empirical PDLD ratio 2005 - #20: Premium asset calc with PDLD \\(\\star\\) 2006 - #23: \\(PDLD\\) from formula and why the loss capping ratios decrease over time 2007 - #5: \\(PDLD_2\\) plug and play 2008 - #14: premium asset \\(\\star\\) 2009 - #6 14.2: Premium after 2nd retro adjustment \\(\\star \\star\\) 2012 - #7 14.3: Retrospective premium asset, good question, 2 methods to do Can calculate a weighted capping ratio \\(\\star\\) 2013 #6 a-c: PDLD 1 to 3 and relationship with loss capping ratio, ratio should decrease monotonically State whether you using the loss capping ration as cumulative or incremental 2015 #14 a: PDLD 1 and 2 plug and play \\(\\star \\star\\) 2016 #18: quarter calculation premium asset TIA 1: Retro formula plug and play TIA 2a: plug and play \\(\\star\\) TIA 3: PDLD and CPDLD and practical application \\(\\star\\) TIA 4: CPDLD for all AYs and premium asset \\(\\star \\star\\) TIA 5: Premium asset for quarterly eval TIA 6: PDLD calc given loss and premium \\(\\star\\) TIA 7: CPDLD, using the enhancement 14.6.1 Question Highlights Figure 14.1: 1999 Question 55 Figure 14.1: 1999 Question 55 Figure 14.2: 2009 Question 6 Figure 14.2: 2009 Question 6 Figure 14.3: 2012 Question 7 Figure 14.3: 2012 Question 7 Figure 14.4: 2014 Question 12 Figure 14.4: 2014 Question 12 Figure 14.5: 2015 Question 14 Figure 14.5: 2015 Question 14 "],
["pc-insurance-company-valuation-r-goldfarb.html", "Chapter 15 P&amp;C Insurance Company Valuation - R. Goldfarb", " Chapter 15 P&amp;C Insurance Company Valuation - R. Goldfarb Discount rate CAPM (15.1) and it’s components \\(\\star\\) Growth rate Table 15.1 for each of the 3 method and it’s components DDM: DDM formula (15.2) Terminal value formula uses CF at 1 and gets you to time 0 Select \\(g\\) by average of \\(ROE\\) (usually \\(\\rho\\) is given) If not then select \\(\\rho\\) too by average of \\(\\rho = 1 - \\dfrac{\\mathrm{E}[Div_1]}{NI}\\) FCFE: FCFE formula (15.3) Discount all the FCFE for \\(V_0\\) Select \\(g\\) by average of \\(ROE\\) and reinvestment rate Advantages FCFE vs FCFF AE: AE formula (15.4) and (15.5) Remember to add the \\(BV_0\\) Clean surplus assumption Advantages Considerations Relative multiples: Price:Earning (15.6) Price:Book (15.7) Transaction multiple and its weakness Option Pricing Merton Real options "],
["assumptions.html", "15.1 Assumptions", " 15.1 Assumptions Key assumptions are cost of captial \\(k\\) and growth rate \\(g\\) It is good to sensitivity test these assumptions on the model output 15.1.1 Risk Adjusted Discount Rate Recognize the risky cashflow by discounting them at a rate higher than the risk-free rate based on CAPM \\[\\begin{equation} k = r_f + \\beta \\left [ \\mathrm{E}(r_m) - r_f \\right ] \\tag{15.1} \\end{equation}\\] Risk of an investment depends on the rest of an investor’s portfolio. We focus instead on equilibrium rates of return Different BU has different risk profile \\(\\Rightarrow\\) Different discount rates Discount rates can vary by period if business mix change Cash flow can have the different risk profile (premium, investment income, paid losses) Simplification is to use average discount rate for the portfolio Alternative way to account for the risky cash flow is to convert the cash flow to certainty equivalent cash flows and discount with risk free instead of the cost of capital Reflect the risk in the cashflow directly Risk free Rate: \\(r_f\\) 90 days t-bill Maturity matched t-notes 20 years T-bonds less liquidity &amp; term premium (~1.2%) Market Risk Premium: \\(\\mathrm{E}(r_m) - r_f\\) 6-8% historically \\(r_f\\) here should be consistent with the one use for CAPM Arithmetic average should be used when forecasting over 1 year (geometric average when forecasting over multiple years) Need to sensitivity test Systematic Market Risk: \\(\\beta\\) Based on regression on stock return vs market return Using industry \\(\\beta\\) Mix of business needs to be similar to industry Industry \\(\\beta\\) should be adjusted for differences in the industry leverage and company leverage \\(\\beta\\) will be higher for firms with more leverage, riskier business units Alternative is to use all equity \\(\\beta\\) to remove bias from leverage Higher growth should have higher \\(\\beta\\) Insurance company has additional leverage from policyholder liabilities Can assume total leverage of insurance companies is similar 15.1.2 Growth Rate Used for the period after the forecast horizon Table 15.1: Growth rate for each method Method Growth Rate: \\(g\\) DDM \\(ROE \\times \\rho\\) FCFE \\(ROE \\:\\: \\times\\) [Reinvestment Rate] AE At most the growth in book value Return on Equity: \\(ROE\\) \\(\\dfrac{NI}{BE} = \\dfrac{\\text{Net Income after Tax}}{\\text{Beginning Equity}}\\) Plowback Ratio: \\(\\rho\\) % of \\(NI\\) that is reinvested in the firm (e.g. dividend payout ratio) Company that have high growth should retain more earnings Reinvestment Rate: \\(\\dfrac{\\Delta Capital}{NI}\\) "],
["goldfarb-ddm.html", "15.2 Dividend Discount Model (DDM)", " 15.2 Dividend Discount Model (DDM) Value of stock under DDM \\[\\begin{equation} V_0 = \\dfrac{\\mathrm{E}[Div_1]}{k - g} \\tag{15.2} \\end{equation}\\] \\(Div_1\\) is paid at the end of year 1 Constant growth assumption where \\(\\mathrm{E}[Div_i] = \\mathrm{E}[Div_0] \\cdot (1+g)^i\\) \\(\\mathrm{E}[Div_1] = (1 - \\rho) NI\\) Remark. Typically forecast a few years and use the above formula for the terminal value Need to use \\(NI\\) after tax When calculating \\(g\\), calculate \\(ROE\\) and \\(g\\) for all years and make selection Firm with high expected growth tend to be riskier \\(\\Rightarrow\\) Higher discount rate Forecast is more susceptible to being wrong, so should be discount more? DDM Assumptions Expected dividends (however they are discretionary) Dividend growth rate (from table 15.1) Risk-adjusted discount rate (From CAPM) "],
["goldfarb-fcfe.html", "15.3 Free Cash Flow to Equity (FCFE)", " 15.3 Free Cash Flow to Equity (FCFE) Discount the stream of free cash flow available to pay shareholders: \\[\\begin{equation} \\begin{array}{l} FCFE = &amp; &amp;NI &amp;\\cdots \\:\\: (1) \\\\ &amp;+ &amp;Non \\:Cash\\:Charges &amp;\\cdots \\:\\: (2) \\\\ &amp;- &amp;\\Delta Working \\:Capital &amp;\\cdots \\:\\: (3) \\\\ &amp;- &amp;\\Delta Capital &amp;\\cdots \\:\\: (4) \\\\ &amp;+ &amp;\\Delta Debt &amp;\\cdots \\:\\: (5) \\\\ \\end{array} \\tag{15.3} \\end{equation}\\] FCFE is free cash flow after: Paying interest on debt Tax benefits of the interest payment Net borrowings \\(NI\\) is net of interest payments to shareholders, after tax \\(\\Delta\\) loss reserve reflected in \\(NI\\) only (It gets net out as non-cash charges and capital expenditures assuming changes in reserve does not impact required capital) \\(\\Delta\\) Working Capital is negligible for P&amp;C insurer This is change in required capital Growth rate From table 15.1 \\[g = \\underbrace{\\dfrac{\\text{Net Income after Tax}}{\\text{Beginning Equity}}}_{ROE} \\times \\underbrace{\\dfrac{\\Delta Capital}{NI}}_{\\text{Reinv. Rate}}\\] Assume portion of FCF not paid out is invested at \\(k\\) Similar to DDM, calculate the \\(ROE\\) and reinvestment rate for all years and make a pick Free Cash Flow Cash flow available to pay out to the firm’s source of capital (for FCFE this is only to equity) net of amounts required to be reinvested to the firm for growth Weakness: require forecasting financial statements, use adjusted accounting measure, large terminal value FCFE vs FCFF We don’t use the free cash flow to firm because there’s additional leverage for p/h liabilities from reserves held Not clear how to calculate cost of capital due to leverage from 2 different source and it complicates the calculation Using FCFE removes the source of leverage Discount rate FCFE pays out all free cash flow while DDM only pays out a portion (rest in marketable securities) DDM has a larger portion of its risk profile from investment in securities and FCFE has a larger portion from u/w risk Should use different discount rate But difference is difficult to quantify \\(\\Rightarrow\\) Use the same discount rate Advantages over DDM Dividend are discretionary Firms also return funds via stock buybacks Focus on free cash flow "],
["goldfarb-ae.html", "15.4 Abnormal Earnings (AE)", " 15.4 Abnormal Earnings (AE) Works with accounting measures of income Need to remove distortions Some same it is more accurate Clean surplus assumption Requires all changes to book value (on the b/s) flow through the I/S (as earnings, dividends, or capital contributions/reductions) Can’t have direct changes to equity Value of Equity \\[\\begin{equation} V_0 = BV_0 + \\sum_{t=1} \\dfrac{AE_t}{(1+k)^t} \\tag{15.4} \\end{equation}\\] Abnormal Earning @ \\(t\\) \\[\\begin{equation} AE_t = NI_t - k \\cdot BV_{t-1} = (ROE_t - k)BV_{t-1} \\tag{15.5} \\end{equation}\\] Earnings (net income) XS of cost of capital Assume AE will trend to zero overtime since it’s difficult to maintain AE is difficult to maintain as competitors will see the AE and move into the market Parameters Considerations Remark. \\(BV_0\\) Reported book value Focus on tangible book value (e.g. take out goodwill) Remove any systematic bias such as over or understated reserve Remark. \\(NI\\) is net of interest payments to shareholders, after tax; Same as DCF model Make complement of the book value adjustments here e.g. any direct adjustment to the B/S that doesn’t flow from the I/S you have to adjust here If reserve is discounted in the \\(BV_0\\), need to change (lower) the \\(ROE\\) as the income will be generated from a larger capital base Remark. \\(g\\) Should be negative as AE tend to 0 Does not require additional capital as the growth from that extra capital will not accrue to today’s shareholders Remark. \\(k\\) CAPM as before Advantages Focus on value creation Earnings above the required return on capital Dividends and CF are just consequence of value creation Small terminal value as it focus on any added value so less leverage Directly using accounting measures so does not need to adjust into a cash flow measure "],
["goldfarb-relative.html", "15.5 Relative Multiples", " 15.5 Relative Multiples We don’t compare to sales (use equity) because of leverage from p/h’s liability Stock price can fluctuate so use an average price Multiples can vary significantly even over short periods of time Assumptions Constant \\(ROE\\), \\(\\rho\\), and \\(k\\) 15.5.1 Price to Earnings \\[\\begin{equation} \\dfrac{P_0}{E_1} = \\dfrac{1 - \\rho}{k - \\underbrace{\\rho \\times ROE}_{g}} \\tag{15.6} \\end{equation}\\] If \\(ROE &gt; k\\) \\(\\Rightarrow\\) Keep a high \\(\\rho\\) Proof. Formula above is based on the DDM Start with (15.2) and \\(\\mathrm{E}[Div_1] = E_1 (1-\\rho)\\) \\(g = \\rho \\times ROE\\) If the shares are priced fairly then \\(P_0 = V_0\\) Plug all the above in and we’ll have (15.6) Remark. P:E Ratio Forward or leading P/E = consensus forecast earnings for next year Trailing P/E = last year’s actual; Can be distorted by unusual events Price = value of the firm derived from any of the methods Earnings = \\(NI\\); Either forward or trailing By default, apply the ratio to next year’s earnings per formula Alternative use of P:E Validating assumptions: reasonability check on the forecast Shortcut to valuation: if you think company will grow similar to the industry Terminal value: use the other method for the forecast horizon then P/E for the terminal value 15.5.2 Price to Book \\[\\begin{equation} \\dfrac{P_0}{BV_0} = 1 + \\dfrac{ROE - k}{k - g} \\tag{15.7} \\end{equation}\\] \\(BV_0 =\\) equity @ t = 0 Useful for firms with substantial holdings in marketable securities Proof. Above formula is based on AE method Start with (15.4) and (15.5) Assume book value grow at constant rate \\(g\\) and the \\(ROE\\) is constant and we have: \\(BV_t = BV_0(1+g)^t\\) Plug in to the formulas and using the geometric series 15.5.3 Transaction Multiples Pros: Based on transaction price of knowledgeable or sophisticated parties, which makes the valuation more meaningful Not subject to random market fluctuations, should have been valued by careful analysis Transactions done by people/experts involved in the companies – would have best estimate of values Cons: Companies tend to overpay Control premium, and other reason M&amp;A overpay IPO’s are under priced Financials used to value the transaction could be different from the information being used now (forecast is different) Economic conditions @ transaction \\(\\neq\\) economic conditions now 15.5.4 Relative Valuation for Multi-line firms Use multiples from pure play peers (monoline firms) to estimate by division Or compare multiples from diversified insurers Choose firms with similar business, \\(ROE\\), claims paying rating, \\(\\beta\\) "],
["option-pricing-models.html", "15.6 Option Pricing Models", " 15.6 Option Pricing Models 2 different ways to value insurance company using options Equity of the firm as a call option Valuation of real options (Additonal source of value the firm has from its ability to engage or disengage in various projects) 15.6.1 Equity as Call Option Merton’s method: Owners of the firm have given assets of the firm to the debt holders, but maintain the right to purchase them at price \\(D\\) when the debt is due at time \\(T\\) At \\(T\\), if the value of the firm \\(V_T &gt; D\\) then the owners will pay back the debt If \\(V_T &lt; D\\), the option will not be exercised and firm goes bankrupt \\[E_T = \\max[V_T - D, 0]\\] Remark. Call option with strike price \\(D\\) and “stock” price \\(V_T\\) Price with Black-Scholes Assume debt to have one expiration date \\(T\\) We need the volatility of the value of all assets of the firm Application to P&amp;C Insurers Mostly theoretical Moody’s KMV credit default model is based on this to estimate probabilities of default for publicly traded companies Difficult to apply to P&amp;C companies due to the additional complexity of having policyholder liabilities that look just like debt to the shareholders And with policyholder liabilities it is hard to reduce it into a debt with single expiration date 15.6.2 Real Options Valuation Not super sure how the ‘value’ of each of the options work Definition 15.1 (Real Options) Management of a company has flexibility on when to take action on projects Real options examples: Abandonment (e.g. Abandoning a project so you cut off the future cash outflow) Value as an American put option (You have the right to sell at a predetermined price anytime up to the expiration date) \\(K\\) = liquidation proceeds Expansion (e.g. similar to the abandonment but with investing) Value as an American call option \\(K\\) = additional investment Contraction (Not total abandonment) Value as American put Value is the gross value of the lost capacity \\(K\\) = cost savings (reduced investment) Defer Value as an American call (since it’s still an investment project) Extend Value as European call since it would only be exercised at the end of the project Extend the life of the project by paying a fixed amount Value is added when the firm can purchase assets below their fair value or has exclusive access to opportunities e.g. flexibility to purchase assets at market price doesn’t add value Practical Considerations, difficulty in: Identifying a new business where such a real option with value exists Assess the current value of these business Determine if the firm has the ability to enter this business at a fixed price, or a price that significantly differs from market value Valuation considerations, technical issues to consider: Valuing the underlying cash flow Time to maturity Exercise type (American vs European) Appropriate valuation formula (e.g. Black-Scholes assumes lognormal distribution for asset price) Reasonableness of real option values Options are more valuable when new information will be available before the expiration Expansion options are only valuable if they have some exclusive right Exercise price must be fixed for the option to have value "],
["past-exam-questions-14.html", "15.7 Past Exam Questions", " 15.7 Past Exam Questions Concepts \\(\\star\\) 2011 - #12 a: higher \\(g\\) should be matched with higher \\(\\beta\\) for being more risky 2011 - #12 c: relationship between \\(g\\) and \\(k\\) 2015 #17 b-c: \\(\\beta\\) discussion DDM 2008 #43 15.1: DDM calc and convert to P:E \\(\\star\\) 2008 #44: DDM calc and interpretation 2008 #45: Comparison on P:E 2009 #34 \\(\\star\\) 2011 - #12 b: DDM calc (Get \\(r_f\\) as t-bond less liquidity premium) \\(\\star\\) High growth should be more risky and have higher \\(\\beta\\) \\(\\star\\) 2012 #12 15.5: Full calc with DDM 2014 #16: DDM calc 2015 #17 a: DDM calc FCFE \\(\\star\\) 2009 #35 15.2: Full FCFE calc \\(\\star\\) 2013 #11 15.6: Full FCFE calc with discussion Flow to Equity vs to firm 2014 #17: FCFE Calc \\(\\star\\) 2015 #16: Discussion on DCF 2016 #19: FCFE AE \\(\\star\\) 2010 #31 15.3: Full AE calc 2013 #12: Full AE Calc + discussion (AE vs DDM) 2015 #15: AE Calc + discussion TIA 1: AE calc and P:E and interpretation Relative Valuation 2011 - #13 15.4: Price to Book value calc + discussion on assumption \\(\\star\\) Why and why not the growth should continue \\(\\star\\) 2014 #15: Price to Book calc using all the ratios given by LoB 2016 #20: P:E and P:B ratios \\(\\star\\) TIA 2: Need to know to use price to book \\(\\star\\) Convert P:B to value, know what BV includes Option pricing TIA 3: Black scholes on projects TIA 4: How to value firm as call option and why it doesn’t work for P&amp;C companies TIA 5: Real options 15.7.1 Question Highlights Figure 15.1: 2008 Question 43 Figure 15.1: 2008 Question 43 Figure 15.2: 2009 Question 35 Figure 15.2: 2009 Question 35 Figure 15.2: 2009 Question 35 Figure 15.3: 2010 Question 31 Figure 15.3: 2010 Question 31 Figure 15.4: 2011 Question 13 Figure 15.4: 2011 Question 13 Figure 15.5: 2012 Question 12 Figure 15.5: 2012 Question 12 Figure 15.6: 2013 Question 11 Figure 15.6: 2013 Question 11 "],
["era-1-1-1-3-introduction-brehm-et-al-.html", "Chapter 16 ERA 1.1 - 1.3 Introduction - Brehm, et al.", " Chapter 16 ERA 1.1 - 1.3 Introduction - Brehm, et al. Requirements to promote ERM External pressure and internal champion Definition of ERM 16.1 7 key aspect of ERM 4 categories of risk Enterprise risk model process Diagnose: Develop threshold for all risk in the 3 different categories 16.1 Analyze: Prioritize and analyze material risk and correlation Critical risk 16.2 Implement: 5 ways to manage the risk: avoid, reduce frequency, mitigate severity, transfer, or retain Monitor the process: Update new risk, ways to control or options to transfer/treating the risk Goal of enterprise risk model and how it help with management functions and strategic decisions Model quality Good vs bad model \\(\\star\\) 4 Key elements that differentiate (for the model and modelers) 3 Factors that impact the suitability and usefulness of the model 4 key elements of enterprise risk model: Underwriting risk: Freq &amp; sev, pricing, parameter (estimation, projection, event and systematic) and cat model Reserving risk: Asset risk for different class of assets: (Bonds, equities, FX, etc) Dependencies, its various sources and how to model them \\(\\star\\) Approach to setting capital based on default avoidance or threshold lower than probability of ruin Capital allocation "],
["era1-1-intro.html", "16.1 ERA 1.1 Historical Context", " 16.1 ERA 1.1 Historical Context Two similar quantitative initiatives from the past Dynamic Financial Analysis RBC (from 1990s) was the precursor to scenario testing and Dynamic Financial Analysis (DFA) but DFA didn’t pan out DFA requires support from many functions and was not widely used due to lack of natural champion Cat Model Primitive in 1980s Faced external and internal pressure to use cat models after HU Andrew Nowadays it would be considered mismanagement for a company not to use cat models There is a spectrum of acceptance for both internal and external, and ERM is somewhere between the 2 examples above External Pressure on ERM as of 2007: International Association of Insurance Supervision (IAIS) began development of an approach of rating insurer based on Basel II See 2004 IAA document Canada (Dynamic Capital Adequacy Test) and Australia (Internaal Model Based Method) developed and implemented regulatory requirements regarding the construction and use of internal risk models UK (Financial Service Authority) implemented Individual Capital Adequacy Standards (ICAS) in 2004: “A firm is required to undertake regular assessments of the amount and quality of capital which in their view is adequate for the size and nature of their business…” NAIC is moving towards a new audit paradigm: Capital adequacy, Asset quality, Reserves, Reinsurance, Management, Earnings and Liquidity (CARRMEL) 2005 S&amp;P stated that a firm’s EMR program will become a critical component in its rating methodology Internal Champion ERM needs an internal champion (e.g. CRO) with: Quantitative skills Operational experience Political skills (executive rank) to work across organizational silos "],
["era-1-2-overview-of-enterprise-risk-management.html", "16.2 ERA 1.2 Overview of Enterprise Risk Management", " 16.2 ERA 1.2 Overview of Enterprise Risk Management Definition 16.1 ERM is the process of systematically and comprehensively: Identifying critical risk Quantifying their impacts, and Implementing integrated strategies To maximize enterprise value "],
["key-aspect-era1.html", "16.3 Key aspects of Enterprise Risk Management", " 16.3 Key aspects of Enterprise Risk Management ERM is: A process, not a one time analysis Enterprise wide basis Focus on critical or material risk Risk: Has upside and downside, it’s when actual outcome \\(\\neq\\) expected Must be quantified; Dependencies must be evaluated and quantified Strategies: Are developed to avoid, mitigate, or exploit risk factors Are evaluated as a tradeoff of risk and return, to maximize firm value "],
["risk-cat-era1.html", "16.4 Risk Taxonomy", " 16.4 Risk Taxonomy Insurance Hazard: Unique to insurers that is intentionally taken on to make a profit, the crusx is having inadequate premium to cover the expsosure In-force exposures: Accumulation (CAT) Underwriting (non-CAT) Past exposures (reserve deterioration) Financial Risk: Volatility in: interest rate, FX, equity, credit, liquidity Liquidity risk is the risk of having insufficient liquid assets to be able to meet a sudden cash demand Focus on assets of insurance company (Insurers tend to have high \\(\\frac{Asset}{Equity}\\) ratio) Financial risk impact our liability as well Operational Risk: Execution risks of the company (things don’t turn out as planned) Relates to threats to company value from actions (either the action itself of its consequences departed from plan) Strategic Risk: Risk about choices: making the right/wrong choice, or refusing to choose, or fail to recognize the need to choose Inherent threat to the company in choosing the wrong plan given the current and expected market conditions Remark. Operational and strategic risks are less amenable to quantitative modeling "],
["enterprise-risk-model-process.html", "16.5 Enterprise Risk Model Process", " 16.5 Enterprise Risk Model Process Diagnose \\(\\Rightarrow\\) Analyze \\(\\Rightarrow\\) Implement \\(\\Rightarrow\\) Monitor \\(\\Rightarrow\\) Repeat 16.5.1 Diagnose High level risk assessment All risks are considered Include preliminary development of thresholds for risk that will be considered material of significant (e.g. Key aspect 3) from above) Table below 16.1 can serve as a structured way to analyze the pillars of the risk taxonomy Table 16.1: Categorization of uncertainties General Environment Industry Firm Specific Political uncertainties (e.g. democratic changes, war, revolution) Input market (e.g. supply, quality) Operating (e.g. labor, supply) Government policy (e.g. fiscal, monetary changes, regulation) Product market (e.g. demand) Liability (e.g. products, employment, production) Marcoeconomic environment (e.g. inflation, interest rates) Competitive uncertainties (e.g. new entrants, rivalry) R&amp;D Social (e.g. terrorism) Credit Natural (e.g. cat events) Behavioral 16.5.2 Analyze Analyze risk identified in step 1 for materiality and prioritize Definition 16.2 (Critical Risk) Risk with potential to exceed the company’s threshold They should be modeled to the extent possible Preferably quantified with a probability distribution of outcomes Correlations among risk factors must be recognized (distribution integrated across the various risk silos) Risk metrics are calculated with the combined distribution to establish a measure of the degree of risk Selected risk measures must be consistent with management’s view toward risk Prioritize risk factors that contribute to adverse scenarios (or risk metrics) above the critical threshold 16.5.3 Implement Focus on managing risks identified and analyzed as material 5 potential actions to manage risk: Avoidance Reduction in chance of occurrence Mitigate effect given occurrence Eliminate or transfer of the consequences Retention (by design or by default) some or all of the risk Speculative risk factor may provide an opportunity to capitalize on the risk, rather than manage it away Perform a cost/benefit or risk/return trade-off analysis 16.5.4 Monitor Management need to monitor the process, compare to expectations Need to update plans and model as company or market or competitors change (not a “project” to be completed) New risk to address New ways to control them New options for treating or transferring risk "],
["ermodel-overview-era1.html", "16.6 ERA 1.3 Enterprise Risk Modeling Overview", " 16.6 ERA 1.3 Enterprise Risk Modeling Overview Goal: understand and quantify the relationships among the risks to the business that arise from asset, liabilities and underwriting All are affected by internal decisions &amp; external factors Modeling can help with management functions and strategic decisions: Determining capital need (to support risk, maintain rating, etc) Identifying sources of significant risk and cost of capital to support them Setting reinsurance strategies Planning growth Managing asset mix Valuing companies for M&amp;A "],
["model-qual-era1.html", "16.7 Quality of Models", " 16.7 Quality of Models Good Model: Show the balance between risk and reward for a range of different strategies (e.g. changing asset mix or reinsurance program or LoB to grow) Recognizes and reflects its own imperfection (Model only approximates reality and the parameters are uncertain) Weak Model: Over(under)state risks \\(\\leadsto\\) Overly cautious (aggressive) choices Factors that affect the suitability and usefulness of the model Data quality Choices on assumptions and mathematical method Variety of different risk elements and how they are represented Key elements that differentiate model quality: Model: Reflects relative importance of various risks to business decisions Includes mathematical techniques to reflect dependencies Modelers: Have a deep knowledge of the fundamentals of those risks Have a trusted relationship with senior management Model should take into account the uncertainty from other models (e.g. CAT, ESG, credit risk) that are used as inputs Risks less amenable to detailed representation in enterprise risk model (Non-quantifiable risk): e.g. Op risk (IT exposures, pension inadequacy, key man risk, rogue traers, frade) They are extremely important to the success of a business Requires specialized management process and difficult to incorporate into an overall risk model Can be modeled in bulk using informed judgments (but with high degree of uncertainty) Use model to manage the risk for which the modeling process is effective While some risks are at best weakly represented in such a model and requires other management methods "],
["key-elements-era1.html", "16.8 Key Elements of Enterprise Risk Model", " 16.8 Key Elements of Enterprise Risk Model Underwriting Risk Reserving Risk Asset Risk Dependencies (Correlation) 16.8.1 Underwriting Risk Loss frequency &amp; severity distribution Pricing risk Parameter risk Estimation risk Projection risk Event risk Systematic risk Cat model uncertainty 16.8.1.1 Loss Frequency and Severity Distribution Use statistical methods to: Estimate parameters Test quality of fit Understand uncertainties that remain Common distributions for insurance loss are available in many papers Transformed Beta and Gamma Distributions and Aggregate Losses - Venter Continuous Distributions - Kreps 16.8.1.2 Pricing Risk Instability from variations in the premiums as well as losses Misestimation of projected losses as well as competitive market pressures Deficiency may exist for many years for long tailed lines \\(\\Rightarrow\\) Accumulation of reserve deficiency Model should include u/w cycle as it contributes significantly to pricing risk See additional details in ERA 5.2 16.8.1.3 Parameter Risk Misestimated parameters, imperfect form and risk not modeled (Likely largest risk net of reinsurance (larger than cat exposure)) Estimation Risk Error in estimations the form and parameters Depends on available data (i.e. more and better data reduces this risk) Projection Risk Error on forecast of future value even if we fit the historical data accurately Examples: Freq/Sev trend from the time of the data to the current and future underwriting periods Development of ultimate losses Affected by macro factors (e.g. inflation) and the corresponding dependencies should be reflected in the model Unexpected change in external risk conditions also add to projection risk, e.g. Lower fuel price \\(\\Rightarrow\\) Increased driving Long term shift to more extreme weather Event Risk: Causal link between a large unpredictable event and losses to an insurer Example: Court ruling that favors a large group of policyholders (e.g. class action) Exposure that existed, unknown, for many years comes to light (e.g. asbestos) New cause of loss emerges that was previously regarded as not covered (e.g. environmental, CD) Regulator or court expands coverage by barring important exclusions New entrant into market reduces rates to grab market share Systematic Risk: Operates simultaneously on a large number of policies Undiversifiable, i.e. do not improve with added volume e.g. macro factors such as inflation See additional detail in ERA 3.2 16.8.1.4 Catastrophe Modeling Uncertainty Exposure to natural and man-made cat Likely largest risk after parameter risk, special case of parameter risk Based on proprietary cat models, further source of uncertainty Differences between commercial models and different versions of the same model Considerable amount of uncertainty in the probabilities of various events and the amount of insured damage Additional uncertainty from data quality Mismatches of company data fields and cat model assumptions Can quantified by the use of more than one model for each peril Cat distributions are subject to the same considerations of parameter risk and correlation as other risk distributions 16.8.2 Reserving Risk Adverse development can be significant for long tailed lines \\(\\uparrow\\) reserve uncertainty \\(\\Rightarrow\\) \\(\\uparrow\\) capital requirement &amp; \\(\\uparrow\\) time holding the capital Model needs to capture both process variance and parameter risk Key aspects of risk modeling process Specifying the reserve runoff model Testing it with quality of fit measures Unearned premium reserve should be modeled in underwriting risk 16.8.3 Asset Risk Need to focus on issues of the specific markets that the company operates Main asset classes: equities, fixed income, real estate Different types of fixed income are important in different regions FX and inflation are closely related to asset modeling as well Asset Modeling: Probabilistic Reality Modeling scenarios consistent with historical patterns Generate a large variety of scenarios against which to test the insurer’s strategy Scenarios are weighted by probability Scenarios should be reasonable when compared to historical patterns Asset liability matching Offset between insurance risk and investment risk P&amp;C companies can opt for longer duration assets and not match the liabilities to increase investment return while still maintain reasonable ALM risk Need to consider that P&amp;C liabilities are inflation sensitive as well Or if liabilities are carried at PV marked to current interest rate Efficient Frontier: \\(\\sigma\\) vs \\(\\operatorname{E}[Return]\\) See how it changes by modifying reinsurance structure or asset mix More details in ERA 2.4 Modeling Considerations Bonds: Model with arbitrage free models Should capture historical features of bond markets (high auto-correlations and distribution of yield spreads) Equities: Incorporate correlations with bonds Geometric Brownian motion model with additional volatility is more realistic (Allow for more extreme motion or even discontinuities) FX: First model interest rates of the currencies then model the FX rates Changes in actual and anticipated interest rates in two countries lead to changes in the FX rates Interest rate movements across different economies are correlated as well 16.8.4 Dependencies Important to capture dependencies or else the model will be unrealistically stable Sources of Dependencies: Simultaneous impact of macro economic conditions on many risks A good ESG should capture the dependence between inflation, interest rates, equity values, etc This will impact asset values Inflation will impact underwriting losses, loss reserve development Interest rates may influence the underwriting cycle U/w cycle, loss trends and reserve development can be correlated across LoB and with each other Cat and other event losses can impact across lines Large man made cat (e.g. 9/11) can have impact on both insurance and market risk Difficult to quantify May reference studies of multiple insurers, public insurance industry information and macroeconomic data Still require professional judgement Modeling Dependencies: Avoid using just the correlation coefficient to describe and oversimplify dependency We are most interested in the tail dependency e.g. High inflation or large cat that impacts multiple LoB that would typically be uncorrelated Use copulas to capture different forms of dependency See ERA 3.3 for more details "],
["setting-cap-era1.html", "16.9 Setting Capital Requirements", " 16.9 Setting Capital Requirements ERM help insurer find the optimum level of capital that balances efficiency and prudence Policyholders (acting through regulators and rating agencies) demand adequate capital Shareholders require that capital be used efficiently Set capital to maximize insurer’s value creates a conceptual framework that unifies the competing requirements of prudence and efficiency See ERA 2.2 for more detail Approaches to setting capital requirements: 1. Default Avoidance: Focused on the tail and protecting policyholders Caveat: S/h will be hurt at much lower loss amount We have the least confidence in the tail of the model \\(\\hookrightarrow\\) More feasible and relevant to focus on thresholds less extreme than default 2. Other Thresholds (Lower than probability of ruin): Probability of downgrade Significant loss \\(\\leadsto\\) \\(\\downarrow\\) financial rating \\(\\leadsto\\) \\(\\downarrow\\) franchise value &gt; loss itself Franchise value: customer base, agency relationships, reputation, infrastructure and expertise Sufficient capital to service renewals Survive and thrive after major cat 3. Other practical method Leverage ratio RBC and rating agency capital Scenario testing 16.9.1 Convert Probability to Loss Associate the selected probability level with amount of financial loss Smallest amount of loss, \\(\\alpha \\%\\) of the time \\(\\Rightarrow\\) \\(VaR_{100 - \\alpha \\%}\\) Average loss, \\(\\alpha \\%\\) of the time \\(\\Rightarrow\\) \\(TVaR_{100 - \\alpha \\%}\\) 1-in-\\(x\\) years event loss no more than \\(y \\%\\) of capital \\(\\Rightarrow\\) \\(\\dfrac{100}{y} \\times TVaR_{1 - \\frac{1}{x}}\\) Important: Risk measures do not tell us how much capital to hold; They tell us for a given level of capital, what is the risk to the company Determination of optimal capital level is not purely a risk measurement exercise Need to balance sufficient financial strength to attract business and the need of shareholders to achieve attractive return Risk measurement quantifies the risk the company is exposed to in relationship to the capital so established Strategic risk decisions can then be made in the framework created Capital is not set by risk measures, but analyzed by them to understand the sources of risk and for further strategic decision making 16.9.2 Risk Adjusted Performance Allocate capital to business units \\(\\Rightarrow\\) Calculate the rate of return \\(\\dfrac{\\text{Profit}}{\\text{Capital}}\\) for that unit \\(\\Rightarrow\\) Compare RoR across BUs Can also allocate risk capital to investments \\(\\Rightarrow\\) Compare riskiness of investments and u/w Validity of such a comparison will depend on the consistency of the risk measures that are developed See ERA 2.2 for more detail "],
["past-exam-questions-15.html", "16.10 Past Exam Questions", " 16.10 Past Exam Questions TIA Exercise TIA 1: Failure of DFA and external pressure for ERM TIA 2: Action from regulators to advance regulations TIA 3: Financial risks and why take on risk TIA 4: 5 ways to manage risk and practical examples TIA 5: 4 steps of ERM TIA 6: Benefits of multi period pricing model TIA 7: Describe model parameter risk in example TIA 8: How to build a good model TIA 9: Dependencies between LoBs TIA 10: Capital allocation for various stakeholders (policyholders vs shareholders) TIA 11: Characteristics that distinguish model quality Past Exam 2011 #18: model underwriting risk, correlation between lines \\(\\star\\) 2013 #16: Reason for AY correlation in a LoB and how to model it \\(\\star\\) 2015 #25: definition for u/w risk and liquidity risk, correlation between the two and mitigation \\(\\star \\star\\) 2016 #21: strengh and weakness of a sample ERM program \\(\\star\\) 2016 #23: Capital requirement setting 16.10.1 Question Highlights n/a -->"],
["era-2-1-corporate-decision-making-using-an-enterprise-risk-model-don-mango.html", "Chapter 17 ERA 2.1 Corporate Decision Making Using an Enterprise Risk Model - Don Mango", " Chapter 17 ERA 2.1 Corporate Decision Making Using an Enterprise Risk Model - Don Mango 3 evolutionary steps in decision analysis: Deterministic \\(\\Rightarrow\\) Risk Analysis \\(\\Rightarrow\\) Certainty Equivalent Argument that 2nd step is sufficient and reasons to move to the 3rd step 4 Benefits of a utility function Attribute cost back to source of risk for decision making Decision Making with an Internal Risk Model (IRM) Quantify corporate risk tolerance and what it is dependent on Spetzler: Benefits of utility curve Walla: Calculates efficient frontier \\(\\Rightarrow\\) Estimate risk tolerance \\(\\Rightarrow\\) Where on the frontier to select the best portfolio based on \\(R\\), utility curve and CE Allocating risk capital Cost benefit analysis with EVA or capital allocation Use a suite of decision metrics that are distinct and independent (to give different perspectives) and are responsive to different dynamics "],
["evoluation-of-corporate-decision-making-under-uncertainty.html", "17.1 Evoluation of Corporate Decision Making Under Uncertainty", " 17.1 Evoluation of Corporate Decision Making Under Uncertainty Decision making under uncertainty 3 evolutionary steps: Deterministic Project Analysis: Forecast cashflow, return metric (e.g. NPV) and sensitivity Risk Analysis: Distribution of inputs (e.g. Revenue) and distribution of outcome Certainty Equivalent: From 2) run the outcomes through a risk preference function of the company 17.1.1 Deterministic Project Analysis Figure 17.1: Deterministic Remark. Single deterministic forecast of cash flow Return metric using NPV or IRR Sensitivity test on inputs (e.g. revenue, expense, cost of capital) Sensitivity doesn’t have any probabilities assigned for each scenarios Uncertainty is handled judgmentally 17.1.2 Risk Analysis Figure 17.2: Risk Analysis Remark. Critical inputs have a distribution (w/ correlations) \\(\\hookrightarrow\\) Output is a distribution of NPV or IRR Uncertainty is handled judgmentally (Good portion has been moved into the distn) 17.1.2.1 Reason to Stay in Step 2 Argument that step 2 is sufficient (Current best practice 2007): Since investors are only compensated for systemic risk and not firm specific risk, no need to focus on firm specific risk Based on portfolio theory where investors holds many different firm so the firm specific risk are diversified away \\(\\therefore\\) Only need to manage systemic risk Counter argument: Management can’t distinguish between systemic and firm risk Risk-adjusted discount rate reflect risk only if there is a time lag For many kinds of risk the time aspect is unimportant (Risk is instantaneous - jump risk) Jump risk is still present as information have a time lag Market based information lack the refinement and discriminatory power (too noisy) for management to be able to do proper cost-benefit analysis and make trade off decisions 17.1.3 Certainty Equivalent Figure 17.3: Certainty Equivalent Remark. Distribution output from risk analysis is input to a utility function for the firm Utility function is based on corporate risk policy Purpose is to express preference in a transparent and consistent manner \\(\\hookrightarrow\\) Automate and formalizes some of the risk assessment, while risk judgement is still required Benefits of utility function Objective: Once parameterized, does not require subjective opinion Consistent: Can be applied to different risk Repeatable: Can apply again to get the same results Transparent: Easy to document 17.1.3.1 Reason to Move to Step 3 Risk managers and shareholders are both interested in preserving franchise value \\(\\hookrightarrow\\) Want risk management in place to protect franchise value \\(\\hookrightarrow\\) Both should support policies to help make risk management more objective, consistent, repeatable, and transaparent \\(\\therefore\\) Provides some degree of support for evolution to step 3 Market Value = Book Value + Franchise Value Franchise Value = Value of future earnings growth "],
["decision-making-with-an-internal-risk-model-irm.html", "17.2 Decision Making with an Internal Risk Model (IRM)", " 17.2 Decision Making with an Internal Risk Model (IRM) Focus on Element 5 below, 1-4 discussed else where Figure 17.4: Risk Model Components Attribute Cost Back to Source of Risk Estimate aggregate loss distribution (4 Above) Distribution of outcomes for each LoB \\(\\Rightarrow\\) Correlate outcomes \\(\\Rightarrow\\) Correlate on external sources Quantify the impact of the loss outcomes on the organization Amount of P&amp;L or level of PHS Can be a distribution Assign a cost to each amount of impact Utility function will be non linear \\(\\Rightarrow\\) Higher cost to events further out in the tail (e.g. $20m loss is more than twice as bad as a $10m loss) Attribute cost to source (e.g. BUs) 17.2.1 5a - Corporate Risk Tolerance Definition 17.1 Corporate risk tolerance How much risk a company is willing to take What tradeoffs the company is willing to make in terms of reduction in expected profits to reduce risk How much fluctuation in annual results it is willing to bear Needed in Steps 2 and 3 above Corporate risk tolerance is a combination of the following factors: Organization Size: Depending how much capital a firm have to deploy to invest in project each year changes how big an investment is Capital: The % impact to the firm’s total capital is more telling than the nominal value \\(\\Rightarrow\\) Firm with higher capital can tolerate bigger nominal value volatility Volatility: Investors might want steady stream of dividend paid \\(\\Rightarrow\\) Lower risk tolerance Public firms with quarterly earnings are rewarded for having consistent predictable profits \\(\\Rightarrow\\) Lower risk tolerance Private firm can afford to have more volatile results 17.2.1.1 Risk Preference Function Risk tolerance can be defined by a risk preference function (or utility function) Translation of impact into cost requires a risk preference function Properties of risk preference function Non linear e.g. \\(U(x) = A + B \\cdot \\operatorname{ln}[x+c]\\) or \\(U(x) = A - e^{ -x / R }\\) Slope decrease further into profit while increases further into losses A linear straight line represents risk neutral utility Source of risk preference function Implicitly taken from an outside source (e.g. capital market) Explicitly derived from firm management attitudes See section below from Spetzler Spetzler’s process output a transparent, objective, mathematical expression of the corporation’s acceptable risk-reward tradeoff This can improve cost-benefit analysis (CBA) by quantifying the minimum decrease in risk sufficient to justify a certain mitigation cost Without such a function, CBA can still be made but will be inconsistent and opaque driven by the individual decision makers’ intuitions and preferences Walls show how to select projects with the efficient frontier and utility curve 17.2.1.2 Spetzler “The Development of Corporate Risk Policy for Capital Investment Decisions” - Spetzler 1968 Paper found that managers have very different risk tolerate and found that they are likely to be too conservative for small projects (damages not meaningful to the firm as a whole) Useful for management to: see the different utility curves for different managers make decisions on where the company utility curve should be and communicate that to the day to day decision makers Benefits of Utility Curve: Transparent, objective mathematical expression of the firm’s acceptable risk/reward trade offs Without, risk/reward decisions criteria will be inconsistent and opaque and driven by individuals 17.2.1.3 Walls “Combining decision analysis and portfolio management to improve project selection in the exploration and production firm” - Walls 2004 Calculates efficient frontier \\(\\Rightarrow\\) Estimate risk tolerance \\(\\Rightarrow\\) Where on the frontier to select the best portfolio Efficient Frontier Given \\(n\\) projects with \\(\\operatorname{E}[NPV_i]\\) and \\(\\sigma_i\\), the firm can choose to participate on each project with % \\(x_i\\) Given the budget based on current portfolio, an efficient frontier is built based on the lowest portfolio \\(\\sigma\\) given different expected NPVs They note that the current portfolio is not optimal The next 3 point is to decide where on the efficient frontier to be on Risk Tolerance Risk tolerance level \\(R\\) and utility function tells you where the firm choose to be on the efficient frontier \\(R\\) is estimated based on where the decision maker is indifferent from a gamble of 50% of \\(R\\) and 50% of \\(-R/2\\) and not taking it (take the expected value?) Expected value is \\(\\dfrac{R}{4}\\) Utility Function \\[U(x) = 1 - e^{ - x / R}\\] Tell us how much risk is the firm willing to tolerate How much reward are we willing to give up for a given reduction in risk and vice versa Certainty Equivalent With the above, calculate Certainty Equivalent (CE) of a given portfolio: CE = The fixed amount that the firm is indifferent between taking the risky portfolio or the fixed amount CE changes based on the \\(R\\) selected Pick the portfolio with the highest CE Negative CE means the firm would be better of not investing in it Firm must answer these questions: How much risk are we willing to tolerate (Picking \\(R\\)) How much reward are we willing to give up for a given reduction of risk and vice versa (Expressing its risk preference, utility curve) Are the risk-reward trade offs available along the efficient frontier acceptable to us (Answer by the first 2 points) 17.2.2 5b - Cost of Capital Allocated Definition 17.2 Risk capital is a measure of firm’s total risk bearing capacity It is only an aggregate measure It gives counterparty confidence that the financial firm can perform Still an open question in actuarial science 17.2.2.1 Allocation Cost of risk capital is being allocated, not the capital itself Allocation of risk capital is theoretical since no capital is actually transferred to the policy Useful to allocate risk capital to risk-taking units (And non risk-taking units too maybe) Total risk capital required is reduced by diversification benefits and the contributions to risk are not linear (See ERA 2.2) Return on Risk Adjusted Capital (RORAC) Risk capital allocation is use as an interim step in assigning the cost of risk capital to portfolio elements Allocate capital in a risk adjusted way \\(\\Rightarrow\\) Riskier sources require more capital Apply firm wide hurdle rate to determine cost of capital for each portfolio elements (e.g. BU) \\(\\neq\\) RAROC as RAROC adjusts the hurdle rate and does not allocate capital in a risk adjusted way Bypassing Allocation Mango argues that concept of allocating capital is meaningless as each risk source has access to the capital of the firm (i.e. the entire pool of risk capital is a shared asset) Focus on cost of capital the risk source uses we get a direct answer Bypass allocation and goes straight to the cost How to determine the cost is the difficult part Idea is based on Merton and Perold where they define risk capital as the amount needed to guarantee the performance of the intermediary’s contractual obligations at the default-free level The cost of purchasing this protection in the market is the cost of risk capital A good candidate for decision variable is EVA if we only have cost of risk capital: EVA EVA = Economic Value Added = NPV - Cost of Capital EVA &gt; 0 means BU is adding value 17.2.3 5c - CBA for Mitigation Cost-benefit analysis (CBA): We can do this once we have the corporate risk tolerance and capital cost allocation CBA with EVA Mitigation effort that has positive EVA should be done CBA with Capital Allocation Projects that reduce capital cost by more than the cost of the project should be undertaken "],
["conclusion-1.html", "17.3 Conclusion", " 17.3 Conclusion Difficult to perform risk management for a firm based on a single metric Recommendation: A suite of decision metrics That are distinct and independent (to give different perspectives, reflecting different dimensions of the space) And are responsive to different dynamics However corporate realities often mandate a compromise be struck in the interest of parsimony We should reduce complexity as much as possible, but not more so \\(\\therefore\\) Using a handful of risk metrics is acceptable -->"],
["era-2-2-risk-measures-and-capital-allocation-g-venter.html", "Chapter 18 ERA 2.2 Risk Measures and Capital Allocation - G. Venter", " Chapter 18 ERA 2.2 Risk Measures and Capital Allocation - G. Venter \\(\\star\\) Risk measures and their pros/cons Moment based Tail based (\\(VaR\\), \\(TVaR\\), \\(XTVaR\\), \\(EPD\\)), see table 18.1 Probability transform e.g. Esscher (18.1) Generalized moments Capital a company should hold depends on practical considerations, not simply derived from a risk measure e.g. Customer reactions, or keepin renewal business Allocation of risk measures Definitions 18.2 Proportional Co-measures, table 18.3 and 18.2 Allocation method properties: Marginal allocation 18.3 Scalable risk measures 18.4 Suitable allocation 18.3 Marginal impact (18.4) A risk measure can have many co-measures but only 1 will be marginal Allocation works best when the risk measure \\(\\propto\\) the market value of risk Can bypass the above if we can just allocate the cost of capital Option pricing or the MV of stop loss that attach at 0 profit "],
["era2-2-intro.html", "18.1 Introduction", " 18.1 Introduction VaR by itself it too simplistic for insurance companies Advantages attributed to Economic Capital Unifying measure for all risks in an organization More meaningful to management than RBC or Capital Adequacy Ratios (e.g. Prem/Surplus, Res/Surplus) Forces firm to quantify risks it faces and combine them into a probability distn Provides a framework for setting acceptable risk levels for the company and its business units Remark. Many other risk measures have the advantages above (not just “economic capital” under VaR) Insurers typically use multiple risk measure to see if a consistent picture emerges VaR is typically calculated from the distribution of all risks and then allocate down to inidividual units This provides consistent measurement of risk across units Target Probability Level Current modeling approaches are not able to accurately estimate losses deep in the tail like 1-in-3000 (99.97%) event Bond ratings are discusses at this level but the ratings are not tied to the probabilities Ratings are defined by factors (e.g. interest coverage) Probabilities are published retrospectively after observing rated bonds for many organizations for many years Target probability level is artificial (no theoretical support) Sometimes it is just backed out based on the modeled distribution Suggestion: Should just express actual capital under various risk measures The company sometimes compare held capital to the loss distn stating that it’s holding capital to cover a 1-in-x year event. But not the other way around like setting the capital based on it Modeling difficulty of the tail can be circumvented by focusing on events that can lead to impairment of the company, not just insolvency "],
["risk-measures.html", "18.2 Risk Measures", " 18.2 Risk Measures 3 major class of risk measures Moment-based (e.g. mean, variance) Tail-based (e.g. VaR, TVaR) Probability Transformation (e.g. WTVaR, Mean of Transformed distn) Definition 18.1 Loss here is not just loss from claims, but “negative profit” after considering income (e.g. premium), expenses etc \\(Y =\\) Losses \\(\\rho(Y) =\\) Risk Measure 18.2.1 Moment-Based Measures \\(\\mathrm{E} \\left[ Y^k \\right]\\) is the kth moment of \\(Y\\) which represent losses 1st moment = Mean 2nd central moment = Variance = \\(\\mathrm{E}\\left[ (Y-\\mathrm{E}[Y])^2 \\right]\\) S.d. is preferred as it has the same units as the loss Semi-s.d. only captures unfavorable deviations Quadratic risk measures are not enough to capture market attitudes to risk (they do not capture skewness) 3rd moment = Skewness Exponential Moments \\[\\mathrm{E}\\left[Y e^{cY/\\mathrm{E}[Y]}\\right]\\] Captures all the moment using taylor expansion Scaled to $ but captures the effect of large losses on the risk exponentially Does not exist unless there is a maximum possible loss 18.2.2 Tail-Based Measures at \\(\\alpha\\) Percentile Table 18.1: Tail-Based Measures Risk Measure Name Description \\(VaR\\) Value at Risk \\(\\alpha\\)th Percentile \\(TVaR\\) Tail Value at Risk Avg loss above \\(\\alpha\\)th percentile \\(XTVaR\\)1 XS Tail Value at Risk \\(TVaR - Mean\\) \\(EPD\\)2 Expected Policyholder Deficit \\((TVaR - VaR) \\cdot (1 - \\alpha)\\) Value of Default Put Option3 Market cost of insuring for losses over \\(VaR_{\\alpha}\\) \\(XTVaR\\) focuses on funding losses above the mean \\(EPD\\) = unconditional expected value of defaulted losses if capital = \\(VaR_{\\alpha}\\) If there was no risk premium required, this is the cost of insuring for losses over \\(VaR_{\\alpha}\\) First part of the equation is the expected loss given default and the second part is to get to the unconditional value Expected loss of insuring the company for losses excess of their capital amount (here it’s \\(VaR_{\\alpha}\\)) Value of default put is great than EPD, it includes an additional risk premium (market cost of insuring for losses over \\(VaR_{\\alpha}\\)) Value of insuring against default Estimate using option pricing 18.2.3 Probability Transforms To recognize that large losses are worse for the firm in more than a linear way (e.g. losses twice as big is &gt; twice as worst) Change the loss distribution function by putting more weight (add probability) to the worst losses e.g. Esscher transform \\[\\begin{equation} f^*(y) = k \\cdot e ^{y/c} \\: f(y) \\tag{18.1} \\end{equation}\\] Lower \\(c\\) \\(\\Rightarrow\\) Higher losses from the transformed distn Can get non sensical results for \\(c\\) that’s too low \\(EPD\\) on a transformed distn can give us the value of the default put The largest losses will have additional weight (to account for the risk premium we discussed above) Remark. Most asset pricing formulas like CAPM and Black-Scholes can be expressed as transformed means \\(\\therefore\\) transformed means are a promising risk measure for determining the market value of risk Complete market is where any risk can be sold, but we work in incomplete markets Theory of pricing in incomplete markets favors: Minimum Martingale Transform (MMT) Minimum Entropy Martingale Transform (MET) These give reasonable approximations of reinsurance prices Mean under then Wang transform closely approximates market prices for bonds and CAT bonds We can calculate other risk measures (e.g. VaR) on the transformed distribution \\(WTVaR\\) (Weighted TVaR) is \\(TVaR\\) on a transformed distribution EPD on transformed distribution can use to estimate value of default put 18.2.4 Generalized Moments Includes more than just \\(\\mathrm{E}[Y^k]\\), can include all of the above risk measures (Not just expectations of powers of the variable) TVaR expressed in generalized moments \\[TVaR_{\\alpha} = \\mathrm{E} \\left[ Y \\cdot \\left( Y &gt; F^{ -1}_Y(\\alpha) \\right) \\big| Y &gt; F^{ -1}_Y(\\alpha) \\right]\\] \\((Y &gt; b) = \\begin{cases} 1 &amp; : Y &gt; b\\\\ 0 &amp; : \\text{otherwise} \\end{cases}\\) Average of \\(Y\\) when \\(Y\\) &gt; than the \\(\\alpha^{th}\\) percentile: \\(F^{ -1}(\\alpha)\\) Spectral Measures This includes many tail measures that can be written in the following form \\[\\rho(Y) = \\mathrm{E} \\left[ Y \\cdot \\eta \\left( F(Y) \\right) \\right]\\] \\(\\eta \\geq 0\\) Multiply the loss by a non-negative scalar \\(\\eta(Y)\\) when taking the expectations TVaR is a spectral measures Blurred VaR Takes the distance from the target percentile as the weighting function \\[\\eta(p) = \\mathrm{exp} \\left \\{ - \\theta (p-\\alpha)^2 \\right \\}\\] Give more weight to losses near \\(\\alpha^{th}\\) percentile while still using the whole distribution \\(\\theta\\) controls how quickly the weight drops of as we get further from \\(\\alpha\\) Takes several results and averages them together Which risk measures to use \\(TVaR\\) at a low percentile is preferred as it captures all risks above this level Set at level where we begin to loss capital High percentile is not recommended as it ignores all risk below that percentile Try to get close to market value of risk \\(WTVaR\\) with the MET is promising, or Exponential moment \\(\\mathrm{E}[Y e^{cY/\\mathrm{E}[Y]}]\\) "],
["required-capital.html", "18.3 Required Capital", " 18.3 Required Capital Capital a company should hold depends on practical considerations, not simply derived from a risk measure Risk measures should be used to measure the safety of the capital Considerations that drive capital requirements: Customers reactions: Well capitalized insurer \\(\\Leftarrow\\) vs \\(\\Rightarrow\\) better price Know your customer base, whether they are price or capital sensitive \\(\\downarrow\\) rating \\(\\Rightarrow\\) \\(\\downdownarrows\\) of cumtomers since customers that want a higher level can quickly leave \\(\\uparrow\\) rating \\(\\Rightarrow\\) \\(\\upharpoonleft\\) of growth since higher rating just provides an opportunity to compete with other insurers that already have the business Keeping renewal business: Have enough capital so that in adverse scenario there is still enough capital to service renewals that is x% of the business Since renewal business are more profitable Once capital level has been established, compare it against different risk measures Look at % of capital lost a various probability levels \\(TVaR\\) is a better risk measure than \\(VaR\\) for a given percentile since \\(VaR\\) is just the smallest loss XS of a percentile, \\(TVaR\\) is the average "],
["capital-allocation.html", "18.4 Capital Allocation", " 18.4 Capital Allocation Allocate the total risk measure to each BUs for the following purposes: Measure the amount of risk each BU contributes Set capacity and policy limits for each BU Calculate risk-adjusted profitability RORAC: \\(\\dfrac{\\text{Profit}}{\\text{Allocated Capital}}\\) EVA = Profit - Cost of Capital Definition 18.2 \\(Y\\): Losses of a company with CDF \\(F(y)\\) \\(X_j\\): Losses for each BU, \\(\\sum_j X_j = Y\\) \\(\\rho(Y)\\): Risk measure of \\(Y\\), \\(\\mapsto\\) a real number \\(\\rho(Y) = \\sum_j r(X_j)\\) \\(r(X_j)\\): Allocation of risk measure to the individual BUs Usually different from \\(\\rho(X_j)\\), which is the risk measure applied to BU \\(j\\) 18.4.1 Proportional Method Allocate risk measure in proportion to the risk measure applied to each unit \\[\\begin{equation} r(X_j) = \\dfrac{\\rho(X_j)}{\\sum_i \\rho(X_i)} \\rho(Y) \\tag{18.2} \\end{equation}\\] This works for any risk measure 18.4.2 Co-Measures \\[\\begin{equation} r(X_j) = \\mathrm{E}[h(X_j) \\cdot L(Y) \\mid g(Y)] \\tag{18.3} \\end{equation}\\] Given that: \\(\\rho(Y)\\) is expressed as a conditional expected value \\[\\rho(Y) = \\mathrm{E}[h(Y) \\cdot L(Y) \\mid g(Y)]\\] \\(h(\\cdot)\\) is additive \\[h(u+v) = h(u) + h(v)\\] \\(\\hookrightarrow\\) \\(\\rho(Y) = \\sum_j r(X_j)\\) This is a marginal allocation that sums to the total risk measure Table 18.2: Co-Measures Risk Measure \\(h(Y)\\) \\(L(Y)\\) Condition Co-Measure \\(TVaR\\) \\(Y\\) \\(1\\) \\(F(Y) &gt; \\alpha\\) \\(\\mathrm{E}[X_j \\mid F(Y) &gt; \\alpha]\\) \\(VaR\\) \\(Y\\) \\(1\\) \\(F(Y) = \\alpha\\) \\(\\mathrm{E}[X_j \\mid F(Y) = \\alpha]\\) Standard Deviation1 \\(Y - \\mathrm{E}[Y]\\) \\(\\dfrac{Y - \\mathrm{E}[Y]}{\\sigma_Y}\\) none \\(\\dfrac{\\mathrm{Cov}(X_j,Y)}{\\sigma_Y}\\) \\(XTVaR\\)2 \\(\\mathrm{E}[X_j \\mid Y &gt; b] - \\mathrm{E}[X_j]\\) Note for s.d.: \\[\\rho(X_j) = \\dfrac{\\mathrm{Cov}(X_j,Y)}{\\sigma_Y} = \\mathrm{E} \\left [ (X_j - \\mathrm{E}[X_j]) \\cdot \\dfrac{Y - \\mathrm{E}[Y]}{\\sigma_Y} \\right ]\\] For \\(XTVaR\\): \\[\\rho(Y) = \\mathrm{E}[Y - \\mathrm{E}[Y ]\\mid Y &gt; b]\\] Note the condition is on loss amount \\(Y\\) not the percentile Expected losses in unit \\(j\\) when the company has a loss above \\(b\\) less the expected losses in unit \\(j\\) Remark. Under \\(TVaR\\) and \\(XTVaR\\) At low threshold, unit with higher mean have higher allocation At higher threshold, unit with higher variance have higher allocation 18.4.2.1 Co-\\(TVaR\\) and -\\(VaR\\) \\(TVaR\\): \\[r(X_j) = \\mathrm{E}[X_j \\mid F(Y) &gt; \\alpha]\\] Average losses in unit \\(j\\), condition on when the firm (Just like in co-measures) has losses XS the \\(\\alpha^{th}\\) percentile \\(VaR\\): \\[r(X_j) = \\mathrm{E}[X_j \\mid F(Y) = \\alpha]\\] Expected value of losses in unit \\(j\\), condition on when the firm has losses at the \\(\\alpha^{th}\\) percentile Caveat: Difficult to calculate because it is estimating a single point in the distn, results from sims can be quite variable Typical you’re conditioning on the BU’s \\(\\alpha^{th}\\) percentile Not marginal if the condition is not on the firm’s \\(\\alpha\\) Marginal allocation means as unit grows, it is charged with the additional capital it requires Not marginal means the individuals don’t sum to the total (?) 18.4.3 Having a Marginal Method Definition 18.3 Marginal Allocation: Allocation is marginal if the change to the company’s risk measure from a small change in a single BU’s (volume) is attributed to that business unit Consistent with concept that price should be proportional to marginal cost Leads to suitable allocation (18.5) Definition 18.4 Scalable Risk Measures: A small proportional change in the business (e.g. 5%) change the risk measure by the same proportion (e.g. 5%) This is Homogenous of degree 1: \\(\\rho(aY) = a \\cdot \\rho(Y)\\) Most measures in currency units have this property (e.g. S.D., TVaR) but not variance (currency2) and probability of default (unitless) Scalable risk measures are both marginal and additive Remark. For many companies and BUs, growth in exposure units can approximate homogeneous growth Proportional change can be achieve from proportional reinsurance and normal growth Except for companies that write large policies compared to their volume; Transformed risk measure might still work Definition 18.5 Suitable Allocation: Growing a unit that have above average ratio of profit/risk will increase the overall ratio of profit/risk for the company Given: \\[\\dfrac{P_j}{r(X_j)} &gt; \\dfrac{P}{\\rho(Y)}\\] We can show: \\[\\dfrac{P + \\epsilon P_j}{\\rho(Y + \\epsilon X_j)} &gt; \\dfrac{P}{\\rho(Y)}\\] Where: \\(P_j\\) is the profit for the \\(j\\)th business unit and \\(P\\) is the profit of the company This is based on pluggin in the definition of \\(r(X_j)\\) below 18.4.4 Marginal Impact A risk measure can have many co-measures but only 1 (the following form) will be marginal \\[\\begin{equation} r(X_j) = \\lim \\limits_{\\epsilon \\rightarrow 0} \\dfrac{\\rho(Y + \\epsilon X_j) - \\rho(Y)}{\\epsilon} = \\dfrac{\\partial \\rho(Y)}{\\partial X_j} \\tag{18.4} \\end{equation}\\] This is the derivative of the firm risk measure w.r.t. growth in BU \\(j\\) Even under non-homogeneous growth, the risk measure is still a decomposition (sum to the total) and is often close to marginal \\(XTVaR\\) \\[\\dfrac{\\partial\\rho(Y)}{\\partial X_j} = r(X_j) = \\mathrm{E}[X_j \\mid F(Y) &gt; \\alpha] - \\mathrm{E}[X_j]\\] Scalable when done XS of a percentile \\(\\alpha\\) (not when done XS a constant \\(b\\)) Variance \\[\\rho(Y) = Var(Y)\\] \\[r(X_j) = \\mathrm{Cov}(X_j,Y)\\] Not scalable (since \\(Var(aY) = a^2 Var(Y)\\)), so not marginal? \\(VaR\\) \\[\\rho(Y) = VaR_{\\alpha}(Y) = \\mathrm{E}[Y \\mid F(Y) = \\alpha]\\] \\[r(X_j) = \\mathrm{E}[X_j \\mid F(Y) = \\alpha]\\] Standard Deviation Spreading the s.d. \\(\\propto\\) mean is not marginal: \\[\\rho(Y) = Stdev(Y) = \\mathrm{E}\\left[ \\dfrac{Y \\cdot Stdev(Y)}{\\mathrm{E}[Y]} \\right]\\] \\[r(X_j) = \\mathrm{E}\\left[ \\dfrac{X_j \\cdot Stdev(Y)}{\\mathrm{E}[Y]} \\right] = \\dfrac{\\mathrm{E}[X_j]}{\\mathrm{E}[Y]} \\cdot Stdev(Y)\\] This is marginal: \\(h(Y) = Y - \\mathrm{E}[Y]\\) and \\(L(Y) = \\dfrac{(Y - \\mathrm{E}[Y])}{Stdev(Y)}\\) \\[\\rho(Y) = \\mathrm{E}\\left[ \\dfrac{Y - \\mathrm{E}[Y]^2}{Stdev(Y)} \\right] = Stdev(Y)\\] \\[r(X_j) = \\mathrm{E}\\left[ \\dfrac{(X - \\mathrm{E}[X])(Y - \\mathrm{E}[Y])}{Stdev(Y)} \\right] = \\dfrac{\\mathrm{Cov}(X_j,Y)}{Stdev(Y)}\\] Exponential Moment \\[\\rho(Y) = \\mathrm{E}[Y \\cdot e^{cY/\\mathrm{E}[Y]}]\\] \\[r(X_j) = r_1(X_j) + \\dfrac{\\mathrm{E}[X_j]}{\\mathrm{E}[Y]} \\cdot c \\cdot \\mathrm{E} \\left [ Y \\cdot e^{cY/\\mathrm{E}[Y]} \\cdot \\left \\{ \\dfrac{X_j}{\\mathrm{E}[X_j]} - \\dfrac{Y}{\\mathrm{E}[Y]} \\right \\} \\right ]\\] Without the curly bracket it is just allocating \\(c \\cdot \\rho(Y)\\) \\(\\propto\\) the mean \\(\\dfrac{\\mathrm{E}[X_j]}{\\mathrm{E}[Y]}\\) Curly bracket (XS ratio factor) adjust for correlation, where BU that have large \\(X_j\\) when Y is also large will have additional allocation BUs that don’t contribute to large losses will be negative in the curly bracket, receive reduced allocation EPD \\[EPD_{\\alpha} = (TVaR_{\\alpha} - VaR_{\\alpha}) \\cdot (1 - \\alpha)\\] \\[\\rho(Y) = \\mathrm{E}[Y - B \\mid F(Y) &gt; \\alpha] \\cdot (1 - \\alpha)\\] where \\(B = VaR_{\\alpha}(Y) = F_Y^{-1}(\\alpha)\\) \\[r(X_j) = (CoTVaR - CoVaR) \\cdot (1-\\alpha)\\] Only scalable when XS of a percentile \\(RTVaR = TVaR + c \\cdot Stdev(Y \\mid F(Y) &gt; \\alpha)\\) \\(h_1(Y) = Y\\) \\(h_2(Y) = Y - \\mathrm{E}[Y|F(Y) &gt; \\alpha]\\) \\(L_1(Y) = 1\\) \\(L_2(Y) = \\dfrac{c[Y - \\mathrm{E}[Y \\mid F(Y) &gt; \\alpha]]}{Stdev(Y \\mid F(Y) &gt; \\alpha)}\\) Condition: \\(F(Y) &gt; \\alpha\\) for both \\(r(X_j) = \\mathrm{E}[X_j \\mid F(Y) &gt; \\alpha] + c \\dfrac{Cov(X_j, Y \\mid F(Y) &gt; \\alpha)}{Stdev(Y \\mid F(Y) &gt; \\alpha)}\\) Table 18.3: Co-Measures Risk Measure \\(h(Y)\\) \\(L(Y)\\) Condition Co-Measure \\(TVaR\\) \\(Y\\) \\(1\\) \\(F(Y) &gt; \\alpha\\) \\(\\mathrm{E}[X_j \\mid F(Y) &gt; \\alpha]\\) \\(VaR\\) \\(Y\\) \\(1\\) \\(F(Y) = \\alpha\\) \\(\\mathrm{E}[X_j \\mid F(Y) = \\alpha]\\) Standard Deviation \\(Y - \\mathrm{E}[Y]\\) \\(\\dfrac{Y - \\mathrm{E}[Y]}{\\sigma_Y}\\) none \\(\\dfrac{\\mathrm{Cov}(X_j,Y)}{\\sigma_Y}\\) \\(EPD\\) \\(X - \\mathrm{E}[X \\mid F(Y) = \\alpha]\\) \\(1-\\alpha\\) \\(F(Y) &gt; \\alpha\\) See 1. \\((CoTVaR - CoVaR) \\cdot (1-\\alpha)\\) 18.4.5 Using Decomposition If the allocated risk measure \\(r(X_j)\\) is a decomposition of the company risk measure \\(\\rho(Y)\\) then we can use the measure to measure risk-adjusted profitability of a BU \\[\\dfrac{P_j}{r(X_j)}\\] If the risk measure \\(\\propto\\) the market value of risk \\(\\Rightarrow\\) the BU with higher ratio have more profit relative to the value of risk they are taking But since don’t know how to determine the market value of risk (i.e. don’t know which risk measure is \\(\\propto\\) the value of risk) \\(\\Rightarrow\\) Use several risk measures and hope one is close and that the indicated strategic directions are consistent with each other Question on how to determine the market value of risk have not been settle yet, likely a risk measure on a transformed probability but we don’t know yet "],
["allocating-the-cost-of-capital.html", "18.5 Allocating the Cost of Capital", " 18.5 Allocating the Cost of Capital Compare profitability among the BU by allocating cost of capital for each BU 2 possible definitions for cost of capital \\(C_j\\) Option pricing: Value of the right to call upon the capital of the firm Added value = \\(P_j - C_j\\) Caveat: Difficult to value as the timing is not fixed in advanced Market value of a stop loss that attach at zero profits for the BU Start with expected value of the stop-loss Use probability transform and theory of pricing in incomplete markets (e.g. MET) Practical estimate: Mean + 30% of s.d. would apply a consistent risk load to the BU "],
["summary.html", "18.6 Summary", " 18.6 Summary Allocating capital, even using marginal decomposition is arbitrary and artificial Arbitrary because different risk measures give us different allocations and therefore different answers Artificial because the business unit has access to the entire capital of the firm, not just a portion of it Using a value added approach, by allocating cost of capital is more economically realistic -->"],
["era-2-3-regulatory-and-rating-agency-capital-adequacy-models-witcraft.html", "Chapter 19 ERA 2.3 Regulatory and Rating Agency Capital Adequacy Models - Witcraft", " Chapter 19 ERA 2.3 Regulatory and Rating Agency Capital Adequacy Models - Witcraft Different ways regulator regulates company’s capital adequacy Leverage ratio: Premium or reserve over surplus Its shortcomings: Does not distinguish between LoBs and nothing besides u/w risk RBC (in the 1990s) Factor model combining several risk include asset, credit, premium, reserve \\(\\star\\) Be able to criticize an example factor model Scenario testing Against scenarios from regulators Can be stochastic or static 2 options to meet capital requirements: Purchase reinsurance Issue surplus notes "],
["era2-3-intro.html", "19.1 Introduction", " 19.1 Introduction History of regulatory &amp; rating agency capital adequacy models Prior to 1990: Leverage Ratios After 1990: Risk Based Capital Model (RBC) Soon after RBC: Scenario Testing (NY interest rate scenarios for life insurers) DCAT in Canada More recently: Stochastic Scenario Testing "],
["era2-3-leverage.html", "19.2 Leverage Ratios", " 19.2 Leverage Ratios IRIS Ratios Developed in early 1970s in the US If companies fail 4 of 12 IRIS tests they will get regulatory scrutiny Still in use today but less weight is given compared to other regulatory capital adequacy measures In US prior to the 1990s (before RBC), 2 leverage ratio tests were used: \\(\\dfrac{\\text{Premium}}{\\text{Surplus}} &lt; 3.0\\) \\(\\dfrac{\\text{Reserves}}{\\text{Surplus}} &lt; a\\) where \\(a\\) is fixed (e.g. \\(a=3.0\\)) This penalize long tail lines more In EU Solvency I: Required capital = higher of: \\(\\dfrac{\\text{Premium}}{\\text{Surplus}}\\) and \\(\\dfrac{\\text{Incurred Claims}}{\\text{Surplus}}\\) And also net leverage: \\(\\dfrac{\\text{Premium} + \\text{Reserves}}{\\text{Surplus}}\\) Does not distinguish between LoBs and nothing besides u/w risk "],
["era2-3-rbc.html", "19.3 Risk-Based Capital Models", " 19.3 Risk-Based Capital Models Main advances Combining several risk include asset, credit, premium, reserve Factor models: Factors vary with the quality and type of asset or LoB Factors applied to accounting values Used in UK, AUS, US, CA, JAP, AM Best and S&amp;P Models recognize accumulation risk (cat) and aggregate loss instead of just occurrence amounts (most of them 1-in-100 or 1-in-250) Factors very significantly between jurisdictions AM Best has much higher factors than the rest: Rating agencies focus on long term viability vs regulatory focus on one year survival Correlation adjustment reduces the combined risk charges especially when the factors are of similar size 19.3.1 Credit Risk Largest component is reinsurance recoverable Many models vary the factors with the credit quality of the reinsurers AM Best: Increases credit charge for companies with high \\(\\dfrac{\\text{Reinsurance Recoverable}}{\\text{Surplus}}\\) UK: Premium ceded to one reinsurer can not &gt; 20% of gross premium Recoverable from an insurance group cannot &gt; 100% of surplus 19.3.2 Reserve Risk Similar to premium factors, vary by LoB and applied to net reserves Japan: Reserve levels are low as payments are made quickly, factor is applied to net loss payment 19.3.3 Accumulation Risk Some models have started to use accumulation risk (but many do not use it yet) Focus is on 1-in-100 or 1-in-250 events "],
["era-2-3-scen-test.html", "19.4 Scenario Testing", " 19.4 Scenario Testing Some regulators require test of capital against a set of scenarios Static scenarios in Canada (DCAT) Stochastic in UK and Australia AUS: 99.5% of 1 year survival rate UK: also have 3 years 98.5% and 5 years 97.5% survival Stochastic modeling requires: Forecasting financials for 1-5 years Probability distn (for many risk sources) where they can be developed Dependencies among risks Reflection of Management Responses "],
["era-2-3-eval-cap-stra.html", "19.5 Evaluating Capital Structure Strategies", " 19.5 Evaluating Capital Structure Strategies 2 options to meet capital requirements: Purchase reinsurance Impact: Premium \\(\\downarrow\\); Capital Requirement \\(\\downarrow\\) Reserves \\(\\downarrow\\); Capital Requirement \\(\\downarrow\\) Reinsurance Recoverable \\(\\uparrow\\); Capital Requirement \\(\\uparrow\\) Recoverables change is about 25% of the premium and loss reserve charge Change in premium or reserve charge have a larger impact then to the recoverable charge due to the covariance adjustment Changes to small risk charges have a small impact on the total risk charge Annual cost: Ceded Premium \\(\\times\\) Profit Margin Issue Surplus Notes Annual cost: (Surplus note yield - Bond yields) \\(\\times\\) Face Value of Surplus Note However surplus note is a longer commitment as they can not be repaid for a certain time period Reinsurance might be cheaper and the company can exit quickly -->"],
["era-2-4-asset-liability-management-brehm.html", "Chapter 20 ERA 2.4 Asset-Liability Management - Brehm", " Chapter 20 ERA 2.4 Asset-Liability Management - Brehm Asset liability Management definition 20.1 Optimal portfolio under 4 different scenarios Going concern is most complicated as it includes cash flow coming in Additional considerations with tax and equity If we are not matching the duration, best to just invest in long bonds 9 steps of asset liability modeling Model Asset Classes, Liabilities, and Current Business Operations Define Risk Metrics Return Metrics Time Horizon Consider Relevant Constraints Simulation Model Efficient Frontier Graph Liabilities Review Results Areas for future research are correlations and reserve models that are explanatory model "],
["era2-4-intro.html", "20.1 Introduction", " 20.1 Introduction Asset-Liability Management \\(\\neq\\) Matching ALMatching: focus on hedging interest rate risk by matching duration or cashflow matching Definition 20.1 (AL Management) Looks at current assets and liabilities + flow from future premiums of a going concern company Identify and exploit hedges of any sort (e.g. use equity as inflation hedge) Insurance liabilities are less liquid than investments \\(\\leadsto\\) Focus mostly on investment With that said, company can still consider u/w and other hedges like reinsurance (that manages the liability instead) "],
["four-op-ptf-era.html", "20.2 Optimal Porfolio Under 4 Scenarios", " 20.2 Optimal Porfolio Under 4 Scenarios From Venter et al. (“Implication of Reinsurance and Reserves on Risk on Investment Asset Allocation”) 1) Assets (No Liabilities) Porfolio Short term treasures: risk free and maintain value well Long bond and stocks are risky Use modern portfolio theory to find alternatives on the efficient frontier 2) Known Liabilities and Known Cash Flows (Fixed in Time and Amount) Investment duration: Short duration \\(\\Rightarrow\\) Reinvestment risk Long duration \\(\\Rightarrow\\) Interest rate risk Risk and conclusions are different if liabilities are discounted at current rates (Such as using Economic balance sheet) 3) Liabilities are Variable and Timing is Variable Precise matching is impossible Requires model that incorporate asset and liability fluctuations Inflation sensitive liabilities further complicates things 4) Going Concern Company, with Cash Flow from Current Operations Can have positive or negative cash flow If asset prices are too low, can pay claims from premium cash flows and use depreciated assets to support loss reserves Need to model: premium, income, losses, cat losses, expenses Need enterprise wide model with holistic view to handle the complexity 20.2.1 Additional Consideration: Tax Investment strategy change in responses to u/w cycle Profitable period: Tax exempt bonds are better Unprofitable period: Taxable bonds are better as the investment income is reduced by u/w losses Assets is also reallocated to maximize income while avoiding Alternative Minimum Tax 20.2.2 Additional Consideration: Equity Generally considered risky and imply a potentially worst downside risk to capital However, they can be useful hedge against inflation Can be tested with an ERModel, but conclusions will be sensitive to input assumptions of the macroeconomic model 20.2.3 VFIC 2002 Published by CAS’ Valuation, Finance &amp; Investment Committee (VFIC) Testing the optimality of duration matching assets &amp; liabilities Simulation models were used against the following scenarios: Long tailed business vs short tailed business (w/ CAT) Profitable vs unprofitable (Generate cash and consumes cash) Growing vs shrinking companies Results were reviewed on GAAP, Stat, Economic B/S basis and several risk measures were used form each accounting basis: Duration matching was one of the optimal strategies Investment choice depends on: Risk metrics selected Return metrics (Venter used US GAAP pretax \\(\\Delta\\) in surplus) Risk tolerance or preference Results for Economic Balance Sheet: Assets are mark to market, liabilities are discounted at current rates Duration match results in low interest rate risk Longer duration is better if not matched: Long duration investments: \\(\\uparrow\\) interest rate risk \\(\\uparrow\\) returns Short duration investments: \\(\\uparrow\\) reinvestment rate risk and \\(\\downarrow\\) return Choosing longer duration vs matching is a value judgement (i.e. is the extra return worth the extra risk) Other Remarks: Duration match is irrelevant for Stat and GAAP accounting as they are not responsive to interest rate movement Adding cash flows from continuing operations complicates this analysis "],
["era-2-4-ALM-app.html", "20.3 Asset-Liability Modeling Approach", " 20.3 Asset-Liability Modeling Approach 9 steps for enterprise-wide model for Asset Liability Management 20.3.1 Step 1: Model Asset Classes, Liabilities, and Current Business Operations Start with models for the following: Asset classes: interest rate model, stock price model, FX model Liabilities: response to inflation/interest rate or economic variables like GDP growth Business operations: premium, expense, response to u/w cycle and economic environment 20.3.2 Setp 2: Define Risk Metrics Risk metrics for different accounting basis: Stat, GAAP, Economic Income based metrics \\(\\sigma\\) of income QtQ, YtY Probability of not meeting earning target \\(\\Delta\\) in surplus or equity B/S based metrics Focus on level of surplus or equity of firm \\(\\sigma\\) and probability of not meeting target VaR, TVaR, WTVaR Probability of ruin or impairment Time Frames Typically 1-5 years 20.3.3 Step 3: Return Metrics Use consistent accounting basis as the risk metrics Income based: Quarterly earnings B/S based: RoE, Terminal value of equity at period end 20.3.4 Step 4: Time Horizon Single period is simpler Multiperiod is more accurate More difficult Include serial correlations of variables: Interest rates Level of insurance prices in market 20.3.5 Step 5: Consider Relevant Constraints Considerations out side of model indications: Asset limits imposed by regulators Cost of regulatory capital of asset class RBC or BCAR capital scores Company’s own investment policy 20.3.6 Step 6: Simulation Model Consider and varies: U/w strategies Reinsurance options Investment strategies Risk and return metrics are calculated over these simulations 20.3.7 Step 7: Efficient Frontier Graph Construction from various portfolio options Based on current portfolio, options with same return but less risk or higher return with same risk should be consider (in between current and the frontier is fine as well) 20.3.8 Step 8: Liabilities Liabilities (in particular future loss reserves) can be modify as well (besides just assets) \\(\\Delta\\) u/w strategies Reinsurance Should analyze various reinsurance structure and compare results Important in multiperiod model 20.3.9 Step 9: Review Results Identify situations that the preferred portfolio performed poorly Develop hedging strategy for those situations Review may highlight type of prevailing conditions that lead to substandard performance e.g. large CAT loss that forces liquidation of assets in soft market Can establish monitoring mechanisms to identify the likelihood of such conditions and make adjustments when they are noticed "],
["future-research-1.html", "20.4 Future Research", " 20.4 Future Research Correlations: Between LoBs; asset &amp; liabilities; over time (low most of the time and high in a crisis) Can materially alter the optimal portfolio Models of unpaid losses are not explanatory model Forecast mean and distribution but, Do not predict loss payments based on economic variables (interest rates, inflation, GDP) Sometimes inflation is hypothesized but rarely explicitly developed from the historic economic data \\(\\hookrightarrow\\) Model is unable to use the results of ESG to make meaningful predictions Asset model do this better (equity, bond pricing) Alternative is to treat the correlation parameters and inflation sensitivity as random variables, then models can be created with parameter estimates -->"],
["era-2-5-measuring-value-in-reinsurance-venter-gluck-brehm.html", "Chapter 21 ERA 2.5 Measuring Value in Reinsurance - Venter, Gluck, Brehm", " Chapter 21 ERA 2.5 Measuring Value in Reinsurance - Venter, Gluck, Brehm Value of reinsurance Provides stability Based on reinsurance premium and expected recoveries Metrics 21.1 and 21.2 Amount of protection Metrics 21.3 Net Premium - Net loss Metrics 21.4, 21.5, 21.6 and 21.7 Distributions Density and CDF Space needle diagram Cost benefit diagram Combined ratio is bad \\(\\star\\) Efficient frontier Frees up capital \\(\\star\\) Cost of capital reduction metrics 21.8 and 21.9 \\(\\star\\) Capital can be based on theoretical or practical model Accumulation risk: as-if reserve 21.10 Capital consumed and the equation (21.1) Adds to the value of the firm Difficult to do but some recent study results "],
["era2-5-intro.html", "21.1 Introduction", " 21.1 Introduction Value of reinsurance Provides stability Frees up capital Adds to the value of the firm Naive calculation: Sum(cash flow over contract length) (-) Premium and reinstatement (+) Ceding comm and reinsurance loss \\(\\hookrightarrow\\) Will always show reinsurance is a bad deal especially after discounting the cashflow Make sense since reinsurers need to make a profit Cases where it’s “profitable” to the cedant, it’ll either be repriced or the cedants results are really poor "],
["era-2-5-sec-1.html", "21.2 Quantifying Stability and Its Value", " 21.2 Quantifying Stability and Its Value Measuring the value of reinsurance is more than calculating the expected cashflow from the reinsurance program Significant judgement is required to evaluate the cost-benefit tradeoff Metrics to measure and value stability: Ratio and difference of \\(\\text{Reinsurance Premium}\\) and \\(\\mathrm{E}[\\text{Recovery}]\\) Expected loss under different programs \\(\\text{Premium} - \\mathrm{E}[\\text{Loss}]\\) under different programs \\(\\sigma\\), percentiles, whole distribution Space needle view, Distribution of other metrics cost benefit diagram, pre tax net income, combined ratio Efficient frontier charts 21.2.1 Reinsurance Premium and Expected Recoveries Definition 21.1 (Metrics 1) \\[\\dfrac{\\mathrm{E}[\\text{Recovery}]}{\\text{Reinsurance Premium}}\\] Definition 21.2 (Metrics 2) Net Cost of Reinsurance \\[\\text{Reinsurance Premium} - \\mathrm{E}[\\text{Recovery}]\\] Remark. On a PV basis? \\(\\mathrm{E}[Recovery]\\) is net of reinstatement premiums, typically based on simulation results Should gauge how significant the net cost of reinsurance is by comparing to the firm’s expected earnings for the year 21.2.2 Amount of Protection Definition 21.3 (Metrics 3) Compare \\(\\mathrm{E}[\\text{Net Loss}]\\) \\(\\forall\\) programs \\[\\mathrm{E}[\\text{Net Loss}] = \\mathrm{E}[\\text{Gross Loss}] - \\mathrm{E}[\\text{Recovery}]\\] 21.2.3 Premium Less Expected Loss Going forward we are considering Net \\(\\text{Premium}\\) - Net \\(\\mathrm{E}[\\text{Loss}]\\) Only consider premium and losses Does not include expenses or investment income Definition 21.4 (Metrics 4) \\[\\text{Premium} - \\mathrm{E}[\\text{Loss}]\\] Compare \\(\\forall\\) programs Definition 21.5 (Metrics 5) \\[\\sigma_{\\text{Premium} - \\mathrm{E}[\\text{Loss}]}\\] Caveat: Can be lowered by removing favorable outcomes Definition 21.6 (Metrics 6) 1st percentile (most favorable) \\(\\text{Premium} - \\mathrm{E}[\\text{Loss}]\\) Definition 21.7 (Metrics 7) Worst simulated outcome \\(\\text{Premium} - \\mathrm{E}[\\text{Loss}]\\) Might be too extreme to look at 1-in-25K (99.996%) Remark. Here we are sticking to high percentile means bad, similar to how we used to look at loss, but can flip it for looking at earnings 21.2.4 Distribution Based Density of \\(\\text{Premium} - \\mathrm{E}[\\text{Loss}]\\) Compare shapes for different programs See if it’s giving up the upside and like how does the tail look CDF of \\(\\text{Premium} - \\mathrm{E}[\\text{Loss}]\\) Pick a percentile and read it across the graph to find the value at that percentile for each curve Look at the difference between different programs at each percentile Note on difference of distribution: Event that generate a given percentile is different across programs Reinsurance changes the order of the outcomes We are interested in the difference of the distributions NOT interested in the distribution of differences (as it doesn’t make sense? due to the above bullet point) Goal here is the choose a reinsurance program and its associated distribution \\(\\Rightarrow\\) More useful to look at the distn themselves Interested in the cost-benefit trade off, cost is the net cost of reinsurance and benefit is the protection against adverse deviation Space Needle View on \\(\\text{Premium} - \\mathrm{E}[\\text{Loss}]\\) Shows different percentiles Colored section is proportional to the probability that the result is in that range Easy to compare each quantile across programs Shows the shape of the distribution Cost Benefit Diagram Cost of reinsurance (for each program) vs loss @ each percentile X-axis being the cost of reinsurance Y-axis is the loss amount (@ given percentile) We should expect the loss to decrease as we increase the cost of reinsurance Pre-Tax Net Income Include expenses and investment income Look at value at each iterations of the sim (percentile) Get a perspective on overall profitability e.g. probability of negative earnings We can look at the probability of negative earnings CDF on Combined Ratio Caveat: Can give distorted results when the net premium is reduced due to significant ceded premium Expense ratio will be higher due to lower net premium (denominator) Can make results look worst for program that have big cession 21.2.5 Efficient Frontier Charts Construct the efficient frontier with a given risk metric (x-axis)and return metric (y-axis): Based on different reinsurance structure options For different percentile of the return metrics Remark. Plot increasing risk to the right so it looks more familiar Look for programs that are up and left along the efficient frontier with the lowest risk metric for a given return metric Return metrics: e.g. Earnings Risk metrics: Probability of making plan Probability of: Surplus &lt; 2 \\(\\times RBC\\) Surplus &lt; BCAR score supporting a target rating Expected loss in a 10 year period, \\(TVaR_{90\\%}\\), exceeds a threshold level or surplus (e.g. \\(TVaR_{90\\%}\\) should not exceed 20% of surplus) x% drop in quarterly EPS "],
["era-2-5-sec-2.html", "21.3 Reinsurance as Capital", " 21.3 Reinsurance as Capital Capital is held to pay for claims when the premium is not sufficient to do so Highly variable results will require more capital \\(\\Downarrow\\) Reinsurance reduce volatility of results \\(\\Rightarrow\\) reduce capital required \\(\\Downarrow\\) Reinsurance can be thought of as borrowed capital We value the reinsurance by the impact it has in reducing Capital (or Cost of Capital) \\(\\therefore\\) We compare the reduction in cost of capital to the cost of the reinsurance Measure the amount of capital reduction that the insurer receives from the reinsurance program Compare the cost of capital reduction from reinsurance program i.e. Compare the marginal cost of reinsurance to the marginal capital to judge the value of reinsurance Definition 21.8 Cost of capital reduction: Metrics 1 Net Benefit = |\\(\\Delta\\)Cost of Capital| - |Net Cost of Reinsurance| Net benefit &gt; 0 then accept Net cost of reinsurance = \\(\\text{Reinsurance Premium} - \\mathrm{E}[Recovery]\\) on a PV basis Alt formula: Net benefit = Net Cost of Reinsurance - \\(\\Delta\\)Cost of Capital Review other criteria when the dollar benefit is the same Definition 21.9 Cost of captial reduction: Metrics 2 \\(ROE = \\dfrac{\\text{Net Cost of Reinsurance}}{\\Delta \\text{Capital}}\\) Compare return on equity We want \\(ROE\\) below the cost of capital here and that lower is better since we are reducing capital Think about it if you’re going from the plan interested to gross, if the \\(ROE\\) is lower you don’t want to change so the current plan is better Tink of this as investing negative capital, and \\(\\therefore\\) we want a low ROE 21.3.1 Change in Capital 2 different methods to determine the change in capital from the reinsurance structure Theoretical models: Use risk measures we discussed earlier (e.g. VaR, TVaR) Practical models: Use captial requirements from rating/regulatory agencies 21.3.1.1 Theoretical Models A risk metric is selected as a proxy to represent the economic capital required e.g. \\(VaR\\), \\(TVaR\\), multiples of \\(VaR\\), \\(XTVaR\\), etc Procedure: Calculate Expected Earnings \\[\\text{Expected Earnings} = \\text{Net Premiums} - \\text{Expense} - \\text{Expected Losses}\\] Calculate Economic Captial \\[\\text{Economic Capital} = \\text{Net Premium} - \\text{Expenses} - \\text{Risk Measures}\\] Measure the \\(\\Delta\\) in Economic Capital and Expected Earnings to determine the comparison metrics above (21.8 and 21.9) We measure the \\(\\Delta\\) against a base (e.g. bare or current) Here \\(\\text{Net Cost of Reinsurance} = \\Delta \\text{Expected Income}\\) Remember that lower \\(ROE\\) is better 21.3.1.2 Practical Models Use capital requirements from rating agencies (e.g. BCAR, S&amp;P, CAR, RBC, ICAR, etc) Typically set target as a multiple of a given regulatory metric (e.g. 175% BCAR or 4 times RBC authorized control level) Reinsurance reduces the capital required due to: Lower net premium and net reserves With a slight offset from increase counterparty risk Pros/Cons Pros: Easier to implement (no need to model loss distn and correlation) Cons: Not as accurate because the measurement of risk is based on proxies and not the actual risk itself e.g. stop loss only have small impact on premium but can have large impact on a risk (like the tail) that the method might not pick up Alternatively, model a year out and predict the probability of falling below certain thresholds \\(\\Rightarrow\\) Capital could be set so that the probability is lower But then we’ll have to model the I/S and reinsurance structure and everything 21.3.2 Theorectical Model Example: Marginal ROE Compare the marginal cost of reinsurance to the marginal capital to judge the value of reinsurance Pick a risk metric as proxy for capital required Calculate marginal capital relative the current program Calculate \\(\\Delta\\) NPV Net Benefit = \\(\\Delta\\) NPV Ceded Loss - \\(\\Delta\\) NPV Ceded Premium Calculate \\(ROE = \\dfrac{\\Delta \\text{NPV Net Benefit}}{\\text{Marginal Capital}}\\) Calculate after-tax \\(ROE\\) Compare the \\(ROE\\) (We still want it lower?) Bare in mind that a high ROE is meaningless if the amount of capital you can invest is small 21.3.2.1 XTVaR Might be better to look at a multiple of XTVaR at a lower percentile XTVaR is the unfunded part of TVaR \\[XTVaR_{\\alpha} = \\mathrm{E}[Y \\mid F(Y) &gt; \\alpha] - \\mathrm{E}[Y]\\] VaR is a single point and can be volatile at high percentiles 21.3.3 Accumulation Risk So far we implicitly assumed that the capital is held for only 1 year Capital calculated so far was static without regard to how long it takes to run it down as claims are paid Capital needs to be held until all claims are paid 21.3.3.1 “Life-time” Amount of Capital \\[\\begin{align} \\text{&quot;Lifetime&quot; amount of capital} \\:\\: = \\:\\: &amp; \\text{Capital for new business} \\\\ &amp;+ \\text{Capital to support all unpaid claims from prior years} \\\\ \\end{align}\\] \\(\\therefore\\) Set the capital requirements based on premium and reserves Caveat: Current reserves may be based on business that is different from this year’s book (e.g. size, mix of business, etc) Use “as-if” reserves for capital calculation Definition 21.10 (As-if Reserves) Calculate the reserves as if the company had been writing the same book the last several years Adjust for inflation Especialy important for long tailed lines where capital requirement for the accumulated outstanding reserve can be much large than the requirement for new business Focus here is to determine the impacts on capital so we can assign a cost to each reinsurance program When comparing reinsurance program, the reserves would be calculated under each reinsurance program Capital calculated for both the new premium, and the loss reserves is a surrogate for the PV of the capital required for the current book over its lifetime 2 Advantages for the “as-if” reserves Measure the impact of accumulated risk caused by correlated factors Likely means “correlated risk factors” that can influence all those accident years (e.g. inflation, change in law) Alternative reinsurance programs can be applied to the premium and as‐if reserves, providing a more valid measure of the accumulated risk and capital used over the full life of the AY When aggregating AYs, we get temporal risk (risk based on time) that affect all AYs at the same time (e.g. severity trend and auto correlation of severity trend) Including all AYs create a bigger spread of possible outcomes 21.3.4 Capital Consumed Concept: How much capital is needed to pay deficiencies Plot/compare PDF and CDF of Capital Consumed: \\[\\begin{equation} \\text{Capital Consumed} = - PV[\\text{Premium} + \\text{Reserves} - \\text{Losses} - \\text{Expenses}] \\tag{21.1} \\end{equation}\\] Apply Reinsurance Example When modeling XoL contracts we need to model severity trend Severity trend impacts all open claims Payment pattern is important as claims that are open longer will have more severity trend applied We can base the capital consumed on different risk measures (e.g. VaR and TVaR) \\[ROE = \\dfrac{\\text{Expected Profit}}{\\text{Capital Consumed}}\\] Do we want increase or decrease ROE? Probably increase Comment from notes that author is not sure about "],
["era-2-5-sec-3.html", "21.4 Reinsurance and Market Value", " 21.4 Reinsurance and Market Value Measure the impact on the value of the firm instead of the impact on capital Given 2 actions, the one that increases the value the most on a risk-adjusted basis would be preferred Caveat: We don’t know how this can be done Currently: we measure value of the stability that reinsurance provides by measuring the reduction in capital Next step: Measure the value of reinsurance by measuring the \\(\\Delta\\) in the value of the company Highlights from recent studies Insureds demand price discounts of 10-20 times the expected cost of an insurer default (EPD) e.g. 0.5% default, expected cost = 100K if default \\(\\Rightarrow\\) EPD = 500 \\(\\Rightarrow\\) Discount demand = 500 \\(\\times\\) 10 or 20 = 5-10K 1% decrease in capital \\(\\Rightarrow\\) 1% loss in pricing level 1% increase in \\(\\sigma_{earnings}\\) \\(\\Rightarrow\\) 0.33% decrease in pricing level Rating upgrades is worth 3% of business growth Rating downgrades can drop business by 5-20% "],
["conclusion-2.html", "21.5 Conclusion", " 21.5 Conclusion Use cost-benefit analysis to compare reinsurance programs Select metrics to determine the benefit of the reinsurance and compare to cost for each reinsurance programs Net cost of reinsurance = NPV of expected decrease in earnings from purchasing reinsurance = \\(\\Delta\\) PV[Expected loss - Premium - Reinstatement + Commission] Combined ratio can be distorted due to expense ratio Stability is the simplest measure of benefit But \\(\\sigma\\) can be misleading as they can be lowered by eliminating favorable results Useful to look at the differences of distributions from the reinsurance options Efficient frontier is useful If we can measure the increase of value of the firm that would be great Increased earnings from reduced financing costs Higher premium from better claims paying ratings Can’t do this right now so the substitute is to measure the reduction in capital requirements Value of reinsurance can be measures vs: A cost of capital, or Capital (ROE) "],
["past-exam-questions-16.html", "21.6 Past Exam Questions", " 21.6 Past Exam Questions TIA Exercise \\(\\star\\) TIA 1: Benefit of using a utility function \\(\\star\\) TIA 2: Reinsurance selection based on RORAC and EVA and reason for not doing allocation \\(\\star\\) TIA 3: Why might a non-marginal allocation lead to bad decision TIA 4: TVaR and VaR calc on a simple distribution TIA 5: Good properties for risk measure allocation Does not ignore risk, if a risk is relevant to the BU, then is should be considered in the capital allocation for pricing TIA 6: Calculate different VaR measure for a givne \\(f(x)\\) \\(\\star\\) TIA 7: blurred VaR and why it is useful TIA 8: Factor model and it’s deficiency TIA 9: UK stochastic model test \\(\\star\\) TIA 10: ALM application TIA 11: Why not duration match, impact of changing interest rate under different accounting \\(\\star\\) TIA 12: All the reinsurance metrics TIA 13: Efficient frontier Past Exam 2011 #16: allocation method and use of transformed probabilities \\(\\star \\star\\) 2011 #17: reinsurance options based on capital cost saving 2011 #19: Compare VaR and TVaR, calculate EPD \\(\\star\\) 2012 #13: Limitations of sample factor method and risk measures; how to reduce capital charge \\(\\star\\) 2012 #18: Pick the risk measures appropriate 2012 #20: Reason for purchasing reinsurance and what form to purchase \\(\\star \\star\\) 2014 #22: risk adjusted capital and allocation with proportional and marginal allocation by first principle 2015 #24: Definition and limitation of VaR TVaR risk adjusted \\(\\star\\) 2015 #26: efficient frontier and reinsurance picking 2016 #22: ALM for soft and hard market \\(\\star\\) 2016 #24: Risk measures for differet LoBs 21.6.1 Question Highlights n/a -->"],
["era-3-1-considerations-on-implementing-internal-risk-model-mango.html", "Chapter 22 ERA 3.1 Considerations on Implementing Internal Risk Model - Mango", " Chapter 22 ERA 3.1 Considerations on Implementing Internal Risk Model - Mango Just know the 4 sections and have an idea of what it covers Startup: Staffing and Scope Scope and purpose Resource commitment and good controls over I/O IRM Parameter Development Data quality, company characteristics, differing risk attitudes Expert opinion, validation, modeling software Implementation Priority setting, communications, pilot program, education Integration and Mainteneance Corporate calendar, update frequency and controls "],
["era3-1-intro.html", "22.1 Introduction", " 22.1 Introduction Key considerations before creation of internal risk model These are decisions at the beginning of the process How the input parameters should be determined How to overcome political hurdles How to integrate model into the company’s business planning Things firm often underestimate when implementing an IRM (this is on a similar scale as implementing a new ledger or reserve review process) Resource commitment: Hiring staff, purchasing systems, software, consultants Timelines: Can take years to implement Organizational impact: Can have large impact on how the company is run, how decisions are made If done properly, IRM can be a hub for: Risk decision making Planning Pricing Reinsurance purchasing Capacity allocation Rating agency interaction Possible components of a risk model: Freq/sev distribution for each LoBs Premium and LR projections for the forecast year Correlations across LoBs Model for distributions of the unpaid by LoB Correlations of the unpaid ESG that simulates economic conditions, which can be used as an input to some of the other model components Asset / market risk model that model asset prices Potential use of the risk model: Model u/w losses for the next year Reserve movements Potential outputs of the risk model: \\(\\sigma\\) of LR for the current AY for each LoB \\(VaR\\) and \\(TVaR\\) for the same LoB at various percentiles \\(\\sigma\\), \\(VaR\\), \\(TVaR\\) of loss reserves Allocation of various capital metrics (e.g. \\(5 \\times VaR_{90\\%}\\)) to each LoB "],
["era-3-1-sec-1.html", "22.2 Startup: Staffing and Scope", " 22.2 Startup: Staffing and Scope Critical: Scope and purpose needs to be be agreed upon by all key stakeholders at the start Other decisions depends on this Fail to do so will lead to backtracking on decisions later on and delay Managing expectations is key IRM has the potential to appear to be all things to all people Different departments will think it will solve a myriad of problems for them Staffing Clear reporting lines for modeling team so direction can be followed Solid vs dotted line (matrix) reporting Multi function oversight committee Key functions that need to be represented U/w, planning, finance, actuarial, risk Resource Commitment: Full time or part time (e.g. temporary for implementation and permanent for upkeep) Inputs &amp; Outputs Needs good control similar to accounting figures, see integration section for further discussion Templates that provide analysis could be locked down to prevent misuse Input example: premium, loss, exposure Output example: \\(\\sigma\\) of LR or profits, \\(VaR\\) and \\(TVaR\\), correlations Purpose, examples: Quantify variation around the plan Provide objective view on the distribution of company LoB results Scope Prospective u/w year only? Or includes reserves, assets? What about op risk? Level of detail: Low detail on whole company, or High detail on a pilot segment 22.2.1 Recommendations Imperative for the IRM team to report to a leader with a reputation for fairness and balance The actual functional reporting line is less important Resource commitment: think of IRM implementation of a new competency Firm will need to hire or transfer employees for the new department Need good controls over inputs and outputs similar (similar to that used with a ledger or reserving) Define initial scope as clearly as possible "],
["era-3-1-sec-2.html", "22.3 IRM Parameter Development", " 22.3 IRM Parameter Development Potential challenges on parameters that need to be developed Data quality Unique characteristics of the company or LoB Differing risk attitudes Expert opinion is important in developing parameters Especially for segments with small volume and low quality data This will increase the time to implement Correlation assessment is important and challenging Lack of data High political sensitivity Spans multiple business units Significant impact on the overall risk profile and capital allocation Validation will be difficult as there are no model to compare Validate by reviewing a series of tests involving complementary variables, e.g.: Compare extreme outcomes from the model to actual extreme outcomes Compare variation of loss ratios to historical variation Model accurately handle correlation Capital allocation make sense? (long tail and cat prone risk have more capital) Gain comfort with the reasonableness of the model through the use of many metrics 22.3.1 Recommendtaions Modeling software How much is pre-built from purchased software What needs to be built in-house Skills of the team should align with what they need to build e.g. Buy an ESG unless your team have finance and economics experts with time series modeling Parameter development Need to develop a systematic way to capture the expert opinion (product expertise) from the following departments: U/w, planning, claims, planning and actuarial Correlations Needs to be owned at a high level (C-suites) as it crosses LoBs and have significant impact on the allocated capital IRM team will provide recommendations on correlation assumptions Validation Validate and test over extended period Provide training so that interested parties all have a basic understanding of the model "],
["era-3-1-sec-3.html", "22.4 Implementation", " 22.4 Implementation Have senior management set priority and timeline for IRM analysis and rollout to prevent ambushes Give opinion leaders reasonable time frames to raise their concerns and have them addressed and not the power to veto or delay the rollout Don’t attempt to sell the concept to various opinion leaders before moving forward There will be a large appetite for the results of the IRM Provide access to the output from the model consistently and predicatively 22.4.1 Recommendations Priority Setting: Have top management set the priority for implementation Communications: Regular communication to a broad audience Pilot Testing: Allows effective preparation of the company for the magnitude of the change Education: Target training so leadership has a similar base level of understanding "],
["era-3-1-sec-4.html", "22.5 Integration and Mainteneance", " 22.5 Integration and Mainteneance Integrate IRM into an organization’s existing corporate calendar: Budget season Certain parameters should be set (e.g. correlations, CoV’s) at the beginning of budget season Others can be set as part of the budget process (e.g. Premium) Timeline for review of internal financials Timeline for bonus pool Update frequency Major updates should be no more than twice a year Minor updates can be handled by scaling \\(\\sigma\\) can be calculated by keeping the CoV constant Correlation parameters don’t need to be updated very frequently Input/Output Needs clear ownership similar to accounting figures Source of inputs Who published the outputs and reports Have analytical templates that are endorsed by IRM to prevent misuse 22.5.1 Recommendations Cycle: Integrate into annual planning process Updating: Major updates no more than twice a year; minor update with scaling Controls: Maintain centralized controls of inputs, outputs and even application templates -->"],
["era-3-2-modeling-parameter-uncertainty-venter-gluck.html", "Chapter 23 ERA 3.2 Modeling Parameter Uncertainty - Venter, Gluck", " Chapter 23 ERA 3.2 Modeling Parameter Uncertainty - Venter, Gluck Parameter risk impact big companies more as their process risk is smaller (23.1) Projection risk Different ways to model the severity trend Model the trend independently or adjust the general inflation then model the superimposed inflation Model with AR(1) (23.2) Estimation risk in selecting the parameters and the downstream impact Use MLE and negative log liklihood Slope of the negative log likelihood determines our confidence in the selection Infomration matrix (23.3) and correlation matrix (23.4) use with joint lognormal to correlate parameters Model risk Use HQIC to adjust the loglikelihood with a penalty for # of parameters Also use a pool of distribution of reasonable parameters Projection models Form of the model we use to do the modeling e.g. expected parameters vs pool of distribution of parameters Should focus on the spread of potential outcomes instead of best estimate Add flexibility and let the parameters reflect the uncertainty in the assumptions "],
["era3-2-intro.html", "23.1 Introduction", " 23.1 Introduction This was first touched on in ERA 1.3 when discussing the components of underwriting risk Parameter risk is the risk inherent in selecting the wrong loss distribution Key for large companies since process risk is reduced with a large book Projection risk can be modeled using time series We can also measure estimation risk in parameter selection for a distribution Model risk is the uncertainty about the distribution of the parameters themselves Paper has 2 recommendations when using projection models "],
["impact-of-parameter-risk.html", "23.2 Impact of Parameter Risk", " 23.2 Impact of Parameter Risk CoV for total losses if the frequency and severity distribution are known \\[\\begin{equation} CV^2(S) = \\underbrace{CV^2(N)}_{\\text{# of Claims}} + \\underbrace{\\dfrac{CV^2(X)}{\\mu_N}}_{\\text{Severity}} \\tag{23.1} \\end{equation}\\] For large company, \\(\\mu_N\\) is large so the 2nd term is small \\(S\\) = Total losses \\(X\\) = Severity \\(N\\) = Numbers of claims However if we add systemic factor that impacts all claims (e.g. inflation), it doesn’t diversify away as the # of expected claims \\(\\uparrow\\) Relative impact on large company is much bigger since their process variance is small For small company there is a large amount of process risk so the overall variability does not increase very much "],
["projection-risk-era3-2.html", "23.3 Projection Risk", " 23.3 Projection Risk Different ways to project trends Simple trend Severity trend and inflation Trend as time series 23.3.1 Simple Trend Forecast one trend using historical average and into the future Caveat: Additional uncertainty due to estimate of ultimate losses is uncertain 23.3.2 Severity Trend and Inflation Claim severity \\(\\neq\\) general inflation \\[\\begin{equation} [\\text{Claim Severity Trend}] \\approx [\\text{General Inflation}] + [\\text{Superimposed Inflation}] \\tag{23.2} \\end{equation}\\] 2 approaches: Model the severity trend independent of general inflation Adjust the data for general inflation and model the residual superimposed inflation For ERM where general inflation is modeled, severity trend should be dependent on the general inflation 23.3.3 Trend as a Time Series More realistic, allow for trends to change over time AR(1) with mean reverting form: \\[\\begin{array}{ccccc} r_{t+1} &amp;= &amp;m + \\alpha_1(r_t - m) &amp;+ &amp;\\epsilon_{t+1} \\\\ &amp;= &amp;(m - \\alpha_1 m) + \\alpha_1 r_t &amp;+ &amp;\\epsilon_{t+1} \\\\ &amp;= &amp;\\alpha_0 + \\alpha_1 r_t &amp;+ &amp;\\epsilon_{t+1} \\\\ \\end{array}\\] \\(m\\) is the long term mean estimated from data \\(\\alpha_1\\) is the correlation from one period to the next \\(\\epsilon_t \\sim N(0,\\sigma)\\) Simple trend understate the projection risk especially for long tail LoBs Simple trend had a 10 year forecast 99th percentile prediction error of +45% "],
["estimation-risk.html", "23.4 Estimation Risk", " 23.4 Estimation Risk Estimation risk is due to the uncertainty in selecting parameters (freq, sev, trend, variability) and specifically the downstream impact of this Maximum Likelihood Estimate For large data sets it has the lowest estimation error of unbiased estimators Uncertainty depends on the slop of the likelihood surface Steeper the slope near the MLE, the more confident we are in the estimate Measured by taking the 2nd derivative of the likelihood w.r.t. each parameters Negative of the 2nd derivative of the log likelihood = information matrix (easier to work with) \\[\\begin{equation} I = \\dfrac{\\partial^2 [\\overbrace{ -LL }^{\\text{Neg Log Likelihood}}]}{\\partial \\underbrace{\\vec{\\alpha}}_{\\text{Parameters}}} \\tag{23.3} \\end{equation}\\] Inverse of the information matrix = co-variance matrix \\[\\begin{equation} \\mathbf{\\Sigma} = I^{ -1 } \\tag{23.4} \\end{equation}\\] If slope of \\(-LL\\) is steep near the selection \\(\\Rightarrow\\) More confidence in the selection Model parameter uncertainty by: Assume join lognormal distribution for the parameters and use the correlation from \\(\\mathbf{\\Sigma}\\) from the MLE e.g. for Gamma, we might we’ll have a mean and \\(\\sigma\\) for \\(\\alpha\\) and \\(\\gamma\\) and correlation between \\(\\ln(\\alpha)\\) and \\(\\ln(\\beta)\\) A new set of parameters are selection for each iterations in the simulation Joint LogNormal distribution is selected (over joint normal) because: Eliminates negative losses Parameters estimates themselves are heavy tailed for heavy tailed distribution like Pareto e.g. \\(\\alpha\\) in simple Pareto \\(F(x) = 1 - \\left( \\frac{\\theta}{x} \\right)^{\\alpha}\\) follows inverse gamma, which is similar to LogNormal Results from simulations on small data sets showed that joint lognormal is reasonable "],
["model-risk.html", "23.5 Model Risk", " 23.5 Model Risk When considering different distributions, need to adjust the log likelihood with penalty for the number of parameters Hannan-Quinn Information Criterion (HQIC) is recommended because it is a compromise of other information criteria which add large or smaller penalties for # of parameter Create a pool of distn that are consider reasonable: Randomly select a mean for the parameters and a \\(\\mathbf{\\Sigma}\\) from the pool of distn Randomly draw the parameters based on the selected distn For each claim that is simulated, draw from the distribution with parameters from (2) "],
["projection-models.html", "23.6 Projection Models", " 23.6 Projection Models The form of the model is very important in measuring variability e.g. simple trend vs time series; expected parameters vs pool of distn for those parameters Use common sense to ensure model is structurally consistent with the underlying process Risk modeling we are more focused on the spread of potential outcomes instead of the best estimate Selecting models using parsimony will lead to unrealistically stable results Add flexibility (e.g. additional parameters) transfer assumptions from the structure to the parameters and allow the risk model reflect the uncertainty in the assumptions "],
["conclusion-3.html", "23.7 Conclusion", " 23.7 Conclusion Parameter risk is a key source of variability, especially for large companies Projection risk, estimation risk, model risk can all be quantified and applied in a simulation model -->"],
["era-3-3-modeling-and-dependency-correlations-and-copulas-g-venter.html", "Chapter 24 ERA 3.3 Modeling and Dependency: Correlations and Copulas - G. Venter", " Chapter 24 ERA 3.3 Modeling and Dependency: Correlations and Copulas - G. Venter \\(\\star\\) Correlation Pearson’s correlation: Formula (24.1) and its properties Outliers will have disproportionate weight Kendall’s \\(\\tau\\): depends on the rank Formula (24.2) for discrete and continuous (24.3) &amp; (24.4) Copulas Limitation of joint distribution and advantages for using copulas Joint distribution plots where the best is to plot the percentile for the marginal distribution Using copula and Sklar’s Theorem 24.1 Joint density function express with copula (24.5) Properties of the main copulas in table 24.1 and the partial perfect correlation copula Simulation for each of the copula Frank, Gumble, HRT is same as Frank, and Normal Gumbel’s formula (24.15) &amp; (24.16) Tail Concentration Functions Left tail and right tail concentration function for each copulas 24.1 Methods for selecting Copulas Plot the percentile plot Empirical tail concentration function Multivariate Copulas: Normal and t-copula and their properties Fitting copulas to data Using the \\(J(z)\\) and \\(\\chi(z)\\) Graph the possible \\(J(z)\\) with empirical \\(J(z)\\) to see which fits best; Similarly for \\(\\chi(z)\\) "],
["era3-3-intro.html", "24.1 Introduction", " 24.1 Introduction We need loss distribution for the whole group Create distribution for each LoB or Company Combine them into one distribution that reflects all the inter-dependencies between BUs Go through 4 copulas that are useful in practice and how to select them for a given dataset "],
["era-3-3-pearson.html", "24.2 Pearson’s Correlation \\(\\rho\\)", " 24.2 Pearson’s Correlation \\(\\rho\\) \\[\\begin{equation} \\rho = \\dfrac{\\sum_i \\tilde{y}_i \\tilde{z}_i}{\\sqrt{\\sum_i \\tilde{y}_i^2 \\sum_i \\tilde{z}_i^2}} = \\dfrac{\\mathrm{E}[YZ] - \\mathrm{E}[Y] \\cdot \\mathrm{E}[Z]}{\\sigma_{y} \\cdot \\sigma_{z}} \\tag{24.1} \\end{equation}\\] \\(\\tilde{y}_i = (y_i - \\bar{y})\\) \\(\\sigma_y^2 = \\mathrm{E}[Y^2] - \\mathrm{E}[Y]^2\\) Only appropriate for distn symmetric and have thin tail Caveat: Value far from the mean will have a disproportionate weight as it focus on the amount of each \\(\\tilde{y}_i\\) and \\(\\tilde{z}_i\\) Properties: Pearson correlation will stays the same under positive linear transformation on \\(Y\\) or \\(Z\\) Monotone function that is not linear might change the Pearson correlation "],
["era-3-3-kendall.html", "24.3 Kendall’s \\(\\tau\\)", " 24.3 Kendall’s \\(\\tau\\) Depends on the order not the value of the data Concordant: When one pair dominates the other given \\((x_1, y_1)\\) and \\((x_2, y_2)\\) \\(x_1 &gt; x_2\\) and \\(y_1 &gt; y_2\\) or, \\(x_2 &gt; x_1\\) and \\(y_2 &gt; y_1\\) Discordant: When the pair is mixed Kendall’s \\(\\tau\\) \\[\\begin{equation} \\tau = \\dfrac{C - D}{\\text{# of pairs}} \\tag{24.2} \\end{equation}\\] \\(\\tau = \\dfrac{C - D}{C + D}\\) if there are no ties \\(\\tau \\in [-1, 1]\\) with same interpretation as Pearson’s Focus on the rank \\(\\Rightarrow\\) changes in one extreme value won’t change the indication Continuous Kendall’s \\(\\tau\\) \\[\\begin{equation} \\tau = 4 \\mathrm{E}[C(u,v)] - 1 \\tag{24.3} \\end{equation}\\] \\[\\begin{equation} \\tau = -1 + 4 \\int_0^1 \\int_0^1 C(u,v)c(u,v)dudv \\tag{24.4} \\end{equation}\\] Definition 24.1 (Correlation vs Dependency) Correlation = mathematical calculation, a statistic, and measure of dependency Dependency = interaction between random variables (broader term) e.g. correlation can be 0 while there are still dependency "],
["motivation-copulas.html", "24.4 Motivations for Using Copulas", " 24.4 Motivations for Using Copulas Limitations of joint distributions There are only a few joint distributions that are tractable to work with (normal, lognormal, exponential, etc) We can’t do Weibull, Pareto or gamma We can’t mix distributions like joining normal with exponential Modeling all business together is not feasible due to inconsistent of mix of business over time 24.4.1 Joint Distribution Plots 3 ways to look at them to try and see if there are dependencies; Each box should have the same number of points if independent Straight plotting on \\((x,y)\\) Draw lines at 25%, 50% and 75% and segment the plot into 16 If the 2 marginal distributions are independent \\(\\Rightarrow\\) there should be about \\(\\frac{1}{16}\\) of the points in each rectangles This is useful to show us actual values, but Might be difficult to see points in some of the rectangles Plot on log log scale \\((\\ln(x), \\ln(y))\\) This alleviate the issue above and shows a clearer picture of the dependencies Plot the percentile from each marginal distn This gives us a clean picture as all the rectangles are the same size since the axis are now the percentile Only need the marginal distributions \\(F(x)\\) and \\(G(y)\\) to plot 24.4.2 Advantages of using copula to describe dependency with percentiles It is independent of the underlying distributions Describe the relationship between the percentiles of 2 different distributions Can update the marginal distn without changing the dependency structure Can joint distn that is not the same "],
["how-to-copula.html", "24.5 How to Use a Copula", " 24.5 How to Use a Copula We can fully describe the joint distribution with: \\[H(x,y) = P(X \\leq x \\: \\&amp; \\: Y \\leq y)\\] If we know this then we know the entire distn for any pair \\((x,y)\\) We can also get the marginal distn: \\[F(x) = \\lim \\limits_{y \\rightarrow \\infty} H(x,y)\\] \\[G(y) = \\lim \\limits_{x \\rightarrow \\infty} H(x,y)\\] and the density: \\[h(x,y) = \\dfrac{\\partial^2 H(x,y)}{\\partial x \\partial y}\\] Describes where the probability lies A bit more intuitive to looks at rather than the CDF It is easier to work with percentiles, so instead of using \\(x\\) &amp; \\(y\\), we’ll use the percentiles \\(u\\) &amp; \\(v\\) and we’ll compare them to the percentile of the r.v. Theorem 24.1 (Sklar’s Theorem) For any joint distn \\(H(x,y)\\) \\(\\exists\\) a function \\(C(u,v)\\) such that \\[H(x,y) = C\\left(F(x), G(y)\\right)\\] \\(C(u,v)\\) is a copula Input is 2 percentiles and output is the joint percentile Density of a Copula Graphs of actual copula is difficult to interpret, so focus on the density \\[c(u,v) = \\dfrac{\\partial^2 C(u,v)}{\\partial u \\partial v}\\] Relate the density of the copula to the density of the joint distn \\[\\begin{equation} h(x,y) = \\underbrace{c \\left( F(x), G(y) \\right)}_{\\text{Density of copula}} \\cdot \\underbrace{\\left[f(x) \\cdot g(y) \\right]}_{\\text{Joint dist if }\\perp\\!\\!\\!\\perp} \\tag{24.5} \\end{equation}\\] Think of the \\(c(u,v)\\) as a multiplier Scales up and down the independent distribution \\(c(u,v) &gt; 1\\) \\(\\Rightarrow\\) Higher density \\(c(u,v) = 1\\) \\(\\Rightarrow\\) Independence \\(c(u,v) &lt; 1\\) \\(\\Rightarrow\\) Lower density Different copulas have different behavior w.r.t. where the dependencies are located We’ll use Kendall’s \\(\\tau\\) to measure correlation as it won’t be affected by the underlying distribution "],
["summary-of-copulas.html", "24.6 Summary of Copulas", " 24.6 Summary of Copulas Table 24.1: Summary of Copulas Copula Shape Dependency \\(\\tau\\) Frank’s Symmetric Light tails Complicated has an integral Gumbel Asymmetric More weight in the right. Higher tail than Frank \\(1 - \\dfrac{1}{a}\\) HRT Asymmetric Less tail on the left but high on the right \\(\\dfrac{1}{2a + 1}\\) Normal Symmetric Higher tail than Frank \\(\\dfrac{2\\mathrm{arcsin}(a)}{\\pi}\\) Key things to know: Density of the copula is the 2nd derivative w.r.t. both \\(u\\) and \\(v\\) Conditional probability of \\(y\\) is the derivative of the copula w.r.t. \\(u\\) (Or \\(x\\) and \\(v\\)) How to simulate a joint distribution Not important to memorize formulas 24.6.1 Frank’s Copula Properties: Small tail dependencies compared to Gumbel and HRT \\(\\tau\\) for Frank is not closed form Frank Copula \\[\\begin{equation} C(u,v) = - \\dfrac{1}{a} \\ln \\left(\\ 1 + \\dfrac{g_u g_v}{g_1} \\right) \\tag{24.6} \\end{equation}\\] \\[\\begin{equation} g_z = e^{-az} -1 \\tag{24.7} \\end{equation}\\] Conditional distribution \\[\\begin{equation} \\Pr(Y \\leq y \\mid X = x) = C_1(u,v) = \\dfrac{g_u g_v + g_v}{g_u g_v + g_1} \\tag{24.8} \\end{equation}\\] Density \\[\\begin{equation} c(u,v) = - \\dfrac{a g_1(1 + g_{u+v})}{(g_u g_v + g_1)^2} \\tag{24.9} \\end{equation}\\] Kendall’s \\(\\tau\\) \\[\\begin{equation} \\tau(a) = 1 - \\dfrac{4}{a} + \\dfrac{4}{a^2} \\int \\limits_0^a \\dfrac{t}{e^t - 1} dt \\tag{24.10} \\end{equation}\\] \\(\\tau\\) is negative when \\(a\\) is negative 24.6.1.1 Simulation Need the conditional distribution \\(P(Y \\leq y \\mid X = x)\\) Given \\(\\tau\\) we can solve for \\(a\\) which is used in the conditional formula For a given \\(\\tau\\), we solve for \\(a\\) using (24.10) Simulate 2 r.v. \\[u,p \\sim U[0,1]\\] Use \\(u\\) to determine \\(x\\) and use \\(p\\) to find \\(y \\mid x\\) \\[x = F^{-1}(u)\\] Use the conditional distribution to find \\(y\\) by setting \\(p = P(Y \\leq y \\mid X = x) = C_1(u,v)\\) and solve for \\(v\\) We know the formula for \\(p\\) and can do some algebra to get \\(v\\) after some algebra \\[\\begin{equation} v = - \\dfrac{1}{a} \\ln \\left( 1 + \\dfrac{p g_1}{1 + g_u(1-p)} \\right) \\tag{24.11} \\end{equation}\\] Plug in \\(g_1\\) and \\(g_u\\) (based of formula (24.7)) in to equation (24.11) to get \\(v\\) Use the \\((u,v)\\) we get from above and then find the values in the 2 distn associated with the percentiles 24.6.2 Gumbel Copula Properties: Asymmetric more probability in the tails than Frank’s Gumbel Copula \\[\\begin{equation} C(u,v) = \\exp\\left\\{ -\\left[(-\\ln(u))^a + (- \\ln (v))^a \\right]^{1/a} \\right\\} \\tag{24.12} \\end{equation}\\] Conditional distribution \\[\\begin{equation} C_1(u,v) = C(u,v) \\left[(-\\ln(u))^a + (- \\ln (v))^a \\right]^{(-1 + 1/a)} \\cdot (-\\ln(u))^{a-1} \\cdot \\dfrac{1}{u} \\tag{24.13} \\end{equation}\\] Density Super complicated… Kendall’s \\(\\tau\\) \\[\\begin{equation} \\tau = 1 - \\frac{1}{a} \\tag{24.14} \\end{equation}\\] \\(v\\) can’t be solve from \\(p=C_1(u,v)\\) \\(\\Rightarrow\\) Need to simulate to some other ways 24.6.2.1 Simulation We can’t solve \\(v\\) like how we did with Frank For a given \\(\\tau\\), we solve for \\(a\\) using (24.14) Simulate 2 r.v.: \\[p,r \\sim U[0,1]\\] Numerically solve for \\(s\\) that satisfies \\[\\begin{equation} s \\ln(s) = a(s-p) \\:\\: : \\:\\: 0&lt;s&lt;1 \\tag{24.15} \\end{equation}\\] Use \\(s\\) we can get: \\[\\begin{equation} (u,v) = \\left( \\exp\\left[ \\ln(s)r^{\\frac{1}{a}} \\right], \\exp\\left[ \\ln(s)(1-r)^{\\frac{1}{a}} \\right] \\right) \\tag{24.16} \\end{equation}\\] 24.6.3 Heavy Right Tail Copula Properties Less correlation in the left tail but high in the right tail HRT Copula \\[\\begin{equation} C(u,v) = [u + v - 1] + \\left[ (1-u)^{-1/a} + (1 - v)^{-1/a} -1 \\right]^{-a} \\:\\: : \\:\\: a &gt; 0 \\tag{24.17} \\end{equation}\\] Conditional distribution \\[\\begin{equation} C_1(u,v) = 1 - \\left[ (1-u)^{-1/a} + (1 - v)^{-1/a} -1 \\right]^{-a-1} \\times (1-u)^{-1-1/a} \\tag{24.18} \\end{equation}\\] Density \\[\\begin{equation} c(u,v) = (1 + \\dfrac{1}{a}) - \\left[ (1-u)^{-1/a} + (1 - v)^{-1/a} -1 \\right]^{-a-2} \\times [(1-u)(1-v)]^{-1-1/a} \\tag{24.19} \\end{equation}\\] Kendall’s \\(\\tau\\) \\[\\begin{equation} \\tau(a) = \\dfrac{1}{2a + 1} \\tag{24.20} \\end{equation}\\] 24.6.3.1 Simulation We can solve \\(v\\) from \\(p=C_1(u,v)\\) so we can simulate it same way as Frank’s 24.6.3.2 Joint Burr Distribution Joint distn where marginal distn are Burr and the conditionals are also Burr Join the 2 marginal Burr using the HRT copula The 2 marginal distn needs to have the same \\(a\\) parameter as the HRT copula Start with 2 Burr distribution: \\[F(x) = 1 - \\left[ 1 + \\left( \\dfrac{x}{b} \\right)^p \\right]^{-a}\\] \\[G(y) = 1 - \\left[ 1 + \\left( \\dfrac{y}{d} \\right)^q \\right]^{-a}\\] Join with HRT with the same parameters \\(a\\) by plugging \\(F(x)\\) and \\(G(y)\\) into \\(C(F(x), G(y))\\) \\[H(x,y) = 1 - \\left[ 1 + \\left( \\dfrac{x}{b} \\right)^p \\right]^{-a} - \\left[ 1 + \\left( \\dfrac{y}{d} \\right)^q \\right]^{-a} + \\left[ 1 + \\left( \\dfrac{x}{b} \\right)^p + \\left( \\dfrac{y}{d} \\right)^q \\right]^{-a}\\] With condition distribution \\[G_{(Y \\mid X)}(y \\mid x) = 1 - \\left[ 1 + \\left( \\dfrac{y}{d_x} \\right)^q \\right]^{-(a+1)}\\] Where \\[d_x = d \\left[ 1 + \\left( \\dfrac{x}{b} \\right)^{\\frac{p}{q}} \\right]\\] Analogous to the joint normal, in that the marginal and conditional distn are the same \\(a\\) is to control correlation \\(p\\) &amp; \\(q\\) is used to fit the tail \\(b\\) &amp; \\(d\\) are used to set the scales of the distn 24.6.4 Normal Copula Joins 2 distn using correlations from the bi-variate normal Properties More dependencies in the tail then Frank Symmetrical The copula take \\(u\\) and \\(v\\) from any distn and converts them to standard normal variables and calculates the probability under the joint normal distn with parameter \\(a\\) Definitions \\(N(x; \\: m,v) =\\) Normal distribution with mean \\(m\\) and variance \\(v\\) Therefore \\[P(X \\leq x) = N(x; \\: m,v) \\:\\:\\:\\:\\: \\mathbb{R}^2 \\rightarrow [0,1]\\] Use the following shorthand for standard normal \\[N(x) = N(x; \\: 0,1)\\] \\(B(x,y; \\: a)\\) Bivariate standard normal distribution with Pearson correlation \\(a\\) \\[P(X \\leq x, Y \\leq y) = B(x, y; \\:a) \\:\\:\\:\\:\\: \\mathbb{R}^2 \\rightarrow [0,1]\\] \\(p(u)\\) is the inverse of the standard normal \\(\\Phi^{-1}(u)\\) \\[p(u) \\:\\:\\:\\:\\: [0,1] \\rightarrow \\mathbb{R}\\] \\[N(p(u)) = u\\] e.g. \\(p(0,95) = 1.645\\) Normal Copula \\[\\begin{equation} C(u,v) = B[p(u), p(v); \\: a] \\tag{24.21} \\end{equation}\\] Conditional distribution \\[\\begin{equation} C_1(u,v) = N(p(v); a p(u), 1-a^2) \\tag{24.22} \\end{equation}\\] Density \\[\\begin{equation} c(u,v) = \\dfrac{\\exp\\left( \\dfrac{[a^2 p(u)^2 - 2a p(u) p(v) + a^2 p(v)^2]}{2(1-a^2)} \\right)}{\\sqrt{(1-a^2)}} \\tag{24.23} \\end{equation}\\] Kendall’s \\(\\tau\\) \\[\\begin{equation} \\tau(a) = \\dfrac{2 \\arcsin(a)}{\\pi} \\tag{24.24} \\end{equation}\\] \\[\\begin{equation} a = \\sin \\left( \\tau \\times \\dfrac{\\pi}{2} \\right) \\tag{24.25} \\end{equation}\\] 24.6.4.1 Simulation Normal copula is simple to simulate from and also easy to generalizes to multiple dimensions Solve for \\(a\\) with the \\(\\tau\\) using (24.25) Simulate 2 r.v. \\[u,r \\sim U[0,1]\\] Get the standard normal value of the 2 percentiles \\[x = p(u)\\] \\[y = p(r)\\] Join the 2 normal value with \\[z = ax + y \\sqrt{1 - a^2}\\] Convert \\(z\\) to a percentile with the normal function to get \\(v\\) \\[ v = N(z)\\] 24.6.5 Partial Perfect Correlation Copula Krep’s Partial Perfect Correlation Copula Sort of artificial, more useful for simulating evens then for description of actual outcomes Generates dependencies by sometimes simulating \\(u\\) &amp; \\(v\\) where they are independent and sometimes 100% dependent by setting \\(v=u\\) Level of dependency is controlled by \\[q(u,p): [0,1]^2 \\rightarrow [0,1]\\] \\(q(u,p)\\) needs to be symmetric Definition Simulate PPC by drawing \\(u\\), \\(p\\) &amp; \\(w\\) from \\(U[0,1]\\) \\(v = \\begin{cases} p &amp;:q(u,p) &lt; w &amp; Independent \\\\ u &amp;:q(u,p) \\geq w &amp; Dependent \\\\ \\end{cases}\\) Partial Perfect Power \\(q(u,p) = (up)^a\\) More concentrate at the top right, since if either \\(u\\) or \\(v\\) is large then the other will likely be at the same percentile \\(\\Rightarrow\\) perfectly correlated "],
["tail-concentration-functions.html", "24.7 Tail Concentration Functions", " 24.7 Tail Concentration Functions Measure how much probability is concentrated in the tails 24.7.1 Left Tail Concentration Function What is the probability that \\(U\\) is small given that \\(V\\) is small \\[L(z) = P(U &lt; z \\mid V &lt; z) = P(V &lt; z \\mid U &lt; z)\\] Works both ways can do \\(L(z) = P(V &lt; z \\mid U &lt; z)\\) as well Left tail concentration function \\[\\begin{equation} L(z) = \\dfrac{C(z,z)}{z} \\tag{24.26} \\end{equation}\\] LTC function for Independent copula: \\[\\begin{equation} L(z) = z \\tag{24.27} \\end{equation}\\] Graph this to 0.5 because all copulas have \\(L(1) = 1\\) See Graph (1) in fig. 24.1 24.7.2 Right Tail Concentration Function What is the probability that one variable is large given that the other variable is large \\[R(z) = P(U &gt; z \\mid V &gt; z) = P(V &gt; z \\mid U &gt; z)\\] Right tail concentration function \\[\\begin{equation} R(z) = \\dfrac{1 - 2z + C(z,z)}{1-z} \\tag{24.28} \\end{equation}\\] RTC function for Independent copula: \\[\\begin{equation} R(z) = 1 - z \\tag{24.29} \\end{equation}\\] Graph this from 0.5 to 1.0 because all copulas has \\(R(0) = 0\\) See Graph (2) in fig. 24.1 24.7.3 LR Graph Combine the 2 above we have the tail concentration graph or the LR graph See Graph (3) in fig. 24.1 The independent copula = product copula Figure 24.1: Tail concentration graph Graph (4) it approaches 0 on the left and right Graph (5) asymmetric, right limit is not 0 (converges about 0.5) Graph (6) asymmetric, right limit is not 0, and higher than Gumbel. Has the thinnest left tail Graph (7) it approaches 0 on the left and right but slower, simply because we couldn’t calculate the tail so deep As the \\(\\tau\\) gets higher, we can start the see the difference between them Table 24.2: Limit of right tail concentration function \\(R = \\lim \\limits_{z \\rightarrow 1} R(z)\\) Copula \\(R\\) \\(\\tau\\) Gumbel \\(2 - 2^{1/a}\\) \\(1 - \\dfrac{1}{a}\\) HRT \\(2^{-a}\\) \\(\\dfrac{1}{2a + 1}\\) Normal 0 \\(\\dfrac{2 \\arcsin(a)}{\\pi}\\) Frank 0 Complicated… PP Power \\(\\dfrac{1}{1 + a}\\) Table is not on syllabus but just for information "],
["how-to-select-copula-given-dataset.html", "24.8 How to Select Copula Given Dataset", " 24.8 How to Select Copula Given Dataset Plot the percentile plot to diagnose Calculate the empirical tail concentration function (LR function) and you can see how each copula fit best Note: Figure 3.3.5 and 3.3.6 is not very good as it mix the underlying distn with the copula 24.8.1 Multivariate Copulas Many options of copulas for 2 variables (pair-wise) but not multivariate Normal and t-copula are the only well known multivariate copulas Parameters: full correlation matrix Normal copula Has no correlation deep in the tail The right tail concentration is very small, and approaches 0 at the limit Just to be clear, the density deep in the tail (z &gt; 0.999) you get very high values still t-copula Can be very strongly correlated in the tails, controlled with \\(n\\) \\(n \\rightarrow \\infty\\), the t-copula approaches the normal copula For small n, it has high density in the tails T-copula also have some density in the other corners Where on variable is high, the other is low "],
["fitting-copulas-to-data.html", "24.9 Fitting Copulas to Data", " 24.9 Fitting Copulas to Data \\(J(z)\\) and \\(\\chi(z)\\) are the empirical statistics we’re interested in that are related to \\(\\tau\\) and \\(R\\) We graph them empirically 24.9.1 \\(J(z)\\) \\[J(z) = - z^2 + \\dfrac{4 \\int_0^z \\int_0^z C(u,v) \\cdot c(u,v)dudv}{C(z,z)}\\] Recall continuous formula for \\(\\tau\\) (24.4) Since \\(C(1,1) =1\\) We have \\[\\lim \\limits_{z \\rightarrow 1} J(z) = \\tau\\] \\(J(z)\\) is the build up of \\(\\tau\\) from 0 (when no data points are considered), up to \\(\\tau\\) when all the data points are considered Maybe?? Graph the empirical \\(J(z)\\) and compare to the theoretical \\(J(z)\\) for each copulas Note: 3.3.14 is incorrect Look at 3.3.16 we can tell that t-copula is the best fit and MM2 is the next closest, MM1 is the worst 24.9.2 \\(\\chi(z)\\) Don’t have much on an intuitive explanation for what the graph represents \\(z \\rightarrow 1\\) it approaches \\(R\\) \\[\\chi(z) = 2 - \\dfrac{\\mathrm \\ln C(z,z)}{\\ln(z)}\\] \\[R = \\lim \\limits_{z \\rightarrow 1} R(z) = \\lim \\limits_{z \\rightarrow 1} \\dfrac{1 -2z + C(z,z)}{1-z}\\] Compare the empirical graph of \\(\\chi(z)\\) with the theoretical copulas to determine which is best fit "],
["past-exam-questions-17.html", "24.10 Past Exam Questions", " 24.10 Past Exam Questions TIA Exercise 3.1 TIA 1: Staffing issues for IRM TIA 2: Getting buy in TIA 3: Expert opinions for parameterization TIA 4: Pilot test benefit \\(\\star\\) TIA 5: Actions vs best practice TIA 6: Tasks enabled by internal model TIA Exercise 3.2 TIA 7: Calculate mean and s.d. using formula (23.1) TIA 8: inflation pick methods TIA 9: inflation model with AR formula and properties TIA 10: How to use a pool of parameter estimates TIA 11: Correlate parameters with joint lognormal and information matrix TIA 12: Mix distributions and parameters in simulation TIA Exercise 3.3 \\(\\star \\star\\) TIA 13: Gumbel copula on 2 uniform distribution \\(\\star \\star\\) TIA 14: HRT copula on 2 Exp and Unif TIA 15: Picking copula based on discription TIA 16: Product copula \\(\\star \\star\\) TIA 17: plot RTC \\(\\star\\) TIA 18: RTC from formula (24.28) TIA 19: Pick copula based on LoB description \\(\\star\\) TIA 20: Reading LR Graph and estimate \\(\\tau\\) for the copula using \\(R\\) TIA 21: Advantages of t-copula \\(\\star\\) TIA 22: Using the PP Copula Past Exams: \\(\\star\\) 2011 #20: Kendall and Pearson calculation and explanation; picking copulas and why use them \\(\\star\\star\\) 2012 #17: Explain, derive and graph RTC 2014 #20: Describe tail dependencies and copula 2015 #21: Benefits of copula and name a better copula than normal \\(\\star\\) 2016 #25: Pick copulas based on RTC, limitation of RTC 24.10.1 Question Highlights n/a -->"],
["era-4-1-4-2-operational-and-strategic-risk-d-mango-g-venter.html", "Chapter 25 ERA 4.1 &amp; 4.2 Operational and Strategic Risk - D. Mango, G. Venter", " Chapter 25 ERA 4.1 &amp; 4.2 Operational and Strategic Risk - D. Mango, G. Venter Op risk \\(\\star\\) Basel definition 25.1 and 7 types of op risk Contrast with strategic risk Underlying cause of insurer failure is actually op-risk Plan loss ratio example on how optimistic loss ratio propagates forward and \\(\\star\\) 3 different reasons for it Cycle management and how to improve performance Requirements from a system performance perspective \\(\\star \\star\\) IP, u/w incentives, market overreaction, owner education 6 Additional persepctives \\(\\star\\) Agency theory, Op-risk for heavey processes, control self-assessment, KRI, 6 \\(\\sigma\\), and \\(\\star\\) op-risk modeling Strategic risk \\(\\star\\) Various definition of strategy 25.5, 25.8, 25.9 and strategic risk 25.6, 25.10, 25.11 Contrast with op-risk Various research to date Miller Baird and Thomas Elements of strategic risk Slywotzky and Drzik \\(\\star\\) Category of strategic risk 25.1 Hertz and Thomas Scenario planning and the 10 Steps Look at the insurance example and the advantages of scenario planning Advance scenario planning Expend to several LoBs with dependencies 4 requirements for stochastic scenario planning \\(\\star\\) Agent based modeling (competitors, customers, regulators) "],
["era-4-1-op-risk-intro.html", "25.1 ERA 4.1 Operational Risk Intro", " 25.1 ERA 4.1 Operational Risk Intro Important cause of insurer insolvency \\(\\Rightarrow\\) Need to be aware and attempt to measure Definition 25.1 (Basel Op-Risk Definition) The risk of loss resulting from inadequate or failed internal process, people, and systems or from external events Include legal risk Exclude strategic and reputation risk 7 Types of Operational Risk Defined by Basel Committee Internal fraud Acts that are intended to defraud, take property, or circumvent: regulations, law, or company policy Includes at least one internal party External fraud Acts by a 3rd party that are intended to defaud, take property, or circumvent the law Employment practices and workplace safety Acts inconsistent with: employment health, safety laws or agreements, which result in payment for: injury, claims for diversity of discrimination issues e.g. repetitive stress, discrimination Clients, products and business practices Unintentional or neligent failure to meet a professional obligation to specific clients Nature of design of a product e.g. client privacy, bad faith Damage to physical assets Loss of damage to physical assets from natural disaster of other events e.g. Physical damage to office, own auto fleets Business disruption and system failures Discruption of business of system failures e.g. processing center downtime, system interruptions, flood Execution, delivery and process management Failed transaction processing or process management Relations with trade counterparties and vendors e.g. policy processing, claims payment errors Remark. Definition of op-risk has gained substantial visibility in ERM For insurance though, the debate is always on going whether it is worth while to hold capital for op-risk Different from banking, insurance op-risk are already baked into the other risk categories we measure and manage But still no consensus on how to define op-risk Could include reputational and strategic risk (just have a consistent risk taxonomy) Understanding of op risk is in its infancy especially with quantitative modeling Do not lose sight of op risk despite it is soft, difficult, poorly understood and lack historical track record "],
["ins-op-risk.html", "25.2 Operational Risk to Insurers", " 25.2 Operational Risk to Insurers Study show that insurer failure are mostly attributed to under reserving and under pricing Underlying cause of insurer failure is actually op-risk e.g. Over accumulation of risk exposure (insufficient initial reserving or premature reserve release) given the asset base e.g. Overly optimistic plan loss ratios Clear that op risk have heavily contributed to insurance impairments and should be explicitly model Currently capital charges for premium, reserve, and growth are partially proxies for operational risk "],
["plan-lr-exp.html", "25.3 Insurer Op Risk: Plan Loss Ratios", " 25.3 Insurer Op Risk: Plan Loss Ratios Mostly impact long tailed LoBs (e.g. 50% reported @ 5yrs, 90% @ 10yrs) Optimistic loss ratio propagates forward Optimistic LR for older years lead to optimistic LR for th recent years Since the ELR is based on older years experience that might be too low, the planned LR is too low as well As the older years losses develop higher than expected \\(\\Rightarrow\\) increase in reserves for all years, and can lead to: Rating downgrade Downgrade of claims paying ability Massive non-renewals No new business Eventual runoff 25.3.1 Three reasons why the planned LR did not work Model unable to forecast accurate LR If the market and other carriers did fine, then the model was effective \\(\\therefore\\) Op-risk: Planning system did not deliver accurate LR, If everyone else failed to price properly, e.g. asbestos \\(\\therefore\\) A true insurance risk (unpredictable nature of long-tailed business) Model was able to forecast accurate LR but was improperly used Op risk: What checks were there to be sure the model was used correctly? Model did accurately forecast the LR but indications were unpopular so ignored Op risk: What governance should be there around management’s decisions "],
["era-cycle-mgmt.html", "25.4 Insurer Op Risk: Cycle Management", " 25.4 Insurer Op Risk: Cycle Management Definition 25.2 (Cycle Management) Prudent management of u/w capacity as market pricing fluctuates in the u/w cycle Simply maintaining market share during soft markets increases the likelihood of impairment 3 questions to ask: Does the company have a proactive cycle management strategy? (if not, need?) e.g. if we reduce rates (or increase exposure for the same premium) during soft market, it’ll lead to increase reserve down the line Does the company know where in the cycle the market stands Are u/w-ers making decisions that are consistent with (1) and (2) 25.4.1 System Performance Perspective Assess an insurance company from a system performance perspective, it needs to be: Stable and available Downgrade impacts this reliable and affordable Insolvent impacts this 25.4.2 Performance Improvement via Cycle Management Areas impacted by implementing cycle management: Planning, u/w, objective setting, incentive bonus Focus on the following to achieve meaningful process improvements: Intellectual property Underwriter incentives Market overreaction Owner education 25.4.2.1 Intellectual Property Majority of an insurer’s franchise value are intangible They time consuming to build but easy to destroy IP examples: Experts in various functions (e.g. u/w, claims, finance, actuarial) Proprietary database Forecasting systems Market relationship Reputation Focus on retaining these core assets through cycle: Retain top talent, grow and develop their skills Maintain presence in core market channels Maintain consistent pattern of investment in systems, models and database 25.4.2.2 Underwriter Incentives Bonus should be based on u/w-er supporting the portfolio goals throughout the year Which may include writing less business U/w-er should know that it won’t affect employment and bonus 25.4.2.3 Market Overreaction Industry tend to overreact (taking prices too low and too high) Companies with most capital during price-improvement phase will reap windfall profits Which will more than offset many years of small u/w losses 25.4.2.4 Owner Education Educate owner/shareholders on the impact of cycle management on financial figures Will be out of step with the market and does not appear healthy during soft markets: Premium volume \\(\\downarrow\\) Need to understand that growing exposure when the price is not adequate is not prudent Overhead expense ratio to premium \\(\\uparrow\\) Typically a metric for efficiency, but should be focus less See this as capital investment in the assets of the firm and they will provide benefits to the firm in the future "],
["add-pers-era-4-1.html", "25.5 Miscellaneous", " 25.5 Miscellaneous Various different miscellaneous topics Agency theory Op-risk management in banking and manufacturing Control Self-Assessment Key Risk Indicators Six Sigma Op-risk modeling 25.5.1 Agency Theory Perespective Incentives of managers and u/w-ers of the firm are not always aligned with shareholders Understand the misalignment rather than try to fund for this op risk: Giving management equity stakes \\(\\xrightarrow{\\text{Caveat}}\\) Manager too aggressive taking on more risk Equity is a large portion of management’s net worth \\(\\xrightarrow{\\text{Caveat}}\\) Too risk adverse Production incentives for u/w-ers \\(\\xrightarrow{\\text{Caveat}}\\) Sloppy u/w-ing or mispricing Key is to be aware of the problems and monitor results Have independent board members in this process 25.5.2 Operational Risk Management in Banking and Manufacturing Op-risks that are common to all business: Pension funding Both financial and HR component Quantifiable: Models that incorporate financial risk and firm demographics IT failure: Hardware or software failure, viruses and internet attacks Need contingency planning Quantifiable Other HR risks (loss of important staff, misdesign of comp program) Loss of key staff (Could be due to misdesign of compensation program) Employee liability, fraud, inadequate training, incompetence Identification and control are more important Reputation risk Damaged from product tampering, bad press, off-hours behavior of key employees Identification and control are more important Lawsuits Business practices can be misinterpreted or reinterpreted Corporate culture makes a different Monitoring is important, funding can be useful too 25.5.3 Control Self-Assessment (CSA) Definition 25.3 (Control Self-Assessment) A process through which internal control effectiveness is examined and assessed Objective: Provide reasonable assurance that all business objectives will be met From Institute of Internal Auditors Objectives of internal control Reliability and integrity of information Compliance with policies, laws and regulations Safeguarding assets Economical and efficient use of resources Accomplishment of objectives and goals for operations or programs 25.5.4 Key Risk Indications (KRIs) Definition 25.4 (KRIs) Broad category of measures that monitor the activities and status of the control environment of an operational risk category Measured frequently (e.g. daily) Have threshold that lead to escalation Purpose: Keep the risk management process dynamic and risk profiles current Forward looking, leading indicators of risk KRIs examples Production: retention ratio, rate/exposure Internal controls: audit results and frequency Staffing: turnover rate, premium/employee, training budget Claims: frequency, severity 25.5.5 Six Sigma Tolerances for output quality of \\(\\pm 3 \\sigma\\) (born out of manufacturing) Provides framework for: Process redesign Project management Customer feedback gathering Internal communication Design trade-offs Documentation Control plans Application: Existing process improvement Predictive design Value proposition: Useful in high volume processing Help identify and eliminate chronic process issues such as: Inefficiencies, errors, overlaps, gaps in communication and coordination Insurance example: U/w-ing: exposure data verification, exposure data capture, classification Claims: coverage verification, ALAE, use of outside counsel, initial case reserve setting Reinsurance: treaty claim reporting, coverage verification, reinsurance recoverable, LoC, collateralization 25.5.6 Operational Risk Modeling Operational risk manager needs to decide if risk should be transferred or retained Based on insurance portfolio risk management, steps for operational risk portfolio management: Identify exposure base for each risk source (premium, headcount, payroll) Measure the exposure level for each BU and each op-risk source Estimate the loss potential per exposure for each op-risk Step 2 + 3 = loss freq and sev distribution for each BU Estimate the impact of mitigation, process improvements, or risk transfer This step requires significant expert opinion as there are no significant amount of loss data both before and after Challenge is the lack of data available or they are recorded in an uncoordinated manner Worthwhile to begin to collect data "],
["era-4-2-intro.html", "25.6 ERA 4.2 Strategic Risk Intro", " 25.6 ERA 4.2 Strategic Risk Intro Definition 25.5 (Strategy) Planning and the use of resources to achieve an organization’s goals Definition 25.6 (Strategic Risk) Intentional risk taking to achieve an organization’s goals Scenario planning is a key part of strategic risk management Think through the possible scenarios the firm could find itself in and to plan responses to those scenarios Advanced scenario planning incorporates several LoBs and includes dependencies among the LoBs Further step is agent based modeling where other parties reactions are also modeled "],
["history-and-definition.html", "25.7 History and Definition", " 25.7 History and Definition Definition 25.7 (Strategy - Merriam-Webster) Science and art of employng the political, economic, pyschological and military forces of nation to afford the maximum support to adopted polices The science and art of military command to meet the enemy under advantageous conditions A careful plan or method An adaptation for achieving evolutionary success, such as foraging strategies of insects Definition 25.8 (Strategy - Mango) Science and art of planning Using political, psychological, and organizational resources To achieve major organizational goals Remark. The above aligns well with the GIRO (General Insurance Research Organization) definition Definition 25.9 (Strategy - GIRO) A long term series of actions designed to take a company from its current state to its desired future state, and aim to provide a sustainable competitive advantage over other companies in the same market Strategy in NOT: Business planning, strategy considers a wider breadth of issues such as the market, and where the company sits Tactics, as these are short term Definition 25.10 (Strategic Risk) Intentional risk taking Unintentional risk as by-products of strategy planning or execution Definition 25.11 (Strategic Risk - OCC) Risk to earnings or capital arising from averse business decisions or improper implementation of those decisions. … Its focus is on how plans, systems and implementation affect the franchise value Remark. OCC’s definition focuses on unintentional risk Which overlaps with op-risk by Basel: The risk of loss resulting from inadequate or failed internal processes, people and systems, or from external events. This definitions include legal risk, but excludes strategic and reputational risk OCC includes system risk in strategic risk Basel includes system risk in op-risk (which excludes strategic risk) "],
["era-4-2-strat-research.html", "25.8 Strategic Risk Management Research to Date", " 25.8 Strategic Risk Management Research to Date Miller Baird and Thomas Slywotzky and Drzik Hertz and Thomas 25.8.1 Miller Definition 25.12 (Risk - Miller) Unpredictability in corporate outcomes (effects) Definition 25.13 (Uncertainty - Miller) Unpredictability of environmental and organizational variables that impact corporate performance (sources) Remark. Our definition of risk is closer to Miller’s definition of uncertainty Our definition of strategic risk (unpredictable impacts of strategies), is what Miller refers to as strategic uncertainty 25.8.2 Baird and Thomas Definition 25.14 (Risk - Old 1921 paper) A condition in which the consequences of a decision and the probabilities associated with the consequences are know entities Remark. Baird and Thomas point out that the above lack clarity When making strategic decisions, the planners rarely know all the possible consequences or their probabilities \\(\\therefore\\) the definition above does not capture strategic risk Lack of knowing consequences or probabilities would be called uncertainty by theorists Definition 25.15 (Strategic Risk - Baird and Thomas) Corporate strategic moves which cause returns to vary Which involve venturing into the unknown Which may result in corporate ruin Where outcomes and probabilities maybe only be partially known, and Where hard to define goals may not be met Elements of strategic risk Voluntariness of exposure (Choose risk because potential benefits are larger than consequences) Controllability of consequences (Outcomes can be contained) Discounting in time (Ability to delay consequences) Discounting in space (Ability to shift risks to competitors) Knowledge of risky situation Magnitude of impact Group/individual factors (Is the leader of the group in favor of risk acceptance) 25.8.3 Slywotzky and Drzik Table 25.1: Categories of Strategic Risk and Magnitude for Insurance Industry Risk Examples Magnitude Comments Industry Capital Intensive; Overcapacity; Accommodation Very High Technology Shift; Patents; Obsolescence Low Possibly innovation in distn Brand Erosion; Collapse Moderate Reputation for fair claims handling Competitor Global Rivals; Unique Competitors Moderate Predatory pricing from competitors Customer Priority Shift; Power; Concentration Moderate Bigger issue for large commercial insurance Project Failure of R&amp;D; IT; Business Development; M&amp;A High Insurers has long history of failed mergers; Low investment in IT Stagnation Flat or declining volume; Price declines High Highly correlated to the cycle; Companies will keep writing business to cover fixed expenses 25.8.4 Hertz and Thomas Definition 25.16 (Risk Analysis - Hertz and Thomas) An input for the strategy development process, aiding strategy formulation, evaluation, choice and implementation Remark. Strategic risk analysis vs strategy formulation No distinction is drawn between strategic risk analysis and strategy formulation Both are viewed as part of a iterative, adaptive and flexible policy dialogue process Definition above focus on how to make better strategy decisions and is close to the strategic error concept "],
["era-4-2-scen-plan.html", "25.9 Scenario Planning", " 25.9 Scenario Planning Effective strategic planning begins with scenario planning Key Characteristics on scenarios: Limited # of scenarios: Each tells a story of how elements may interact with each other under different conditions Test for internal consistency and plausibility Explore joint impact of several variables and change multiple variables at one time to capture new states after major shock Include subjective interpretation of factors that cannot be explicitly modeled Attempts to capture the richness of range of possibilities \\(\\Rightarrow\\) Lead the makers to consider changes they might otherwise ignore 25.9.1 Key Steps in Scenario Planning Process Define scope: (e.g. time frame, geography, market segment) Identify major stakeholders: (e.g. employees, owners, regulators, customers, competitors, suppliers) Identify basic trends (and their influence on organization) Identify key uncertainties: Leverage points of impact Construct initial scenario theme Check for consistency and plausibility Do outcomes fit together? Develop learning scenarios: Identify themes and naming scenarios Identify research needs: Areas that need additional research Develop quantitative models: Should formalize some interactions in a quantitative model? Evolve toward decision scenarios: Iterative process of reviewing scenarios with the goal of converging to test scenarios and generate new ideas 25.9.2 Insurance Example Scenarios will be less detailed than a typical strategy or plan document Goal is to generate a first-order approximation of the possible states Best place to try this is the plan portfolio mix (e.g. coming year WP and ELR by segments) Traditional unilateral planning approach: Set planned premium and ELR based on current year base LR, cost trend, price change, target premium volume, LR Caveat: Not thinking through other possibilities \\(\\Rightarrow\\) Not reacting properly while fixating on “making plan” Commitment to only one plan leads to organizational inertia, inflexibility that is unrealistic and potentially detrimental to the firm e.g. If price decrease, company might want to reduce premium volume instead of the inflexibility of making the planned premium volume 25.9.2.1 Basic Scenario Planning Example Must decide ahead of time: Range of scenarios and relative likelihood Responses to those scenarios Plans must have enough detail to give them operational weight Advantages: Thinks through responses beforehand and can agree on best response ahead of time With the rate change example, it allow u/w-ers to be flexible in their approach in case prices are not as expected Saves time during a crisis Operational inertia is reduced, flexibility is built into the system "],
["adv-plan-era-4-2.html", "25.10 Advanced Scenario Planning &amp; ERM", " 25.10 Advanced Scenario Planning &amp; ERM Model Expansion Expend into several LoBs Internal consistency requires coordination Need to model dependencies Responses will be politically charged, as limited u/w capacity must be allocated across the company Better to do this during planning than in the heat of a market crisis Stochastic Scenario Planning Generate stochastically simulated scenarios that have responses based on a set of rules (This has been done with success in asset management) Need to define goals that we want to achieve (e.g. profit or premium level etc) Need to have downside constraints (e.g. capital loss of x% with y% probability) Need to determine the essential environmental variables that define the state of the world (e.g. size of market, price level, current market share, etc) Define action rules that respond to environmental variables (e.g. hold price, increase u/w capacity according to price adequacy levels) Express as mathematical formulas that can be programmed into the simulation 25.10.1 Agent-Based Modeling Agents can be: competitors, customers, regulators Program each agent to respond to the state of the market e.g. Chase market share, focus on technical price, customer may reduce coverage if prices are high, etc Creates a complex system with emergent properties that describe the behavior of agents There emergent properties are often difficult to ascertain without running the simulations -->"],
["era-5-4-approaches-to-modeling-the-underwriting-cycle-major.html", "Chapter 26 ERA 5.4 Approaches to Modeling the Underwriting Cycle - Major", " Chapter 26 ERA 5.4 Approaches to Modeling the Underwriting Cycle - Major Characteristics of underwriting cycle 4 stage of insurance business and what drives each of them First stage is the classic cycle and the next 3 shows how things stabilize and then eventually breakdown Various theories of underwriting cycle Approaches to modeling the cycle Need: Criterion variable, predictor variable, and competitor intelligence \\(\\star \\star\\) Styles of modeling: Soft has 3 methods (Scenarios, Delphi, Formal Competitor Analysis) Technical use different time series models Behavioral is in between Different impact on supply and demand under different scenarios Gron supply curve Capital flow (fig. 26.2) under different profitability Interaction of the various components 26.3 "],
["era5-uw-cycle-intro.html", "26.1 Introduction", " 26.1 Introduction Important to consider how the firm interacts with a competitive environment in an ERM framework Price competition is inevitable due to: Low barriers of entry Lack of patent or copyright product Price and quantity is difficult to access outside the firm Price depends on premium charged as well as limits, deductible, terms and conditions It’s the ratio of premium to expected losses that define prices 26.1.1 Underwriting cycle Definition 26.1 (Underwiting Cycle) Recurring increase and decrease of prices and profits Remark. Result of a dynamical system with both feedback and external shocks and slow adjustment Each LoB has it’s own cycle Capital plays an important role \\(\\Rightarrow\\) multiline insurers creates dependencies between LoBs’ cycle 26.1.2 Four Stages Insurance Business Steward describes the evolution of insurance business, the stages has sometimes takes decades to play out At each stage, different factor drives the cycle Stage 1: Emergence (Driven by competitive factors) Classic u/w-ing cycle here at this stage New LoB, thin data, inaccurate pricing \\(\\hookrightarrow\\) \\(\\uparrow\\) demand with erratic pricing \\(\\Rightarrow\\) Price wars \\(\\hookrightarrow\\) solvency crisis \\(\\Rightarrow\\) force out weak competitors \\(\\Rightarrow\\) price correction \\(\\hookrightarrow\\) \\(\\uparrow\\) profitability \\(\\Rightarrow\\) new entrants and repeat Stage 2: Control (Driven by statistical lags) Stop the cycle with help from rating bureau or insurance department Stage 3: Breakdown (Driven by mix of the two) Control regime breaks down due to new technology or social changes New type of competitors take business away Stage 4: Reorganization (Driven by competitive factors) Return to stage 1, new configuration of the market phase emerges "],
["era-5-cycle-theory.html", "26.2 Theories of the Cycle", " 26.2 Theories of the Cycle Different researches findings below are often contradictory due to: Research typically focused on different LoBs Often focus on just one stage 26.2.1 Institutional Factors Lags in data can lead to cycles Reporting and regulatory delays leads to 2nd order autoregression 26.2.2 Competition Competition drives prices down: Players might not behave rationally Someone might underestimate the expected loss (winner’s curse) Companies strategy is either aggressive growth (through price reduction) or price maintenance Once a large portion of the market is dropping prices, price maintenance is not sustainable Incumbents drop price so as to convince the aggressive players that they should focus on profits, rather than market share and thus get back to higher prices sooner Dowling’s 4 phases of cycle Cheating (optimistic reserve levels) Pain Fear Restoration 26.2.3 Supply and Demand, Capital Constraints and Shocks Capital determines the available supply of insurance Losses that reduce capital will reduce the supply \\(\\Rightarrow\\) Increase price Capital is not replaced quickly once reduced (debatable now with ILS) \\(\\therefore\\) Market does not rebound quickly Best clients may leave first when capital \\(\\downarrow\\) \\(\\Rightarrow\\) anti-selection \\(\\therefore\\) Declining profits exacerbated by anti-selection 26.2.4 Economic Linkages Profitability of firm links to the economy Investment income, cost of capital Expected losses may impacted by: inflation, GNP growth, unemployment Price of risk (may be set by the market) Is generally ignored by insurers 26.2.5 All of the Above No single theory can explain the u/w cycle completely "],
["era-5-cycle-mod-app.html", "26.3 Approaches to Modeling the Cycle", " 26.3 Approaches to Modeling the Cycle 3 approach: Soft approach Technical approach Behavioral modeling (Econometric) Below show the importance of the dimension to each style: Data quantity, variety and complexity Soft &gt; Behavioral &gt; Technical Recognition of human factors Soft &gt; Behavioral &gt; Technical Mathematical formalism and rigor Technical &gt; Behavioral &gt; Soft Before we model, first we define what it is we want to know and look for leading indicators that foretell the turn in the cycle: Criterion Variable What variable we are interested in? Conceptually we want price e.g. Price to coer a standard risk But impossible to define in insurance Use loss ratio or combined ratio etc as a proxy Maybe with adjustments for TVM (e.g. include investment income) Predictor Variable: Information available to calculate the current period criterion and also forecast the forward period criterion Historical criterion variable and it’s components: (e.g. loss, expense) Internal financial variables: (e.g. reserve, capital, capital flow, reinsurance cessions) Regulatory rating variables: (e.g. downgrades and upgrades) Reinsurance sector financials: (e.g. capital held by reinsurers) Econometric variable: (e.g. inflation, unemployment, GDP) Financial market variables: (e.g. interest rates, stock market returns) Competitor Intelligence: Gather information on customers at renewal time and competitors (e.g. Firm’s own agents, customere surveys at renewal, trade publications/news, rate filing) Provides more detailed information on the state of the u/w cycle Need to beware of antitrust and legal issues (e.g. industrial spying) Goal is to look for leading indicatiors that foretell the turn in the cycle 26.3.1 Soft Approach Starts with intense focus on data gathering and intelligence Goal: Give analysts insight into the complex u/w cycle Collect as much information as we can This is a human approach, focused on three methods: Scenarios Detailed written statement describing a possible future state of the world Help company think about how it might respond to different future states Goal is to have a detailed description of the environment and analyzed by minds Delphi Method Gather expert opinion without biasing the group to the opinion of the most senior persons Process: Give background to participants Gather opinions using questionnaire Results are summarized and distributed Allow participants to reconsider and articulate reason for disagreeing Repeat until consensus is reached Formal Competitor Analysis First determine current state, motives, and likely behavior for the main competitors Need database of competitor information that has key financials, new items, and behavioral metrics Overtime, distinction between normal and abnormal statistics becomes evident Key to predicting turns in the u/w cycle is unusually profitable or distressed financial conditions reproduced over a large number of firms 26.3.2 Technical Approach Focus here is on a small number of industry financial statistics (possibly only 1) We have at best only a rudimentary theory underlying the model Autoregressive Model Research shown cycle can be modeled with \\(AR(2)\\) or \\(AR(3)\\) model For \\(AR(2)\\): \\[X_t = b_o + b_1 X_{t-1} + b_2 X_{t-2} + \\sigma \\epsilon_t\\] Use autoregressive model to model P&amp;C industry combined ratios Results showed weak mean regression with lag 1 but strong mean regression at lag 2 Model can be used to forecast a few periods into the future and estimate the distribution for those forecasts VARMAX Generalized multivariate time series that can handle multiple simultaneous variable and utilize external variables General Factor Model Looks like \\(AR(1)\\) but with non-normal mean and a moving temporary mean (determined by \\(z_{t-1}\\)) \\[X_t = c + d(Z_{t-1} - X_{t-1}) + \\tau \\delta_t\\] \\(Z_t = a + b \\cdot Z_{t-1} + \\sigma \\epsilon_t\\) \\(Z_t\\) maybe an unknown or unobservable variable Neither error term needs to be normal Complicated to fit this model Generalized method of moments or efficient method of moments 26.3.3 Behavioral Modeling Econometric Modeling Sit between soft and technical model’s concern for structural insight (soft) and statistical validity (technical) Same as soft approach: Where we need large quantity, variety and complexity of data Recognize human factors Same as technical approach: Require mathematical formalism Tehcnical \\(\\Rightarrow\\) statistical validity Can be done at an industry level or company level Industry level can be more detailed Company level requires maintaining many individual models and their interactions Can lead to insight from emergent properties (See ERA 4.2) "],
["era-5-supply-demand.html", "26.4 Supply and Demand", " 26.4 Supply and Demand Figure 26.1: Supply and Demand on quantity vs price Supply New competition or technology increases the quantity available at a given price \\(\\hookrightarrow\\) Shifting curve \\(\\searrow\\) More quantity for same price Higher capital requirement \\(\\hookrightarrow\\) Shifting curve \\(\\uparrow\\) Less quantity for same price Demand Excess capital makes insurance more valuable \\(\\hookrightarrow\\) Shifting curve \\(\\nearrow\\) Shock to capital \\(\\hookrightarrow\\) Shifting curve \\(\\swarrow\\) Remark. Curve above is for industry If the industry is well capitalized, its promise to pay claims are worth more and the insurance produce is more valuable and more demand for it If there’s a shock event that reduce the industry’s capital, it will reduce demand for the product Company demand curve will be flatter since customers can go to another company for small changes in price \\(Quantity = f(Price)\\) Alternative way to think about this When companies are capital rich, customers want more product for the same price When new competition comes into the market, the same price will have more produce available Market price = where supply meets demand Difficult to empirically estimate the curves because only the equilibrium is observable 26.4.1 Gron Supply Curve Plot 4 above in fig. 26.1 The first flat section: There is a minimum price for insurance product \\[\\text{Minimum Price} = \\text{Expected Losses} + \\text{Marginal Expenses}\\] Supply is perfectly elastic for a certain quantity, where the supply will increase without an increasing price The curved section: At certain point firm will require additional capital to support the business, and the price must increase Once price hits a a certain level, profits are enough to cover the additional capital and the supply curve approaches a price asymptotically This is high AE from Goldfard’s perspective Plot 5 above in fig. 26.1 Shock scenario: Curve would simply shift \\(\\leftarrow\\) Expected loss + marginal expense is still the same Amount of capital the company have before it needs to seek additional capital has changed "],
["capital-flows.html", "26.5 Capital Flows", " 26.5 Capital Flows Figure 26.2: Capital Flow Capital \\(\\uparrow\\) in a firm from retained earnings and capital infusions Capital \\(\\downarrow\\) from operating losses and capital withdrawal Capital is attracted when profit expectations are high Capital exits either voluntarily or under financial distress "],
["assembling-the-components.html", "26.6 Assembling the Components", " 26.6 Assembling the Components Figure 26.3: Underwriting Cycle Components Economic factors and capital both affect the supply and demand curves Level of price and losses affect the supply curve (but possibly with delay) Capital is impacted directly by premium and losses and specifically profitability Output of this model is an equilibrium price The behavioral model can be used to simulate a distn of this price Should be validated against historical experience "],
["conclusion-4.html", "26.7 Conclusion", " 26.7 Conclusion U/w cycle is the behavior of a complex dynamic system Output of the modeled u/w cycle should feed into the firm’s demand and retention models They help to forecast how clients will react to the difference between the firm’s pricing and that of other prices available in the market "],
["past-exam-questions-18.html", "26.8 Past Exam Questions", " 26.8 Past Exam Questions TIA Exercise 4.1 TIA 1: System performance of insurance company TIA 2: Manage u/w cycle to improve performance TIA 3: Risk of setting apriori LR using historical for long tail LoBs TIA 4: Op risk for comp structure for CEO TIA 5: How to use KRI TIA 6: Op risk to consdier when expanding business TIA Exercise 4.2 TIA 7: How to react to competitors and avoid operational inertia TIA 8: Why scenarios will be less detail than plan TIA 9: Best practice of scenario planning TIA 10: Advance scenario planning TIA 11: Mango’s definition of strategic risk vs the OCC version TIA 12: Categories of strategic risk TIA 13: Essence of Mango’s definition of strategic risk TIA 14: Categorize strategic and op risk Op risk is mostly implementation and execution Strategic risk is more decisions TIA Exercise 5.4 TIA 15: difficulty with modeling price in insurance TIA 16: Stewert’s u/w cycle stages TIA 17: Definition of u/w cycle and how is it driven by competition and capital \\(\\star\\) TIA 18: Model underwriting cycle based on lag of information TIA 19: VARMAX vs AR model TIA 20: supply demand curve for firm vs market TIA 21: Gron supply curve features TIA 22: Industry capital flow graph features TIA 23: Industry capital flow graph features TIA 24: Soft vs technical approach of u/w cycle modeling TIA 25: Drawback of technical approach TIA 26: Delphi method TIA 27: Formal Competitor Analysis for u/w cycle review Past Exam 2011 #20: Identify and describe op risk presented in sample 2012 #22: Describe op-risk and identify op-risk in example 2014 #25: Issue with bridging model and u/w cycle management to improve performance \\(\\star\\) 2015 #28: Op risk vs strategic risk and strategic risk faced by insurance companies 2016 #22: ALM strategy to deal with u/w cycle 2016 #26: Op risk with planned loss ratio and difficulty in seperating op risk from underwriting risk 2016 #27: agency theory and problems when they’re not align, 4 actions to manage properly u/w cycle 26.8.1 Question Highlights n/a "],
["formulas.html", "Chapter 27 Formulas", " Chapter 27 Formulas Links to all key formulas "],
["loss-development-using-credibility-e-brosius-1.html", "27.1 Loss Development Using Credibility - E. Brosius", " 27.1 Loss Development Using Credibility - E. Brosius \\(\\star\\) Least Square (1.1) Theoretical Bayesian: Poisson - Binomial (1.2) Negative Binomial - Binomial (1.3) \\(\\star\\) Best linear estimator formula (1.4) Estimate from data (prop. 1.5: Alt form for \\(Z\\) with equation (1.1) \\(\\star\\star\\) Use \\(VHM\\) and \\(EVPV\\) for \\(Z\\) Case load effect "],
["credible-claims-reserve-the-benktander-method-t-mack-1.html", "27.2 Credible Claims Reserve: The Benktander Method - T. Mack", " 27.2 Credible Claims Reserve: The Benktander Method - T. Mack GB method formula (2.1) Iterative form of BF and GB and it’s extension: Theorem 2.1 "],
["credible-claims-reserve-benktander-neuhaus-and-mack-w-hurlimann-1.html", "27.3 Credible Claims Reserve: Benktander, Neuhaus and Mack - W. Hurlimann", " 27.3 Credible Claims Reserve: Benktander, Neuhaus and Mack - W. Hurlimann \\(\\star\\) Credibility equation (3.1) and the different \\(Z\\) 3.1 Cape Code Credibility method MSE "],
["measuring-the-variability-of-chain-ladder-reserve-estimate-t-mack-1.html", "27.4 Measuring the Variability of Chain Ladder Reserve Estimate - T. Mack", " 27.4 Measuring the Variability of Chain Ladder Reserve Estimate - T. Mack \\(\\star\\) 3 different weight and variance assumptions from table 4.1 \\(\\star\\) Mean squared error calculation The big MSE formula (4.4) How to get the \\(\\alpha\\)’s we need for the MSE formula (4.5) and (4.6) Confidence Interval Normal: equation (4.7) \\(\\star\\) Log-normal: equation (4.8) Use log-normal when \\(s.e.(\\hat{R}_i) &gt; \\dfrac{R_i}{2}\\) \\(\\star\\) 4 test of assumptions to check Test 2. Residuals Formula for residual (4.10) \\(\\star \\star\\) Test 3. CY Test How to get \\(z\\) (4.11) Expected value (4.12) and variance (4.13) Or just memorize them up to 6 (Table 4.2) \\(\\star \\star\\) Test 4. Adjacent LDF Correlation \\(T\\) for age \\(k\\) (4.15) and \\(S\\) (4.14) \\(T\\) for the whole triangle (4.16) CI to compare with resutls (4.17) This is test at a lower CI "],
["testing-the-assumptions-of-age-to-age-factors-g-venter-1.html", "27.5 Testing the Assumptions of Age-to-Age Factors - G. Venter", " 27.5 Testing the Assumptions of Age-to-Age Factors - G. Venter \\(\\star \\star\\) Goodness of fit measure: Adj SSE (5.1) AIC (5.2) BIC (5.3) 6 testable implications \\(\\star \\star\\) Is there a better estimate for \\(q\\) than \\(f \\times c\\) Number of parameters (Table 5.3) Counting parameters \\(\\star \\star\\) BF parameters (Table 5.4) Iteration process \\(\\star \\star\\) No correlation among columns Remember we are testing \\(f(d)\\) so LDF - 1 Pearson correlation calculation (5.4) and test statistics (5.5) To test for all different pairs (not just adjacent pairs) (5.6) No particularly high or low diagonals Setting up regression with CY dummy variable (5.7) "],
["ldf-curve-fitting-and-stochastic-reserving-a-maximum-likelihood-approach-d-clark.html", "27.6 LDF Curve-Fitting and Stochastic Reserving: A Maximum Likelihood Approach - D. Clark", " 27.6 LDF Curve-Fitting and Stochastic Reserving: A Maximum Likelihood Approach - D. Clark \\(\\star\\) Paper definition/standards: We use the average age of the period here (so minus 6 month) Reserve estimate \\(\\star \\star\\) Given \\(G(x)\\) distribution Loglogistic (6.1), or Weibull (6.2) Might have to estimate parameters 2 Reserving method \\(\\star\\) LDF Method \\(\\star\\) Cape Cod Method Test for truncation by looking at age twice the triangle size Different truncation method Reserve process variance \\(\\star \\star\\) Estimation of \\(\\sigma^2\\) (6.4) Note the \\(n\\) and \\(p\\) for the 2 methods We’re looking at incremental losses \\(\\star \\star\\) Test \\(iid\\) assumpation with residual plots by calculating standardized residual Counting average accident date "],
["a-model-for-reserving-workers-compensation-high-deductibles-j-siewert-1.html", "27.7 A Model for Reserving Workers Compensation High Deductibles - J. Siewert", " 27.7 A Model for Reserving Workers Compensation High Deductibles - J. Siewert 6 methods and their pros and cons: Loss Ratio Method Per occ (7.1) and aggregate (7.2) formula Implied Development Direct Development See the 2 methods, (7.3) and (7.4), for calculating the \\(^{XS}LDF^L_t\\) (7.4) is basically from method 5 proposition 7.3 Credibility Weight Method Formula (7.5) weighting 1 and 3 Development Method \\(\\star \\star\\) Relativities 7.1, severity LDF formulas, know them well to manipulate and know what formula requires what Proposition 7.1 and 7.2 are simlar one for limited one for XS Might need to break out the \\(LDF_t\\) into it’s components Proposition 7.3 combines the above Note the 3 proposition above works with LDF if we sub out the ultimate relativities Proposition 7.4 and 7.5 are for incremental LDFs Distribution Method Use Weibull for \\(R_t^L\\) Finally there’s the method for accounting for aggregate charge with collective risk model or table M "],
["claims-development-by-layer-r-sahasrabuddhe-1.html", "27.8 Claims Development by Layer - R. Sahasrabuddhe", " 27.8 Claims Development by Layer - R. Sahasrabuddhe \\(\\star \\star\\) Setup base triangle Set AY CY trend triangle Unlimited mean loss table with (8.1) detrended LEV triangle @ \\(L\\) and LEV for the last row @ \\(B\\) with (8.2) Use (8.3) to get base layer \\(\\star \\star\\) Convert the base LDFs to any layer Convert base layer LDFs (8.4) Convert to LDFs for XS Layers: (8.5) Practical adjustments Formula when we don’t have severity distribution for every age (8.6) and how to estimate (8.7) "],
["using-the-odp-bootstrap-model-a-practitioners-guide-shapland-1.html", "27.9 Using the ODP Bootstrap Model: A Practitioner’s Guide - Shapland", " 27.9 Using the ODP Bootstrap Model: A Practitioner’s Guide - Shapland Model Parameters: Mean (9.1) Variance (9.3) and dispersion factor Need residual: unscaled (9.8), scaled (9.9), standardized (9.10) \\(\\star\\) Simulation procedure Parameter variance (9.14) Process variance (9.15) \\(\\star\\) Practical issues: \\(\\star\\) Negative Incremental Values During fitting: (9.16) and (9.17) During simulation: (9.21) \\(\\star\\) Heteroscedasticity Stratified sampling Hetero adjustment to residuals Non-constant scale parameter "],
["obtaining-predictive-distributions-for-reserves-which-incorporate-expert-opinions-r-verrall-1.html", "27.10 Obtaining Predictive Distributions for Reserves Which Incorporate Expert Opinions - R. Verrall", " 27.10 Obtaining Predictive Distributions for Reserves Which Incorporate Expert Opinions - R. Verrall Model is defined with (ODP or ODNB) and stochastic row parameters Moments for Mack: (10.1) &amp; (10.2) Moments for ODP: (10.3) Moments fod OD NB: (10.6) Bayesian BF calculation: Gamma moments (10.10) Impact of \\(\\beta_i\\) on the Gamma variance Credibility formula (10.11) and credibility weight (10.12) Use and calculate the stochastic column parameters Use the parameters with (10.13), see fig. 10.1 Calculate the parameters \\(\\gamma_i\\) pictorially, see fig. 10.2 "],
["stochastic-loss-reserving-using-bayesian-mcmc-models-g-meyer-1.html", "27.11 Stochastic Loss Reserving Using Bayesian MCMC Models - G. Meyer", " 27.11 Stochastic Loss Reserving Using Bayesian MCMC Models - G. Meyer Interpretation of all the test: KS-test: (11.1), (11.2), and (11.3) \\(p-p\\) plot (fig. 11.1) Too light tailed: Shallow slope near corner and steep in the middle Too heavy tailed: Steep slope near corner and shallow in the middle Biased upwards: Bow down Freq vs Count plot (fig. 11.2) Bayesian Models (Cumulative): Lognormal (11.4) \\(\\beta = 0\\) when it’s done developing \\(\\sigma\\) constraint (11.6) Variations Leveled Chain-Ladder (LCL): Add variability to the row parameter with \\(\\alpha\\) Mean (11.5) Correlated Chain-Ladder (CCL): Add AY correlation with \\(\\rho\\) Mean (11.7) Changing Settlement Rate (CSR): LCL with speed up claims closure with \\(\\gamma\\) Mean (11.8) $&gt;0 $ for increase payout speed Bayesian Models (Incremental): Distribution (11.11) Based on mixed lognormal distribution (11.10) (skewed log-normal (11.9) was not used) \\(\\sigma\\) constraint (11.13) is different Additional constraint on \\(\\beta\\) so that it is decreasing Constraint on CY trend \\(\\tau\\) Additional constraint on \\(\\sigma\\) so that it cannot increase drastically period to period Variation Correlated Incremental Trend (CIT): LIT with added AY correlation \\(\\rho\\) Mean (11.12) Leveled Incremental Trend (LIT): Use skewed distribution and CY trend \\(\\tau\\) "],
["a-framework-for-assessing-risk-margins-k-marshall-et-al-1.html", "27.12 A Framework for Assessing Risk Margins - K. Marshall et al.", " 27.12 A Framework for Assessing Risk Margins - K. Marshall et al. \\(\\star\\) Correlation The 3 main risk sources are independent of each other and therefore can be sum with square root rule (12.2) Independent (12.3): Assume independence across lines, weight by liabilities Internal (12.4): Base on correlation matrix \\(\\Sigma\\), again weighted by liabilities External (12.5) &amp; (12.6): Correlation between each valuation group and risk categories \\(\\Rightarrow\\) then roll up to the risk categories and assume they are independent of each other Risk Margin: (12.7) \\(\\alpha\\)-tile (12.8) "],
["reinsurance-loss-reserving-patrik-1.html", "27.13 Reinsurance Loss Reserving - Patrik", " 27.13 Reinsurance Loss Reserving - Patrik Long tail reserving method: Stanard Buhlmann Credibility IBNR method (\\(Z = p_k \\times CL\\) and weighting reserve on \\(CL\\) and \\(SB\\)) "],
["estimating-the-premium-asset-on-retrospectively-rate-policies-m-teng-and-m-perkins-1.html", "27.14 Estimating the Premium Asset on Retrospectively Rate Policies - M. Teng and M. Perkins", " 27.14 Estimating the Premium Asset on Retrospectively Rate Policies - M. Teng and M. Perkins Retro formula: (14.1) Retro rating formula approach \\(\\star\\) PDLD formulas (14.4), (14.5), and (14.6) Basic premium factor vs charge (charge is after tax) Empirical approach Assume premium lags (typically 9 month) \\(\\star \\star\\) Cumulative PDLD (14.7) and calculating it recursively (14.8) Practical application First adjustment period might cover more than one policy period Feldblum’s adjustment to the empirical \\(CPDLD_1\\) (14.9) "],
["pc-insurance-company-valuation-r-goldfarb-1.html", "27.15 P&amp;C Insurance Company Valuation - R. Goldfarb", " 27.15 P&amp;C Insurance Company Valuation - R. Goldfarb Discount rate CAPM (15.1) and it’s components \\(\\star\\) Growth rate Table 15.1 for each of the 3 method and it’s components DDM: DDM formula (15.2) Select \\(g\\) by average of \\(ROE\\) (usually \\(\\rho\\) is given) If not then select \\(\\rho\\) too by average of \\(\\rho = 1 - \\dfrac{\\mathrm{E}[Div_1]}{NI}\\) FCFE: FCFE formula (15.3) Select \\(g\\) by average of \\(ROE\\) and reinvestment rate AE: AE formula (15.4) and (15.5) Relative multiples: Price:Earning (15.6) Price:Book (15.7) "],
["era-2-2-risk-measures-and-capital-allocation-g-venter-1.html", "27.16 ERA 2.2 Risk Measures and Capital Allocation - G. Venter", " 27.16 ERA 2.2 Risk Measures and Capital Allocation - G. Venter Risk measures Moment based Tail based (\\(VaR\\), \\(TVaR\\), \\(XTVaR\\), \\(EPD\\)), see table 18.1 Probability transform e.g. Esscher (18.1) Generalized moments Allocation of risk measures Definitions 18.2 Proportional Co-measures, table 18.3 and 18.2 Allocation method properties: Marginal allocation 18.3 Scalable risk measures 18.4 Suitable allocation 18.3 Marginal impact (18.4) "],
["era-2-5-measuring-value-in-reinsurance-venter-gluck-brehm-1.html", "27.17 ERA 2.5 Measuring Value in Reinsurance - Venter, Gluck, Brehm", " 27.17 ERA 2.5 Measuring Value in Reinsurance - Venter, Gluck, Brehm Value of reinsurance Provides stability Based on reinsurance premium and expected recoveries Metrics 21.1 and 21.3 Amount of protection Metrics 21.3 Premium - loss Metrics 21.4, 21.5, 21.6 and 21.7 Efficient frontier Frees up capital Cost of capital reduction metrics 21.8 and 21.9 Capital can be based on theoretical or practical model Accumulation risk: as-if reserve 21.10 Capital consumed and the equation (21.1) "],
["era-3-3-modeling-and-dependency-correlations-and-copulas-g-venter-1.html", "27.18 ERA 3.3 Modeling and Dependency: Correlations and Copulas - G. Venter", " 27.18 ERA 3.3 Modeling and Dependency: Correlations and Copulas - G. Venter \\(\\star\\) Correlation Pearson’s correlation: Formula (24.1) Kendall’s \\(\\tau\\): Formula (24.2) for discrete and continuous (24.3) &amp; (24.4) Copulas Using copula and Sklar’s Theorem 24.1 Joint density function express with copula (24.5) Properties of the main copulas in table 24.1 and the partial perfect correlation copula Simulation for each of the copula Frank, Gumble, HRT is same as Frank, and Normal Gumbel’s formula (24.15) &amp; (24.16) Tail Concentration Functions Left tail and right tail concentration function for each copulas 24.1 "]
]
