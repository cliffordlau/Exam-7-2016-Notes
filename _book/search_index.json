[
["using-the-odp-bootstrap-model-a-practitioners-guide.html", "Chapter 9 Using the ODP Bootstrap Model: A Practitioner’s Guide", " Chapter 9 Using the ODP Bootstrap Model: A Practitioner’s Guide Flow of the paper: Parametize \\(\\rightarrow\\) Bootstrap \\(\\rightarrow\\) Practical Issues \\(\\rightarrow\\) Correlations Remember this uses incremental triangles Parametize \\(m_{wd} = \\operatorname{exp} \\left [\\alpha_w + \\sum_j \\beta_j \\right]\\) \\(\\operatorname{Var}[q(w,d)] = \\phi m_{wd}^z\\) Parametize with GLM or Wtd average Bootstrap Create sample trianlge from mean and randomly sampled residuals \\(\\Rightarrow\\) Estimate parameters from sampled triangle \\(\\Rightarrow\\) Calculate mean and variance of the sampled triangle \\(\\Rightarrow\\) Draw losses from gamma Dispersion factor with standard (9.13), England &amp; Verrall (??), and standardized Practical Issues Negative incremental values (fit and simulate) Non-zero sum of residuals N-year wtd average Missing values Outliers Heteroskedasticity Partial latest year exposure or partial diagonal Expsoure adjustment Parametric bootstrap Diagnostics Other Multiple models Model outputs Correlations "],
["introduction.html", "9.1 Introduction", " 9.1 Introduction Paper focus on over-dispersed Poisson (ODP) bootstrap Incremental losses are modeled as ODP random variable Goal is to generate a distribution of possible outcomes Just FYI, not important for exam Other papers on bootstrap Statistics: Bradley Efron (1979) Actuarial: England &amp; Verrall (1999; 2002), Pinheiro, et al. (2003), Kirschner, et al. (2008) Practical motivation for modeling loss distribution Definition of actuarial estimate in ASOP 43 can be based on a first moment from a distribution While ASOP 36 (SAO) focus on deterministic point estimates SEC is looking for more information on reserving risk in the 10-K Rating agencies are building dynamic risk models and welcome actuary input Companies that use dynamic risk models for internal risk management need unpaid claim distributions SII and IFAS are moving towards unpaid claim distribution Advantages of bootstrap Generates a distribution of the estimate of unpaid claims Can be tailored to statistical features of our data Reflects that loss distn are usually skewed to the right Disadvantages of bootstrap Takes more time to create, but okay once set up 9.1.1 Stochastic vs Static Model ODP bootstrap is a specific form of GLM Benefit of GLM: It can be specifically tailored to the statistical features found in the data Contrast with algorithms that force the data to be fit to a static model (fig. 9.1) Figure 9.1: Stochastic vs Static Model Diagram Just FYI, not important for exam Some authors define a model as having a defined structure and error distribution, so under this more restrictive definition bootstrapping would be considered to be a method or algorithm However, using a less restrictive definition of a model as an algorithm that produces a distribution, bootstrapping would be defined as a model "],
["notations.html", "9.2 Notations", " 9.2 Notations Definition 9.1 Same notations as Venter for \\(n \\times n\\) triangle \\(w\\): Accident (exposure) year \\(d\\): Development age \\(q(w,d)\\): incremental loss for AY \\(w\\) from age \\(d-1\\) to \\(d\\) \\(c(w,d)\\): cumulative loss \\(F(d)\\): Incremental LDF from age \\(d\\) to \\(d+1\\) \\(f(d)\\): \\(F(d) - 1\\), for forcasting incremental losses \\(G(w)\\): Factor relating to accdient (exposure) year \\(w\\); ultimate gross level \\(h(k)\\): Factor relating to the diagonal \\(k\\) along which \\(w+d\\) is constant Figure 9.2: Incremental Triangle Chainladder assumptions: LDFs are the same for each row \\(F(w,d) = F(d)\\) Each AY has a parameter representing it’s level e.g. CL project based on level of losses to date "],
["bootstrap-model.html", "9.3 Bootstrap Model", " 9.3 Bootstrap Model Benefits of the bootstrap model: Allows us to estimate the distribution with very little data We don’t have to make any assumptions about the underlying distribution (non-parametric) The ODP part is the error distribution ODP bootstrap models: Incremental claims diretly as the response With the same linear predictor as Kremer (1982) Using a GLM with log-link function and an ODP poisson error Where a specific form of this model is identical to the volume weighted chain ladder Using bootstrap (sampling residuals with replacement) to estimate the distribution of point estimates (Instead of simulating from a multivariate normal for a GLM) 9.3.1 GLM Parameters Mean and variance for each \\(q(w,d)\\) in the triangle (per fig. 9.2) 9.3.1.1 Mean and log-mean for \\(q(w,d)\\) \\[\\begin{equation} \\mathrm{E}[q(w,d)] = m_{wd} = \\exp \\left [\\alpha_w + \\sum_{i=2}^d \\beta_i \\right] \\:\\: : \\: \\: w \\in [2, n] \\tag{9.1} \\end{equation}\\] \\[\\begin{equation} \\ln \\left( \\mathrm{E}[q(w,d)] \\right) = \\ln(m_{w,d}) = \\eta_{w,d} = \\alpha_w + \\sum_{i=2}^d \\beta_i \\:\\: : \\: \\: w \\in [2, n] \\tag{9.2} \\end{equation}\\] Remark. \\(\\alpha\\)’s are the individual level parameters \\(\\beta\\)’s adjust for the development trends after the first development period We don’t use \\(\\beta_1\\) which effectively means \\(\\beta_1 = 0\\) \\(\\alpha_i\\) and \\(\\beta_j\\) are selected to minimize error between \\(\\operatorname{ln}(actual) - \\operatorname{ln}(forecast)\\) Equivalence for using Venter notation: \\(h(w) = e^{\\alpha}\\) \\(f(d) = e^{\\sum \\beta}\\) 9.3.1.2 Variance for \\(q(w,d)\\) \\[\\begin{equation} \\operatorname{Var}[q(w,d)] = \\phi m_{wd}^z \\tag{9.3} \\end{equation}\\] \\(\\phi\\): Dispersion factor Scale factor estimated as part of the fitting procedure while setting the variance proportional to the mean Estimated from the residuals \\(z\\): Error distribution Paper focus on \\(z = 1\\) for Over Dispersed Poisson (ODP) Specifies the whole mean-variance relationship (not only the first 2 moments) Table 9.1: Distribution with corresponding \\(z\\) \\(z\\) Distribution 0 Normal 1 Poisson 2 Gamma 3 Inverse Gaussian 9.3.2 Fitted Triangle We can fit the \\(\\alpha\\)’s and \\(\\beta\\)’s defined above using the GLM framework, or the simplified GLM method 9.3.2.1 Parameterize with GLM Framework Start with a \\(3 \\times 3\\) incremental triangle Table 9.2: \\(3\\times 3\\) incremental triangle: w/d 1 2 3 1 \\(q(1,1)\\) \\(q(1,2)\\) \\(q(1,3)\\) 2 \\(q(2,1)\\) \\(q(2,2)\\) 3 \\(q(3,1)\\) Log transform of the triangle Table 9.3: \\(3\\times 3\\) log incremental triangle: w/d 1 2 3 1 \\(\\ln[q(1,1)]\\) \\(\\ln[q(1,2)]\\) \\(\\ln[q(1,3)]\\) 2 \\(\\ln[q(2,1)]\\) \\(\\ln[q(2,2)]\\) 3 \\(\\ln[q(3,1)]\\) Create a system of equations based on equation (9.2) \\[\\begin{equation} \\begin{split} \\ln[q(1,1)] &amp;= 1\\alpha_1 + 0\\alpha_2 + 0\\alpha_3 + 0\\beta_2 + 0\\beta_3 \\\\ \\ln[q(2,1)] &amp;= 0\\alpha_1 + 1\\alpha_2 + 0\\alpha_3 + 0\\beta_2 + 0\\beta_3 \\\\ \\ln[q(3,1)] &amp;= 0\\alpha_1 + 0\\alpha_2 + 1\\alpha_3 + 0\\beta_2 + 0\\beta_3 \\\\ \\ln[q(1,2)] &amp;= 1\\alpha_1 + 0\\alpha_2 + 0\\alpha_3 + 1\\beta_2 + 0\\beta_3 \\\\ \\ln[q(2,2)] &amp;= 0\\alpha_1 + 1\\alpha_2 + 0\\alpha_3 + 1\\beta_2 + 0\\beta_3 \\\\ \\ln[q(1,3)] &amp;= 0\\alpha_1 + 0\\alpha_2 + 1\\alpha_3 + 1\\beta_2 + 1\\beta_3 \\\\ \\end{split} \\tag{9.4} \\end{equation}\\] Express the above in matrix form \\[\\begin{equation} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\alpha_2 \\:\\:\\: \\alpha_3 \\:\\:\\: \\beta_2 \\:\\:\\: \\beta_3 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - &amp; - &amp; - \\\\ - &amp; 1 &amp; - &amp; - &amp; - \\\\ - &amp; - &amp; 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 &amp; - \\\\ - &amp; 1 &amp; - &amp; 1 &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 &amp; 1 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\end{bmatrix} \\end{array} \\tag{9.5} \\end{equation}\\] Remark. \\(\\mathbf{X}\\) is the design matrix that defines the parameters used to estimate the losses in each cell Use iteratively weighted least squares or MLE1 to solve for the parameters in the in \\(\\mathbf{A}\\) that minimize the squared difference between \\(\\mathbf{Y}\\) and \\(\\mathbf{S}\\), the solution matrix \\[\\begin{equation} \\mathbf{S} = \\begin{bmatrix} ln[m_{1,1}] \\\\ ln[m_{2,1}] \\\\ ln[m_{3,1}] \\\\ ln[m_{2,1}] \\\\ ln[m_{2,2}] \\\\ ln[m_{1,3}] \\\\ \\end{bmatrix} \\tag{9.6} \\end{equation}\\] After solving the sysmte of equations we will have: \\[\\begin{equation} \\begin{split} \\ln[m_{1,1}] &amp;= \\eta_{1,1} &amp;= \\alpha_1 \\\\ \\ln[m_{2,1}] &amp;= \\eta_{2,1} &amp;= \\alpha_2 \\\\ \\ln[m_{3,1}] &amp;= \\eta_{3,1} &amp;= \\alpha_3 \\\\ \\ln[m_{1,2}] &amp;= \\eta_{1,2} &amp;= \\alpha_1 + \\beta_2\\\\ \\ln[m_{2,2}] &amp;= \\eta_{2,2} &amp;= \\alpha_2 + \\beta_2\\\\ \\ln[m_{1,3}] &amp;= \\eta_{1,3} &amp;= \\alpha_1 + \\beta_2 + \\beta_3\\\\ \\end{split} \\tag{9.7} \\end{equation}\\] The above solution shown as a triangle below Table 9.4: \\(3\\times 3\\) GLM fitted log incremental triangle: w/d 1 2 3 1 \\(\\ln[m_{1,1}]\\) \\(\\ln[m_{1,2}]\\) \\(\\ln[m_{1,3}]\\) 2 \\(\\ln[m_{2,1}]\\) \\(\\ln[m_{2,2}]\\) 3 \\(\\ln[m_{3,1}]\\) Exponentiate the triangle above to get our fitted (or expected) incremental results of the GLM model Table 9.5: \\(3\\times 3\\) GLM fitted incremental triangle: w/d 1 2 3 1 \\(m_{1,1}\\) \\(m_{1,2}\\) \\(m_{1,3}\\) 2 \\(m_{2,1}\\) \\(m_{2,2}\\) 3 \\(m_{3,1}\\) 9.3.2.2 Simplified GLM GLM model = Chainladder w/ volume-weighted averages when: Variance \\(\\propto\\) Mean \\(\\varepsilon(w,d) \\sim\\) Poisson A parameter for each row and column (except 1st column) Benefits: Replace GLM fitting with much simpler calculation LDFs are easier to explain Still works even when there are negative incremental values Procedure for fitting incremental triangle: Select LDFs baed on vol-wtd Start from the last cumulative diagonal and divide backwards by each incremental LDFs to get the cumulative fitted triangle Subtracting out the cumulative diagonals to get your incremental fitted triangle 9.3.3 Residuals Important formulas below Unscaled Pearson residuals \\[\\begin{equation} \\begin{split} r_{w,d} &amp; = &amp; \\dfrac{A - E}{\\sqrt{\\operatorname{Var}(E)}} &amp;\\\\ &amp; = &amp; \\dfrac{q(w,d) - m_{wd}}{\\sqrt{m^z_{wd}}} &amp;\\\\ &amp; = &amp; \\dfrac{q(w,d) - m_{wd}}{\\sqrt{m_{wd}}} &amp; \\:\\:\\:\\: \\text{Recall }z = 1\\text{ for ODP Poisson}\\\\ \\end{split} \\tag{9.8} \\end{equation}\\] Mean and variance as defined above Residual for the right and bottom corners of the triangle are going to be 0 Because a unique parameter is used for those 2 cells Alternatively we can use Anscombe residual We prefer Pearson because its calculation is consistent with the scale parameter \\(\\phi\\) Scaled Pearson residuals (England &amp; Verrall) \\[\\begin{equation} r^S_{w,d} = r_{w,d} \\times \\underbrace{\\sqrt{\\dfrac{N}{N-p}}}_{f^{DoF}} \\tag{9.9} \\end{equation}\\] Degrees of freedom adjustment, to effectively allow for over dispersion of the residuals in the sampling process and add process variance to approximate a distribution of possible outcomes Increase the variability of the pseudo triangle Standardized residuals (Pinheiro et al.) \\[\\begin{equation} r^H_{w,d} = r_{w,d} \\times \\underbrace{\\sqrt{\\dfrac{1}{1-H_{i,i}}}}_{f^H_{w,d}} \\tag{9.10} \\end{equation}\\] \\[\\begin{equation} \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{WX})^{-1}\\mathbf{X}^T\\mathbf{W} \\tag{9.11} \\end{equation}\\] \\[\\begin{equation} \\mathbf{W} = \\begin{bmatrix} m_{1,1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; m_{2,1} &amp; 0 &amp; 0 \\\\ \\vdots &amp; 0 &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; \\cdots &amp; m_{1,n}\\\\ \\end{bmatrix} \\tag{9.12} \\end{equation}\\] Hat matrix adjustment factor \\(f^H_{w,d}\\) is based on the diagonal on the hat matrix \\(\\mathbf{H}\\) (Going down the column of the triangle from left to right) \\(\\mathbf{W}\\) is a \\(2n \\times 2n\\) matrix \\(\\mathbf{X}\\) is the design matrix from (9.5) Benefits: \\(f^H_{w,d}\\) account forthe exclusion of zero-value residuals Or the zero-value residuals will have some variance but we just don’t know what it is yet so we should sample from the remaining residuals but not the zeros \\(f^H_{w,d}\\) is an improvement on \\(f^{DoF}\\) 9.3.4 Dispersion Factor Dispersion factor \\[\\begin{equation} \\phi = \\dfrac{\\sum r_{wd}^2}{N-p} \\tag{9.13} \\end{equation}\\] \\[N = \\dfrac{n (n+1)}{2}\\] \\[p = 2n-1\\] \\(N =\\) # of data points (including first column unlike Ventor) \\(N\\) can be less than indicated above if the tail incremental developments are all 0’s \\(p =\\) # of parameters One for each row, one for each column minus first column \\(p\\) can be less than \\(2n-1\\) if the later incremental values are all 0’s and therefore not needed for fitting Alternate method for \\(\\phi\\) \\[\\phi \\sim \\phi^H = \\dfrac{\\sum (r^H_{w,d})^2}{N}\\] We can still use the same dispersion factor even with the scaled and standardized residuals, this just give us another method to estimate \\(\\phi\\) You can also use other methods such as orthogonal decomposition or Newton-Raphson to solve for the parameters↩ "],
["odp-boot-sim.html", "9.4 Bootstrap Simulation Procedure", " 9.4 Bootstrap Simulation Procedure Bootstrap simulation procedure, repeat steps 1 - 5 at least 10,000 times Model our losses, determine mean and residual for each cell Create a sampled \\(triangle^*\\) from the residuals and the means Sample with replacement on the Pearson residuals (9.8) from our original triangle from Step 0. (Since data needs to be \\(iid\\) for bootstrap) (Note the distribution of the residual will be purely empirical) Simulated loss: \\[\\begin{equation} q^*(w,d) = m_{wd} + r_p^* \\sqrt{m_{wd}^z} \\tag{9.14} \\end{equation}\\] (\\(r^*_p\\) is the realized sample from i.) Compile the sampled cumulative triangle Estimate dispersion factor \\(\\phi\\) for Step 3 This can vary per Dispersion Factor section Determine parameters from \\(triangle^*\\): For GLM Framework, calculate the \\(\\alpha_w\\)’s, \\(\\beta_d\\)’s For Simplified GLM calculate the weighted average LDFs and Ultimate Loss Calculate mean and variance2 for the future cells (unpaid): \\((m^*_{wd}, \\phi m^*_{wd})\\) For GLM Framework: \\(m^*_{wd} = \\operatorname{exp} \\left [\\alpha_w + \\sum \\limits_{i=2}^d \\beta_i \\right]\\) For Simplified GLM, back out the \\(c^*(w,d)\\) by \\(\\dfrac{Ult_w}{CDF_d}\\) then get the \\(m^*_{wd}\\) Variance: \\(\\phi m_{wd}^{*}\\) (for ODP \\(z=1\\) on the \\(m^{*z}_{wd}\\)) Add process variance: draw losses3 from \\(Gamma(m_{wd}^*,\\phi m_{wd}^*)\\) for the future cells (unpaid) Simulate loss from the gamma distribution for each future cells Use \\(u \\sim U[0,1]\\) and \\(F^{-1}_{gamma}(u)\\) Calculate simulated unpaid: sum the bottom half of triangle Step 3 and 3 were added in England &amp; Verrall (2002), in their 1999 paper it doesn’t have this step (stopped at 2) and instead just suggest you to add process variance by multiply the results by \\(f^{RoF}\\)↩ Poisson distribution can be used to remain more consistent with the underlying theory of GLM framework, but it is considerably slower to simulate, so gamma is a close substitute that performs much faster in simulation although it can be more skewed than the Poisson. Indeed other distributios could be used as well to better approximate the ovserved “skewness” of the residuals from the diagnostics↩ "],
["variations-on-the-odp-bootstrap.html", "9.5 Variations on the ODP Bootstrap", " 9.5 Variations on the ODP Bootstrap Reason to use paid data: For insurance risk it is best to focus on the claim payment stream: It measures the variability of the actual cash flows that directly affect the bottom line Case reserves temper the volatility Changes in case reserves and IBNR reserves will also impact the bottom line, but to a considerable extent the changes in IBNR are intended to counter the impact of the changes in case reserves To some degree, then, the total reserve movements can act to mask the underlying changes due to cash flows Reason to include case reserves: Case reserves contain valuable information about potential future payments 9.5.1 Bootstrapping the Incurred Loss Triangle 2 approaches to model the unpaid loss distribution using incurred loss triangle Method 1: Modeling the incurred data and convert the ultimate values to a payment pattern Run the paid and incurred data model in parallel For each iteration and each AY individually: Use the payment pattern (from paid model) to convert the ultimate values (from incurred model) to a payment stream Method 1 Advantages: We improve the ultimate estimates by incorporating the case reserves while still focusing on the payment stream for measuring risk Which effectively allows a distribution of IBNR to become a distribution of IBNR and case reserves Can make it more sophisticated by correlating some part of the paid and incurred models (e.g. the residual sampling and/or process variance portions) So that if we have large payment @ an older age, the incurred should be large as well Method 2: Applying the ODP bootstrap to the Munich chain ladder model See Liu and Verral (2010) Method 2 Advantages: Don’t have to model the paid loss twice Explicitly measuring and imposing a framework around the correlation of the paid and outstanding losses 9.5.2 Bootstrapping the BF and Cape Cod Method ODP issue: Distribution for the most recent AYs can produce results with more variance than you would expect when compared to earlier AYs in the actual data Due More LDFs are used to extrapolate the sampled values for the most recent accident years and the random samples of incremental values Similar to the leverage effect of the deterministic chainladder Solution: Incorporate BF or Cape Cod Have the a-priori be stochastic e.g. draw the BF a-priori from a distribution or apply Cape Cod to each simulated triangle More complicated approach is to modify the underlying assumptions of the GLM framework which would results in a completely different set of residuals (this is beyond scope) "],
["glm-bootstrap-model.html", "9.6 GLM Bootstrap Model", " 9.6 GLM Bootstrap Model Limitations of ODP Bootstrap carry over from chainladder Does not adjust for CY trend May over fit the data from using too many parameters We can solve the above by going back to the GLM framework instead of using the Simplified GLM when we’re at Step 2 of the simulation GLM benefits Not forced to use a specific number of parameters (e.g. GLM Variation 1 and 2) Allos for CY trend Can work with shapes that are non triangles (e.g. data with only the last \\(x\\) diagonals) We can forecast past the end of the triangle (e.g. have the \\(\\beta\\)’s continue the decay) Also see section on practical issues GLM drawbacks Solving GLM at each iteration can slow down the process Model not explainable using development factors Below subsections are just examples of the variations discussed above 9.6.1 GLM Variation 1: Reduce Row Parameters Use only 1 AY parameter \\(\\alpha_1\\) Similar to Venter Cap Code method ?? with \\(h(w) = h\\) Moves away from the Chainladder assumption that each AY has its own level Also note that the residual in cell (3,1) will no longer be zero since it shares the \\(\\alpha_1\\) with all the other rows Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\beta_2 \\:\\:\\: \\beta_3 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; - \\\\ 1 &amp; 1 &amp; - \\\\ 1 &amp; 1 &amp; - \\\\ 1 &amp; 1 &amp; 1 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] 9.6.2 GLM Variation 2: Reduce Column Parameters Use only 1 development year parameter \\(\\beta_2\\) This just assumes the losses decay by \\(e^{-\\beta_2}\\) for all ages Also note that the residual in cell (1,3) will no longer be zero since it shares the \\(\\beta_2\\) with all the other columns Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\alpha_2 \\:\\:\\: \\alpha_3 \\:\\:\\: \\beta_2 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - &amp; - \\\\ - &amp; 1 &amp; - &amp; - \\\\ - &amp; - &amp; 1 &amp; - \\\\ 1 &amp; - &amp; - &amp; 1 \\\\ - &amp; 1 &amp; - &amp; 1 \\\\ 1 &amp; - &amp; - &amp; 2 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\beta_2 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] 9.6.3 GLM Variation 3: Reduce Row and Column Parameters Further reduce the parameters to just 1 row and 1 column parameter \\(\\alpha_1\\) and \\(\\beta_2\\) Flexibility of the GLM Bootstrap so that we’re not always stuck with \\(p = 2n-1\\) as stated earlier This will gives us 6 residuals to sample from (the corners will not longer be 0’s) Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) See diagnostics section on how to determine which parameters are statistically significant \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\beta_2 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - \\\\ 1 &amp; - \\\\ 1 &amp; - \\\\ 1 &amp; 1 \\\\ 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\beta_2 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] 9.6.4 GLM Variation 4: Calendar Year Parameter We can also add calendar year trend \\(\\gamma_k\\) where \\(k\\) is the CY \\(\\gamma_2\\) is the 2nd diagonal and etc \\(\\gamma_k\\)’s are incremental decay similar to the \\(\\beta_d\\)’s \\(\\therefore\\) The total impact on the 3rd diagonal is \\(e^{\\gamma_2 + \\gamma_3}\\) Note that the model here have 7 parameters and 6 values \\(\\therefore\\) It has no unique solution Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\alpha_2 \\:\\:\\: \\alpha_3 \\:\\:\\: \\beta_2 \\:\\:\\: \\beta_3 \\:\\:\\: \\gamma_2 \\:\\:\\: \\gamma_3 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -\\\\ - &amp; 1 &amp; - &amp; - &amp; - &amp; 1 &amp; -\\\\ - &amp; - &amp; 1 &amp; - &amp; - &amp; 1 &amp; 1\\\\ 1 &amp; - &amp; - &amp; 1 &amp; - &amp; 1 &amp; -\\\\ - &amp; 1 &amp; - &amp; 1 &amp; - &amp; 1 &amp; 1\\\\ 1 &amp; - &amp; - &amp; 1 &amp; 1 &amp; 1 &amp; 1\\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\gamma_2 \\\\ \\gamma_3 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] 9.6.5 GLM Variation 5: One Parameter for Each Dimension Again we can simplify things by having only 1 parameters for each dimension: \\(\\alpha_1\\), \\(\\beta_2\\), and \\(\\gamma_2\\) Use this as a starting point then add or remove parameters as needed Below is our \\(3 \\times 3\\) triangle example analogous to our example in (9.5) \\[\\begin{equation*} \\begin{array}{ccccc} \\mathbf{Y} &amp; = &amp; \\mathbf{X} &amp;\\times &amp; \\mathbf{A} \\\\ &amp; &amp; \\alpha_1 \\:\\:\\: \\beta_2 \\:\\:\\: \\gamma_2 &amp; &amp;\\\\ \\begin{bmatrix} ln[q(1,1)] \\\\ ln[q(2,1)] \\\\ ln[q(3,1)] \\\\ ln[q(1,2)] \\\\ ln[q(2,2)] \\\\ ln[q(1,3)] \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} 1 &amp; - &amp; - \\\\ 1 &amp; - &amp; 1 \\\\ 1 &amp; - &amp; 2 \\\\ 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 2 \\\\ 1 &amp; 2 &amp; 2 \\\\ \\end{bmatrix} &amp; \\times &amp; \\begin{bmatrix} \\alpha_1 \\\\ \\beta_2 \\\\ \\gamma_2 \\\\ \\end{bmatrix} \\end{array} \\end{equation*}\\] "],
["odp-vs-glm-bootstrap-summary.html", "9.7 ODP vs GLM Bootstrap Summary", " 9.7 ODP vs GLM Bootstrap Summary ODP Bootstrap is a specific case of the GLM model with the following parameters: Parameters for every AY Parameters for every development year minus the first Variance of the incremental losses \\(\\propto\\) mean Solution is the same as volume weighted Chainladder GLM Bootstrap is the general model Can have as few as 1 row and/or column parameters Can include CY trend Variance of the incremental losses $% \\(m^z_{wd}\\) (where \\(z=1\\) for this paper) "],
["odp-prac-issues.html", "9.8 Practical Issues", " 9.8 Practical Issues Practical issues we might run into with ODP bootstrap Negative Incremental Values Non‐Zero Sum of Residuals Using an L-year Weighted Average Missing Values Outliers Heteroscedasticity Heteroecthesious Data Exposure Adjustment Tail Factors Fitting a Distribution to ODP Bootstrap Residuals 9.8.1 Negative Incremental Values GLM doesn’t work with negative incremental values because of \\(\\ln[q(w,d)]\\) Need to work around this in: Model fitting (e.g. Step 0 and 1 of the Bootstrap process) Simulating for process variance with negative means (e.g. Step 4 of the Bootstrap process) Also additional work around on extreme outcomes from negative values 9.8.1.1 Model Fitting Method 1: Use \\(-ln(abs\\{q(w,d)\\})\\) \\[\\begin{equation} Cell_{w,d} = \\begin{cases} \\ln[q(w,d)] &amp; \\text{if } q(w,d) &gt; 0 \\\\ 0 &amp; \\text{if } q(w,d) = 0 \\\\ -\\ln[abs \\{ q(w,d) \\}] &amp; \\text{if } q(w,d) &lt; 0 \\\\ \\end{cases} \\tag{9.15} \\end{equation}\\] Remark. Doesn’t work when the column sum to a negative value Method 2: Subtract a negative constant \\(\\Psi\\) \\[\\begin{equation} q^+(w,d) = q(w,d) - \\Psi \\\\ \\ln[q^+(w,d)] \\text{ for all } Cell_{w,d} \\tag{9.16} \\end{equation}\\] Pick \\(\\Psi =\\) largest negative value in the column Apply (9.16) before solving the GLM system of equations (e.g. (9.2) and (9.4)) Then adjust the fitted values by additing back \\(\\Phi\\) to reduce each fitted incremental value \\[\\begin{equation} m_{w,d} = m^+_{w,d} + \\Psi \\tag{9.17} \\end{equation}\\] Can use this method combined with method 1 to take care of the extra large negative ones Need to make use the absolute value for the residual and re-sampling formula, modify (9.8) and (9.14) with below: \\[\\begin{equation} r_{wd} = \\dfrac{q(w,d)-m_{wd}}{\\sqrt{abs\\{m^z_{wd}\\}}} \\tag{9.18} \\end{equation}\\] \\[\\begin{equation} q^*(w,d) = m_{wd} + r^*_p \\sqrt{abs\\{m^z_{wd}\\}} \\tag{9.19} \\end{equation}\\] Method 3: USe simplified GLM Use ODP bootstrap (i.e. Chainladder with volume weighted average LDFs) This will yield different estimate than using the GLM framework with adjustment 1 or 2 9.8.1.2 Simulating Negative Values From above, we might have the fitted \\(m_{wd}\\) that are negative, which will be an issue when used in Step 4 of the bootstrap simulation, when we need to model the process variance with \\(Gamma(m_{wd},\\phi m_{wd})\\) Since \\(Gamma\\) only takes positive parameters Adjustment to the Gamma Distribution with negative \\(m_{wd}\\) \\[\\begin{equation} Gamma(abs\\{m_{wd}\\}, \\phi abs\\{m_{wd}\\}) + 2m_{wd} \\tag{9.20} \\end{equation}\\] This will maintain the right skew of Gamma while having the mean of \\(m_{wd}\\) Alternatively if we use \\(-Gamma(abs\\{m_{wd}\\}, \\phi abs\\{m_{wd}\\})\\) it’ll flip the curve to skew left 9.8.1.3 Extreme Outcomes from Negative Values Column with negative mean in the early ages can results in vary large LDFs (and lead to simulated outcomes that are 1,000 times greater than our mean) Negative mean causes one column of cumulative values to sum close to 0 and the next to sum to a much larger number resulting in extremely large LDF and there for projection that are extremely large Need to address this as it’ll throw off the mean even if you don’t care about the high percentiles 3 options to address this: Remove the extreme iterations Beware of understating the the likelihood of extreme outcomes Recalibrate the Model First need to identify the source of the negative losses Review data used and parameter selection e.g. remove the AYs that might not represent current behavior e.g. if due to S&amp;S then you can just model them separately and then correlate them during simulation Limit Incremental Losses to 0 Either with the simulated mean (Step 2) or the process var step (Step 4) Replace with negatives with 0s Can just do it in certain columns 9.8.2 Non-Zero Sum of Residuals Residuals are supposed to be iid with mean zero and constant variance \\(\\therefore\\) Sum of our residuals from the triangle should be 0 Not necessarily the case since this is just a sample Consequence: Simulated outcomes will be higher than the mean if sum of residuals are positive (and vice versa) 2 options to address this: Keep it if we believe this to be characteristics of the data set Add a constant to each non-zero residual so that it sums to 0 Then sample from the adjusted residuals If residuals are significantly different from zero then the fit of the model should be questioned 9.8.3 Using L-year Weighted Average Select LDFs based on the latest \\(L\\) years GLM Bootstrap Only use \\(L+1\\) diagonals of data to get \\(L\\) diagonals of LDFs Excluded diagonals are given zero weight and we’ll have less CY trend parameter (if we’re using it) In the simulation we’ll only sample residuals for the trapezoid used to parameterize the model (since that’s all we’ll need to estimate parameters) Simplified GLM Get L-year weighted average LDFs Will only have residuals (to sample from) for the most recent L + 1 diagonals In the simulation we’ll create the entire resampled triangle (Since we need the cumulative losses for each row) For projection using the resampled triangle we’ll still only use the L-year average LDFs The 2 methods will results in different results GLM Bootstrap: Models the incremental losses in the trapezoid Simplified GLM: Models the same losses but in relation to the cumulative losses, which include the non-modeled losses in the diagonals excluded 9.8.4 Missing Value ODP Bootstrap: Missing data impact: LDFs Fitted triangle (if missing value lies on the most recent diagonal) Residuals Degree of freedom Solutions: Impute from surrounding values Modify LDFs to exclude missing value Similar to the L-year weighted average: Missing value will be resampled so the cumulative losses can be calculated Projection from the resampled triangle will exclude the missing cell for resampled LDF selection GLM Bootstrap Impact on the is limited, we’ll just have less observations 9.8.5 Outliers Remove outliers if they are not representative of the variability of the losses, below are the options: Remove the entire row (easy if it’s the 1st row of the triangle) Remove the values and treat them as missing values Not use the residual but do create a sampled value in that cell Significant number of outliers might indicate bad model fit GLM Bootstrap Pick new parameters (grouping parameters) Change the error term distribution from \\(z=1\\) ODP Bootstrap Use L-year weighted average Heteroscedasticity may exist See adjustment next sub section and diagnostics Since we dont’ make a distribution assumption, the number of outliers could mean the data is quite skewed and it’s appropriate that is showing up in the simulation 9.8.6 Heteroskedasticity Non constant variance (bootstrap assumes residuals are \\(iid\\)) Stratified Sampling Split the triangle into groups with similar variance and only sample residuals that are in the group Cons: each group may not be that large Hetero-Adjustment Group the residuals then calculate the \\(\\sigma\\) of the residuals in each group and scale up Hetero-adjustment factor: \\(h^i\\) = the largest \\(\\sigma\\) \\(\\div\\) each group’s \\(\\sigma\\) \\(r_{wd}^{i,H} = r_{wd} \\times f_{wd}^H \\times h^i\\) Residual \\(\\times\\) Hat Matrix Factor \\(\\times\\) Hetero Factor Need to divide the sampled residual by \\(h^i\\) to reflect the variability of group \\(i\\) \\(q^{i*}(w,d) = m_{wd} + \\dfrac{r^{i*}}{h^i}\\sqrt{m_{wd}}\\) 9.8.7 Heteroecthesious Data Accident years have different level of exposures Partial First Development Period Only want partial accident year No impact to residuals for bootstrap 2 options: Reduce the mean of the incremental cells by pro ration in the process variance step Prorate after the process variance step Partial Last Calendar Period Latest diagonal is partial Simplified GLM Determine LDF excluding latest diagonal then interpolate LDFs for ultimate GLM Adjust the exposure in the last diagonal to make them consistent with the rest of the triangle (probably means adjusting annualizing the loss) Then prorate the losses similar to the first scenario 9.8.8 Exposure Adjustment Consider dividing the losses by the exposure in each AY if there are significant changes in exposure and model pure premium Multiply the PP results by the exposure after the process variance step 9.8.9 Parametric Bootstrapping Might not have enough data to sufficiently represent the tail Fit a distribution to the residual and sample from the distribution instead 9.8.10 Fitting a Distribution to ODP Bootstrap Residuals new? "],
["odp-diagnostics.html", "9.9 Diagnostics", " 9.9 Diagnostics Judge the quality of the model Test Assumptions in model Gauge quality of model fit Guide the adjustments of model parameters 9.9.1 Residual Graphs Graph residuals vs CY, AY, Age, forecast loss Want to see random variability around zero 9.9.2 Normality Test Normality is not required, only need this if we’re doing parametric bootstrap with normal distribution Plot residuals against the normal best fit based on the percentiles Use p-value &gt; 5% as the test Or use something that penalize number of parameters \\(AIC = 2p + n \\left [ 1 + \\operatorname{ln}(2\\pi\\dfrac{RSS}{n})\\right]\\) \\(BIC = n \\operatorname{ln}\\left( \\dfrac{RSS}{n}\\right) + p \\operatorname{ln}(n)\\) \\(RSS\\) = actual residual - expected residual from normal 9.9.3 Outlier Remove true outliers but do not want to remove points that are realistic extreme scenarios Use box &amp; whisker plot Shows 25%ile to 75%ile Whiskers are 3 times the inter quartile range 9.9.4 Parameter Adjustments Test model with different sets of parameters Don’t need unique parameter for each row and column 9.9.5 Review Model Results Read summarized output by AYs Mean, s.e., CoV, Min, Max, Median The all year s.e. should be greater than any individual year The all year CoV should be less than the CoV for any individual year CoV should be highest for older years due small mean unpaid CoV also high for most recent AY due to higher volatility Larger parameter uncertainty or volatility from CL method Check min max for reasonability For the triangles: Check incremental means as well in triangle form Check s.d. of incremental values "],
["using-multiple-models.html", "9.10 Using Multiple Models", " 9.10 Using Multiple Models Use different methods (Paid/Inc’d Dev, BF, etc) by assigning weights by AYs Method 1: In the process variance step of bootstrap, use the same underlying U(0,1) to draw from each model then weight the models by the % Method 2: Run each model independently for each simulation (i.e. use different U(0,1)) then for each AY use the weights to randomly select one of the modeled results. Results will be a mixture of the various models Important to review the statistics in the above section for each output Fit the unpaid claim distribution to Normal, LogNormal, and Gamma. Then compare with the fit based on the actual residuals on various statistics Not sure what distribution this is talking about 9.10.1 Other Model Outputs Estimated Cash Flow Results Since bootstrap generates simulation for each cell in the bottom half of the triangle we can use this to get cash flow forecasts by CY and the percentiles as well Estimated Ultimate Loss Ratio Results We can estimate the variability of ultimate loss ratio since we vary and simulate the whole “square” Distribution Graphs Draw a distribution of the simulated unpaid in a histogram Can also smooth the histogram with Kernel density function For each point it takes a weighted average of the points around it; giving less weight to points further from it "],
["correlation.html", "9.11 Correlation", " 9.11 Correlation Correlate the loss distribution over several LoB Multivariate distribution requires the same underlying distribution which doesn’t work here for ODP Location Mapping When sampling the residuals, sample from the same place in the triangle for all the lines we want to correlate Disadvantages: Requires all LoB to have the same size triangle with no missing values or outliers Cannot stress the correlations among the LoBs Re-Sorting Use Iman-Conover algorithms or Copulas Advantages: Can accommodate different shapes and sizes Can make different correlation assumptions Can strengthen the correlation for extreme events (e.g. Copulas) Calculate correlation matrix using Spearman’s Rank Order Re-sorting based on the ranks of unpaid claims by AYs Using residuals to correlate LoBs (Both location mapping &amp; re-sorting) are both liable to create correlations close to zero Reserve Risk: Correlate total unpaid by correlating the incremental paid. May or may not be a reasonable approximation Pricing Risk: Correlate loss ratios over time Not as likely to be close to zero Use different correlation assumption than for reserve risk "],
["miscellaneous.html", "9.12 Miscellaneous", " 9.12 Miscellaneous Model Testing Based on testing from General Insurance Reserving Oversight Committee, the ODP Bootstrap with England &amp; Verrall residual out perform the Mack model by forecasting the 99%-ile better ODP losses only exceed 99%-ile ~3% of the time compare to Mack’s 8-13% Future Research Test ODP bootstrap on realistic data from CAS loss simulation model Expand ODP bootstrap with Munich Chainladder, claim counts and severity Research other risk analysis measures and use for ERM Use for SII requirements Research in correlation matrix (difficult to estimate) "],
["past-exam-questions.html", "9.13 Past Exam Questions", " 9.13 Past Exam Questions Exercises \\(\\star\\) Use simplied GLM and then back out the GLM parameters Reduce parameters Minimize square error for GLM Benefit of simplified GLM Residuals Dispersion Dispersion with hat matrix adj Simulate loss Setup GLM Negative values Simulat ngative Partial triangle Stratified Dealing with correlation Outliers Practical Issues 2013 #7: negatvie values 2014 #7: List 4 practical issues and solutions 2015 #10: Heteroscedasticity, why important, adjustments description 2015 #11: Negative values, outliers, exposure level Diagnostics \\(\\star\\) 2014 #9: Evaluate the results given mean unpaid, s.e., CoV by AYs 9.13.1 Question Highlights n/a -->"]
]
