[
["era-3-3-modeling-and-dependency-correlations-and-copulas-g-venter.html", "Chapter 24 ERA 3.3 Modeling and Dependency: Correlations and Copulas - G. Venter", " Chapter 24 ERA 3.3 Modeling and Dependency: Correlations and Copulas - G. Venter \\(\\star\\) Correlation Pearson’s correlation: Formula (24.1) and its properties Outliers will have disproportionate weight Kendall’s \\(\\tau\\): depends on the rank Formula (24.2) for discrete and continuous (24.3) &amp; (24.4) Copulas Limitation of joint distribution and advantages for using copulas Joint distribution plots where the best is to plot the percentile for the marginal distribution Using copula and Sklar’s Theorem 24.1 Joint density function express with copula (24.5) Properties of the main copulas in table 24.1 and the partial perfect correlation copula Simulation for each of the copula Frank, Gumble, HRT is same as Frank, and Normal Gumbel’s formula (24.15) &amp; (24.16) Tail Concentration Functions Left tail and right tail concentration function for each copulas 24.1 Methods for selecting Copulas Plot the percentile plot Empirical tail concentration function Multivariate Copulas: Normal and t-copula and their properties Fitting copulas to data Using the \\(J(z)\\) and \\(\\chi(z)\\) Graph the possible \\(J(z)\\) with empirical \\(J(z)\\) to see which fits best; Similarly for \\(\\chi(z)\\) "],
["era3-3-intro.html", "24.1 Introduction", " 24.1 Introduction We need loss distribution for the whole group Create distribution for each LoB or Company Combine them into one distribution that reflects all the inter-dependencies between BUs Go through 4 copulas that are useful in practice and how to select them for a given dataset "],
["era-3-3-pearson.html", "24.2 Pearson’s Correlation \\(\\rho\\)", " 24.2 Pearson’s Correlation \\(\\rho\\) \\[\\begin{equation} \\rho = \\dfrac{\\sum_i \\tilde{y}_i \\tilde{z}_i}{\\sqrt{\\sum_i \\tilde{y}_i^2 \\sum_i \\tilde{z}_i^2}} = \\dfrac{\\mathrm{E}[YZ] - \\mathrm{E}[Y] \\cdot \\mathrm{E}[Z]}{\\sigma_{y} \\cdot \\sigma_{z}} \\tag{24.1} \\end{equation}\\] \\(\\tilde{y}_i = (y_i - \\bar{y})\\) \\(\\sigma_y^2 = \\mathrm{E}[Y^2] - \\mathrm{E}[Y]^2\\) Only appropriate for distn symmetric and have thin tail Caveat: Value far from the mean will have a disproportionate weight as it focus on the amount of each \\(\\tilde{y}_i\\) and \\(\\tilde{z}_i\\) Properties: Pearson correlation will stays the same under positive linear transformation on \\(Y\\) or \\(Z\\) Monotone function that is not linear might change the Pearson correlation "],
["era-3-3-kendall.html", "24.3 Kendall’s \\(\\tau\\)", " 24.3 Kendall’s \\(\\tau\\) Depends on the order not the value of the data Concordant: When one pair dominates the other given \\((x_1, y_1)\\) and \\((x_2, y_2)\\) \\(x_1 &gt; x_2\\) and \\(y_1 &gt; y_2\\) or, \\(x_2 &gt; x_1\\) and \\(y_2 &gt; y_1\\) Discordant: When the pair is mixed Kendall’s \\(\\tau\\) \\[\\begin{equation} \\tau = \\dfrac{C - D}{\\text{# of pairs}} \\tag{24.2} \\end{equation}\\] \\(\\tau = \\dfrac{C - D}{C + D}\\) if there are no ties \\(\\tau \\in [-1, 1]\\) with same interpretation as Pearson’s Focus on the rank \\(\\Rightarrow\\) changes in one extreme value won’t change the indication Continuous Kendall’s \\(\\tau\\) \\[\\begin{equation} \\tau = 4 \\mathrm{E}[C(u,v)] - 1 \\tag{24.3} \\end{equation}\\] \\[\\begin{equation} \\tau = -1 + 4 \\int_0^1 \\int_0^1 C(u,v)c(u,v)dudv \\tag{24.4} \\end{equation}\\] Definition 24.1 (Correlation vs Dependency) Correlation = mathematical calculation, a statistic, and measure of dependency Dependency = interaction between random variables (broader term) e.g. correlation can be 0 while there are still dependency "],
["motivation-copulas.html", "24.4 Motivations for Using Copulas", " 24.4 Motivations for Using Copulas Limitations of joint distributions There are only a few joint distributions that are tractable to work with (normal, lognormal, exponential, etc) We can’t do Weibull, Pareto or gamma We can’t mix distributions like joining normal with exponential Modeling all business together is not feasible due to inconsistent of mix of business over time 24.4.1 Joint Distribution Plots 3 ways to look at them to try and see if there are dependencies; Each box should have the same number of points if independent Straight plotting on \\((x,y)\\) Draw lines at 25%, 50% and 75% and segment the plot into 16 If the 2 marginal distributions are independent \\(\\Rightarrow\\) there should be about \\(\\frac{1}{16}\\) of the points in each rectangles This is useful to show us actual values, but Might be difficult to see points in some of the rectangles Plot on log log scale \\((\\ln(x), \\ln(y))\\) This alleviate the issue above and shows a clearer picture of the dependencies Plot the percentile from each marginal distn This gives us a clean picture as all the rectangles are the same size since the axis are now the percentile Only need the marginal distributions \\(F(x)\\) and \\(G(y)\\) to plot 24.4.2 Advantages of using copula to describe dependency with percentiles It is independent of the underlying distributions Describe the relationship between the percentiles of 2 different distributions Can update the marginal distn without changing the dependency structure Can joint distn that is not the same "],
["how-to-copula.html", "24.5 How to Use a Copula", " 24.5 How to Use a Copula We can fully describe the joint distribution with: \\[H(x,y) = P(X \\leq x \\: \\&amp; \\: Y \\leq y)\\] If we know this then we know the entire distn for any pair \\((x,y)\\) We can also get the marginal distn: \\[F(x) = \\lim \\limits_{y \\rightarrow \\infty} H(x,y)\\] \\[G(y) = \\lim \\limits_{x \\rightarrow \\infty} H(x,y)\\] and the density: \\[h(x,y) = \\dfrac{\\partial^2 H(x,y)}{\\partial x \\partial y}\\] Describes where the probability lies A bit more intuitive to looks at rather than the CDF It is easier to work with percentiles, so instead of using \\(x\\) &amp; \\(y\\), we’ll use the percentiles \\(u\\) &amp; \\(v\\) and we’ll compare them to the percentile of the r.v. Theorem 24.1 (Sklar’s Theorem) For any joint distn \\(H(x,y)\\) \\(\\exists\\) a function \\(C(u,v)\\) such that \\[H(x,y) = C\\left(F(x), G(y)\\right)\\] \\(C(u,v)\\) is a copula Input is 2 percentiles and output is the joint percentile Density of a Copula Graphs of actual copula is difficult to interpret, so focus on the density \\[c(u,v) = \\dfrac{\\partial^2 C(u,v)}{\\partial u \\partial v}\\] Relate the density of the copula to the density of the joint distn \\[\\begin{equation} h(x,y) = \\underbrace{c \\left( F(x), G(y) \\right)}_{\\text{Density of copula}} \\cdot \\underbrace{\\left[f(x) \\cdot g(y) \\right]}_{\\text{Joint dist if }\\perp\\!\\!\\!\\perp} \\tag{24.5} \\end{equation}\\] Think of the \\(c(u,v)\\) as a multiplier Scales up and down the independent distribution \\(c(u,v) &gt; 1\\) \\(\\Rightarrow\\) Higher density \\(c(u,v) = 1\\) \\(\\Rightarrow\\) Independence \\(c(u,v) &lt; 1\\) \\(\\Rightarrow\\) Lower density Different copulas have different behavior w.r.t. where the dependencies are located We’ll use Kendall’s \\(\\tau\\) to measure correlation as it won’t be affected by the underlying distribution "],
["summary-of-copulas.html", "24.6 Summary of Copulas", " 24.6 Summary of Copulas Table 24.1: Summary of Copulas Copula Shape Dependency \\(\\tau\\) Frank’s Symmetric Light tails Complicated has an integral Gumbel Asymmetric More weight in the right. Higher tail than Frank \\(1 - \\dfrac{1}{a}\\) HRT Asymmetric Less tail on the left but high on the right \\(\\dfrac{1}{2a + 1}\\) Normal Symmetric Higher tail than Frank \\(\\dfrac{2\\mathrm{arcsin}(a)}{\\pi}\\) Key things to know: Density of the copula is the 2nd derivative w.r.t. both \\(u\\) and \\(v\\) Conditional probability of \\(y\\) is the derivative of the copula w.r.t. \\(u\\) (Or \\(x\\) and \\(v\\)) How to simulate a joint distribution Not important to memorize formulas 24.6.1 Frank’s Copula Properties: Small tail dependencies compared to Gumbel and HRT \\(\\tau\\) for Frank is not closed form Frank Copula \\[\\begin{equation} C(u,v) = - \\dfrac{1}{a} \\ln \\left(\\ 1 + \\dfrac{g_u g_v}{g_1} \\right) \\tag{24.6} \\end{equation}\\] \\[\\begin{equation} g_z = e^{-az} -1 \\tag{24.7} \\end{equation}\\] Conditional distribution \\[\\begin{equation} \\Pr(Y \\leq y \\mid X = x) = C_1(u,v) = \\dfrac{g_u g_v + g_v}{g_u g_v + g_1} \\tag{24.8} \\end{equation}\\] Density \\[\\begin{equation} c(u,v) = - \\dfrac{a g_1(1 + g_{u+v})}{(g_u g_v + g_1)^2} \\tag{24.9} \\end{equation}\\] Kendall’s \\(\\tau\\) \\[\\begin{equation} \\tau(a) = 1 - \\dfrac{4}{a} + \\dfrac{4}{a^2} \\int \\limits_0^a \\dfrac{t}{e^t - 1} dt \\tag{24.10} \\end{equation}\\] \\(\\tau\\) is negative when \\(a\\) is negative 24.6.1.1 Simulation Need the conditional distribution \\(P(Y \\leq y \\mid X = x)\\) Given \\(\\tau\\) we can solve for \\(a\\) which is used in the conditional formula For a given \\(\\tau\\), we solve for \\(a\\) using (24.10) Simulate 2 r.v. \\[u,p \\sim U[0,1]\\] Use \\(u\\) to determine \\(x\\) and use \\(p\\) to find \\(y \\mid x\\) \\[x = F^{-1}(u)\\] Use the conditional distribution to find \\(y\\) by setting \\(p = P(Y \\leq y \\mid X = x) = C_1(u,v)\\) and solve for \\(v\\) We know the formula for \\(p\\) and can do some algebra to get \\(v\\) after some algebra \\[\\begin{equation} v = - \\dfrac{1}{a} \\ln \\left( 1 + \\dfrac{p g_1}{1 + g_u(1-p)} \\right) \\tag{24.11} \\end{equation}\\] Plug in \\(g_1\\) and \\(g_u\\) (based of formula (24.7)) in to equation (24.11) to get \\(v\\) Use the \\((u,v)\\) we get from above and then find the values in the 2 distn associated with the percentiles 24.6.2 Gumbel Copula Properties: Asymmetric more probability in the tails than Frank’s Gumbel Copula \\[\\begin{equation} C(u,v) = \\exp\\left\\{ -\\left[(-\\ln(u))^a + (- \\ln (v))^a \\right]^{1/a} \\right\\} \\tag{24.12} \\end{equation}\\] Conditional distribution \\[\\begin{equation} C_1(u,v) = C(u,v) \\left[(-\\ln(u))^a + (- \\ln (v))^a \\right]^{(-1 + 1/a)} \\cdot (-\\ln(u))^{a-1} \\cdot \\dfrac{1}{u} \\tag{24.13} \\end{equation}\\] Density Super complicated… Kendall’s \\(\\tau\\) \\[\\begin{equation} \\tau = 1 - \\frac{1}{a} \\tag{24.14} \\end{equation}\\] \\(v\\) can’t be solve from \\(p=C_1(u,v)\\) \\(\\Rightarrow\\) Need to simulate to some other ways 24.6.2.1 Simulation We can’t solve \\(v\\) like how we did with Frank For a given \\(\\tau\\), we solve for \\(a\\) using (24.14) Simulate 2 r.v.: \\[p,r \\sim U[0,1]\\] Numerically solve for \\(s\\) that satisfies \\[\\begin{equation} s \\ln(s) = a(s-p) \\:\\: : \\:\\: 0&lt;s&lt;1 \\tag{24.15} \\end{equation}\\] Use \\(s\\) we can get: \\[\\begin{equation} (u,v) = \\left( \\exp\\left[ \\ln(s)r^{\\frac{1}{a}} \\right], \\exp\\left[ \\ln(s)(1-r)^{\\frac{1}{a}} \\right] \\right) \\tag{24.16} \\end{equation}\\] 24.6.3 Heavy Right Tail Copula Properties Less correlation in the left tail but high in the right tail HRT Copula \\[\\begin{equation} C(u,v) = [u + v - 1] + \\left[ (1-u)^{-1/a} + (1 - v)^{-1/a} -1 \\right]^{-a} \\:\\: : \\:\\: a &gt; 0 \\tag{24.17} \\end{equation}\\] Conditional distribution \\[\\begin{equation} C_1(u,v) = 1 - \\left[ (1-u)^{-1/a} + (1 - v)^{-1/a} -1 \\right]^{-a-1} \\times (1-u)^{-1-1/a} \\tag{24.18} \\end{equation}\\] Density \\[\\begin{equation} c(u,v) = (1 + \\dfrac{1}{a}) - \\left[ (1-u)^{-1/a} + (1 - v)^{-1/a} -1 \\right]^{-a-2} \\times [(1-u)(1-v)]^{-1-1/a} \\tag{24.19} \\end{equation}\\] Kendall’s \\(\\tau\\) \\[\\begin{equation} \\tau(a) = \\dfrac{1}{2a + 1} \\tag{24.20} \\end{equation}\\] 24.6.3.1 Simulation We can solve \\(v\\) from \\(p=C_1(u,v)\\) so we can simulate it same way as Frank’s 24.6.3.2 Joint Burr Distribution Joint distn where marginal distn are Burr and the conditionals are also Burr Join the 2 marginal Burr using the HRT copula The 2 marginal distn needs to have the same \\(a\\) parameter as the HRT copula Start with 2 Burr distribution: \\[F(x) = 1 - \\left[ 1 + \\left( \\dfrac{x}{b} \\right)^p \\right]^{-a}\\] \\[G(y) = 1 - \\left[ 1 + \\left( \\dfrac{y}{d} \\right)^q \\right]^{-a}\\] Join with HRT with the same parameters \\(a\\) by plugging \\(F(x)\\) and \\(G(y)\\) into \\(C(F(x), G(y))\\) \\[H(x,y) = 1 - \\left[ 1 + \\left( \\dfrac{x}{b} \\right)^p \\right]^{-a} - \\left[ 1 + \\left( \\dfrac{y}{d} \\right)^q \\right]^{-a} + \\left[ 1 + \\left( \\dfrac{x}{b} \\right)^p + \\left( \\dfrac{y}{d} \\right)^q \\right]^{-a}\\] With condition distribution \\[G_{(Y \\mid X)}(y \\mid x) = 1 - \\left[ 1 + \\left( \\dfrac{y}{d_x} \\right)^q \\right]^{-(a+1)}\\] Where \\[d_x = d \\left[ 1 + \\left( \\dfrac{x}{b} \\right)^{\\frac{p}{q}} \\right]\\] Analogous to the joint normal, in that the marginal and conditional distn are the same \\(a\\) is to control correlation \\(p\\) &amp; \\(q\\) is used to fit the tail \\(b\\) &amp; \\(d\\) are used to set the scales of the distn 24.6.4 Normal Copula Joins 2 distn using correlations from the bi-variate normal Properties More dependencies in the tail then Frank Symmetrical The copula take \\(u\\) and \\(v\\) from any distn and converts them to standard normal variables and calculates the probability under the joint normal distn with parameter \\(a\\) Definitions \\(N(x; \\: m,v) =\\) Normal distribution with mean \\(m\\) and variance \\(v\\) Therefore \\[P(X \\leq x) = N(x; \\: m,v) \\:\\:\\:\\:\\: \\mathbb{R}^2 \\rightarrow [0,1]\\] Use the following shorthand for standard normal \\[N(x) = N(x; \\: 0,1)\\] \\(B(x,y; \\: a)\\) Bivariate standard normal distribution with Pearson correlation \\(a\\) \\[P(X \\leq x, Y \\leq y) = B(x, y; \\:a) \\:\\:\\:\\:\\: \\mathbb{R}^2 \\rightarrow [0,1]\\] \\(p(u)\\) is the inverse of the standard normal \\(\\Phi^{-1}(u)\\) \\[p(u) \\:\\:\\:\\:\\: [0,1] \\rightarrow \\mathbb{R}\\] \\[N(p(u)) = u\\] e.g. \\(p(0,95) = 1.645\\) Normal Copula \\[\\begin{equation} C(u,v) = B[p(u), p(v); \\: a] \\tag{24.21} \\end{equation}\\] Conditional distribution \\[\\begin{equation} C_1(u,v) = N(p(v); a p(u), 1-a^2) \\tag{24.22} \\end{equation}\\] Density \\[\\begin{equation} c(u,v) = \\dfrac{\\exp\\left( \\dfrac{[a^2 p(u)^2 - 2a p(u) p(v) + a^2 p(v)^2]}{2(1-a^2)} \\right)}{\\sqrt{(1-a^2)}} \\tag{24.23} \\end{equation}\\] Kendall’s \\(\\tau\\) \\[\\begin{equation} \\tau(a) = \\dfrac{2 \\arcsin(a)}{\\pi} \\tag{24.24} \\end{equation}\\] \\[\\begin{equation} a = \\sin \\left( \\tau \\times \\dfrac{\\pi}{2} \\right) \\tag{24.25} \\end{equation}\\] 24.6.4.1 Simulation Normal copula is simple to simulate from and also easy to generalizes to multiple dimensions Solve for \\(a\\) with the \\(\\tau\\) using (24.25) Simulate 2 r.v. \\[u,r \\sim U[0,1]\\] Get the standard normal value of the 2 percentiles \\[x = p(u)\\] \\[y = p(r)\\] Join the 2 normal value with \\[z = ax + y \\sqrt{1 - a^2}\\] Convert \\(z\\) to a percentile with the normal function to get \\(v\\) \\[ v = N(z)\\] 24.6.5 Partial Perfect Correlation Copula Krep’s Partial Perfect Correlation Copula Sort of artificial, more useful for simulating evens then for description of actual outcomes Generates dependencies by sometimes simulating \\(u\\) &amp; \\(v\\) where they are independent and sometimes 100% dependent by setting \\(v=u\\) Level of dependency is controlled by \\[q(u,p): [0,1]^2 \\rightarrow [0,1]\\] \\(q(u,p)\\) needs to be symmetric Definition Simulate PPC by drawing \\(u\\), \\(p\\) &amp; \\(w\\) from \\(U[0,1]\\) \\(v = \\begin{cases} p &amp;:q(u,p) &lt; w &amp; Independent \\\\ u &amp;:q(u,p) \\geq w &amp; Dependent \\\\ \\end{cases}\\) Partial Perfect Power \\(q(u,p) = (up)^a\\) More concentrate at the top right, since if either \\(u\\) or \\(v\\) is large then the other will likely be at the same percentile \\(\\Rightarrow\\) perfectly correlated "],
["tail-concentration-functions.html", "24.7 Tail Concentration Functions", " 24.7 Tail Concentration Functions Measure how much probability is concentrated in the tails 24.7.1 Left Tail Concentration Function What is the probability that \\(U\\) is small given that \\(V\\) is small \\[L(z) = P(U &lt; z \\mid V &lt; z) = P(V &lt; z \\mid U &lt; z)\\] Works both ways can do \\(L(z) = P(V &lt; z \\mid U &lt; z)\\) as well Left tail concentration function \\[\\begin{equation} L(z) = \\dfrac{C(z,z)}{z} \\tag{24.26} \\end{equation}\\] LTC function for Independent copula: \\[\\begin{equation} L(z) = z \\tag{24.27} \\end{equation}\\] Graph this to 0.5 because all copulas have \\(L(1) = 1\\) See Graph (1) in fig. 24.1 24.7.2 Right Tail Concentration Function What is the probability that one variable is large given that the other variable is large \\[R(z) = P(U &gt; z \\mid V &gt; z) = P(V &gt; z \\mid U &gt; z)\\] Right tail concentration function \\[\\begin{equation} R(z) = \\dfrac{1 - 2z + C(z,z)}{1-z} \\tag{24.28} \\end{equation}\\] RTC function for Independent copula: \\[\\begin{equation} R(z) = 1 - z \\tag{24.29} \\end{equation}\\] Graph this from 0.5 to 1.0 because all copulas has \\(R(0) = 0\\) See Graph (2) in fig. 24.1 24.7.3 LR Graph Combine the 2 above we have the tail concentration graph or the LR graph See Graph (3) in fig. 24.1 The independent copula = product copula Figure 24.1: Tail concentration graph Graph (4) it approaches 0 on the left and right Graph (5) asymmetric, right limit is not 0 (converges about 0.5) Graph (6) asymmetric, right limit is not 0, and higher than Gumbel. Has the thinnest left tail Graph (7) it approaches 0 on the left and right but slower, simply because we couldn’t calculate the tail so deep As the \\(\\tau\\) gets higher, we can start the see the difference between them Table 24.2: Limit of right tail concentration function \\(R = \\lim \\limits_{z \\rightarrow 1} R(z)\\) Copula \\(R\\) \\(\\tau\\) Gumbel \\(2 - 2^{1/a}\\) \\(1 - \\dfrac{1}{a}\\) HRT \\(2^{-a}\\) \\(\\dfrac{1}{2a + 1}\\) Normal 0 \\(\\dfrac{2 \\arcsin(a)}{\\pi}\\) Frank 0 Complicated… PP Power \\(\\dfrac{1}{1 + a}\\) Table is not on syllabus but just for information "],
["how-to-select-copula-given-dataset.html", "24.8 How to Select Copula Given Dataset", " 24.8 How to Select Copula Given Dataset Plot the percentile plot to diagnose Calculate the empirical tail concentration function (LR function) and you can see how each copula fit best Note: Figure 3.3.5 and 3.3.6 is not very good as it mix the underlying distn with the copula 24.8.1 Multivariate Copulas Many options of copulas for 2 variables (pair-wise) but not multivariate Normal and t-copula are the only well known multivariate copulas Parameters: full correlation matrix Normal copula Has no correlation deep in the tail The right tail concentration is very small, and approaches 0 at the limit Just to be clear, the density deep in the tail (z &gt; 0.999) you get very high values still t-copula Can be very strongly correlated in the tails, controlled with \\(n\\) \\(n \\rightarrow \\infty\\), the t-copula approaches the normal copula For small n, it has high density in the tails T-copula also have some density in the other corners Where on variable is high, the other is low "],
["fitting-copulas-to-data.html", "24.9 Fitting Copulas to Data", " 24.9 Fitting Copulas to Data \\(J(z)\\) and \\(\\chi(z)\\) are the empirical statistics we’re interested in that are related to \\(\\tau\\) and \\(R\\) We graph them empirically 24.9.1 \\(J(z)\\) \\[J(z) = - z^2 + \\dfrac{4 \\int_0^z \\int_0^z C(u,v) \\cdot c(u,v)dudv}{C(z,z)}\\] Recall continuous formula for \\(\\tau\\) (24.4) Since \\(C(1,1) =1\\) We have \\[\\lim \\limits_{z \\rightarrow 1} J(z) = \\tau\\] \\(J(z)\\) is the build up of \\(\\tau\\) from 0 (when no data points are considered), up to \\(\\tau\\) when all the data points are considered Maybe?? Graph the empirical \\(J(z)\\) and compare to the theoretical \\(J(z)\\) for each copulas Note: 3.3.14 is incorrect Look at 3.3.16 we can tell that t-copula is the best fit and MM2 is the next closest, MM1 is the worst 24.9.2 \\(\\chi(z)\\) Don’t have much on an intuitive explanation for what the graph represents \\(z \\rightarrow 1\\) it approaches \\(R\\) \\[\\chi(z) = 2 - \\dfrac{\\mathrm \\ln C(z,z)}{\\ln(z)}\\] \\[R = \\lim \\limits_{z \\rightarrow 1} R(z) = \\lim \\limits_{z \\rightarrow 1} \\dfrac{1 -2z + C(z,z)}{1-z}\\] Compare the empirical graph of \\(\\chi(z)\\) with the theoretical copulas to determine which is best fit "],
["past-exam-questions.html", "24.10 Past Exam Questions", " 24.10 Past Exam Questions TIA Exercise 3.1 TIA 1: Staffing issues for IRM TIA 2: Getting buy in TIA 3: Expert opinions for parameterization TIA 4: Pilot test benefit \\(\\star\\) TIA 5: Actions vs best practice TIA 6: Tasks enabled by internal model TIA Exercise 3.2 TIA 7: Calculate mean and s.d. using formula (??) TIA 8: inflation pick methods TIA 9: inflation model with AR formula and properties TIA 10: How to use a pool of parameter estimates TIA 11: Correlate parameters with joint lognormal and information matrix TIA 12: Mix distributions and parameters in simulation TIA Exercise 3.3 \\(\\star \\star\\) TIA 13: Gumbel copula on 2 uniform distribution \\(\\star \\star\\) TIA 14: HRT copula on 2 Exp and Unif TIA 15: Picking copula based on discription TIA 16: Product copula \\(\\star \\star\\) TIA 17: plot RTC \\(\\star\\) TIA 18: RTC from formula (24.28) TIA 19: Pick copula based on LoB description \\(\\star\\) TIA 20: Reading LR Graph and estimate \\(\\tau\\) for the copula using \\(R\\) TIA 21: Advantages of t-copula \\(\\star\\) TIA 22: Using the PP Copula 24.10.1 Question Highlights n/a -->"]
]
