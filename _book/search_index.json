[
["measuring-the-variability-of-chain-ladder-reserve-estimate-t-mack.html", "Chapter 4 Measuring the Variability of Chain Ladder Reserve Estimate - T. Mack", " Chapter 4 Measuring the Variability of Chain Ladder Reserve Estimate - T. Mack Know the 3 assumptions of chain ladder Know the 3 different weight and variance assumptions Weight \\(w_{j,k}\\) Description Variance Residual 1 Simple Average \\(\\alpha_k^2 \\times \\mathbf{c_{j,k}^2}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{j,k}^2}}}\\) \\(c_{j,k}\\) Weighted Average \\(\\alpha_k^2 \\times \\mathbf{c_{j,k}}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{j,k}}}}\\) \\(c_{j,k}^2\\) Least Square \\(\\alpha_k^2 \\times \\mathbf{1}\\) \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{1}}}\\) Mean squared error calculation \\(\\begin{align} MSE(\\hat{c}_{i,I}) = \\hat{c}_{i,I}^2 \\Bigg \\{ \\sum_{k = I + 1 - i}^{I-1} \\frac{\\hat{\\alpha}_k^2}{\\hat{f_k}^2} \\Bigg ( \\frac{1}{\\hat{c}_{i,k}} + \\underbrace{\\frac{1}{\\sum_{j=1}^{I-k}c_{j,k}}}_{\\text{Column x latest}}\\Bigg ) \\Bigg \\} \\end{align}\\) \\(\\begin{align} \\hat{\\alpha}_k^2 = \\frac{1}{I - k - 1} \\sum_{j=1}^{I-k} c_{j,k} \\Big ( \\underbrace{\\frac{c_{j,k+1}}{c_{j,k}}-\\hat{f_k}}_{\\text{AY LDFs - Selected}} \\Big )^2 \\end{align}\\) \\(\\hat{\\alpha}_{I-1}^2 = \\operatorname{min} \\left( \\dfrac{(\\hat{\\alpha}_{I-2}^2)^2}{\\hat{\\alpha}_{I-3}^2},\\hat{\\alpha}_{I-3}^2 \\right) = \\begin{cases} \\hat{\\alpha}_{I-3}^2 &amp; \\text{if } \\hat{\\alpha}_{I-3}^2 &lt; \\hat{\\alpha}_{I-2}^2 \\\\ \\hat{\\alpha}_{I-2}^2 \\times \\dfrac{\\hat{\\alpha}_{I-2}^2}{\\hat{\\alpha}_{I-3}^2} &amp; \\text{else}\\\\ \\end{cases}\\) Confidence Interval Normal: equation (4.6) Log-normal: equation (4.7) Know the 4 test of assumptions Test 1. Intercept Test 2. Residuals Test 3. CY Test Rank small and large \\(\\operatorname{E}[z_n] = \\dfrac{n}{2} - c_n\\) \\(c_n = {n - 1 \\choose m}\\frac{n}{2^n}\\) \\(m = \\operatorname{floor}\\left[ \\dfrac{n-1}{2} \\right]\\) \\(\\operatorname{Var}(z_n) = \\dfrac{n(n-1)}{4} - c_n (n-1) + \\operatorname{E}[Z_n] - \\operatorname{E}[z_n]^2\\) \\(Z = \\sum_{diagonal} z\\) Since \\(Z \\sim\\) Normal, can sum the mean and variance assuming \\(\\perp\\!\\!\\!\\perp\\) Test 95% CI: \\(\\operatorname{E}[Z] \\pm 2 \\times \\sigma\\) Test 4. Adjacent LDF Correlation \\(S = \\sum \\limits_{\\in rows} \\Big \\{ [Rank \\: Col \\: i \\: LDF] - [Rank \\: Col \\: j \\: LDF] \\Big \\}^2\\) \\(T_k = 1 - \\dfrac{S}{n(n^2-1)/6}\\) \\(T = \\dfrac{\\sum T_k (n_k - 1)}{\\sum (n_k-1)} = \\dfrac{\\sum_k (I - k -1)T_k}{\\sum_k I - k -1}\\) \\(\\operatorname{E}[T] \\pm Z \\sqrt{\\operatorname{Var}(T)}\\) \\(\\operatorname{E}[T] = 0\\) \\(\\operatorname{Var}[T] = \\dfrac{1}{(I-2)(I-3)/2}\\) Use \\(Z = 0.67\\) for range of [25%, 75%] "],
["cl-ass.html", "4.1 Chain Ladder Assumptions", " 4.1 Chain Ladder Assumptions Definition 4.1 Notations use for Mack \\(c_{i,k} =\\) cumulative losses for AY \\(i\\) @ age \\(k\\) \\(f_k =\\) LDF from \\(k\\) to \\(k + 1\\), \\(k \\in [1:I-1]\\) \\(I =\\) size of the triangle Proposition 4.1 (Chain Ladder Assumption 1) \\(\\operatorname{E}\\left [c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ] = c_{i,k} \\: f_k\\) Expected incremental losses are \\(\\propto\\) losses reported to date Proportion depends on the age of AY Proposition 4.2 (Chain Ladder Assumption 2) \\(\\left \\{c_{i,1} \\cdots c_{i,I} \\right \\} \\: {\\perp\\!\\!\\!\\!\\perp} \\: \\left \\{c_{j,1} \\cdots c_{j,I} \\right \\}\\) for \\(i \\neq\\ j\\) Losses in each AYs are \\({\\perp\\!\\!\\!\\!\\perp}\\) of the losses in other AYs This assumption make our estimate unbiased Proposition 4.3 (Chain Ladder Assumption 3) \\(\\operatorname{Var}\\left (c_{i,k+1} \\mid c_{i,1} \\cdots c_{i,k}\\right ) = \\alpha_k^2 \\: c_{i,k}\\) Variance of the incremental losses is \\(\\propto\\) losses reported to date Proportion depends on the age (\\(k\\)) of AY (i.e. same for each column but varies for each column) \\(\\hat{f}_k\\) is selected to minimize the variance Need to double check on the below: Proposition 4.4 (Chain Ladder Assumption 4) Variance is \\(\\propto\\) mean Based on assumption 1 and 3 (proposition 4.1 &amp; proposition 4.3) If variance follows distribution x, then it implies the variance is \\(\\propto\\) loss 4.1.1 Implications of Assumptions When using CL, we are making assumptions on how we select the factor \\(\\hat{f}_k\\) and the application \\(\\hat{c}_{i,k+1} = c_{i,k} \\times \\hat{f}_k\\) Which requires the following assumptions: Unbiased estimate of each \\(f_k\\) \\(\\mathrm{E}\\left[ \\hat{f}_k \\right] = f_k\\) \\(\\hat{f}_k\\) are representative of the true \\(f_k\\) Based on assumption 1 &amp; 2 (prop. 4.1 &amp; prop. 4.2) Unbiased estimate of Ultimate \\(\\mathrm{E}\\left[ \\hat{c}_{iI} \\right] = c_{ik} \\times \\hat{f}_k \\times \\cdots \\times \\hat{f}_{I-1} = \\mathrm{E}\\left[ c_{iI} \\right]\\) Multiplying the \\(\\hat{f}_k\\)â€™s by the paid to date will give us an unbiased estimate of the future losses Based on assumption 1 &amp; 2 (prop. 4.1 &amp; prop. 4.2) To use volume weighted average LDF Based on assumption 3 (prop. 4.3) To calculate the confidence interval Based on assumption 3 (prop. 4.3) 4.1.2 Proof for Assumption 3 Work in progress section To estimate \\(f_k\\) we can weight the historical LDFs in many different ways, putting it in general terms: \\[\\begin{equation} \\hat{f}_k = \\sum_i \\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right) \\times w_{i,k} \\:\\:\\:\\: : \\:\\:\\:\\: \\sum_i w_{i,k} = 1 \\tag{4.1} \\end{equation}\\] Remark. We assume each of the \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) are an unbiased estimate of \\(f_k\\) From assumption 1 &amp; 2? \\(\\therefore\\) Any weighting of them is also unbiased Remark. When we use weighted volume average \\(w_{i,k} = \\dfrac{c_{i,k}}{\\sum \\limits_{j} c_{j,k}}\\) The weights that minimize the variance is inversely proportional to the variance of the item we are estimating We want weights \\(w_{i,k}\\) for each \\(i\\) on \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) The weight we apply to each \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) varies based on the variance of \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) And if \\(\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\) has high variance its weight will be lower to minimize the total variance of our estimate of \\(\\hat{f}_k = \\left( \\dfrac{c_{1,k+1}}{c_{1,k}} \\right) \\times w_{1,k} + \\cdots + \\left( \\dfrac{c_{I,k+1}}{c_{I,k}} \\right) \\times w_{I,k}\\) High variance estimate get lower weight: \\[\\dfrac{1}{w_{i,k}} \\propto \\mathrm{Var}\\left( \\dfrac{c_{i,k+1}}{c_{i,k}} \\right)\\] Since \\(c_{i,k}\\) is known, we can pull it out of the variance term \\[\\dfrac{1}{w_{i,k}} \\propto \\dfrac{\\mathrm{Var}(c_{i,k+1})}{c_{i,k}^2}\\] and we get \\[\\begin{equation} w_{i,k} \\times \\mathrm{Var}(c_{i,k+1}) \\propto c_{i,k}^2 \\tag{4.2} \\end{equation}\\] Recall the weight for the volume weighted average is: \\[\\begin{align} w_{i,k} &amp;= \\dfrac{c_{i,k}}{\\sum \\limits_{j} c_{j,k}}\\\\ w_{i,k} &amp;\\propto c_{i,k}\\\\ \\end{align}\\] Applying the above to equation (4.2) we get: \\[\\begin{align} c_{i,k} \\times \\mathrm{Var}(c_{i,k+1}) &amp;\\propto c_{i,k}^2 \\\\ \\mathrm{Var}(c_{i,k+1}) &amp;\\propto c_{i,k} \\\\ \\mathrm{Var}(c_{i,k+1}) &amp;= \\alpha^2_k \\times c_{i,k} \\\\ \\end{align}\\] And we have chainladder assumption 3 (Prop. 4.3) 4.1.3 LDF Selections Assumptions Recall equation (4.1) and (4.2) Weight \\(w_{i,k}\\) Description Variance Residual (4.9) 1 Simple Average \\(\\alpha_k^2 \\times \\mathbf{c_{i,k}^2}\\) \\(\\varepsilon_{i,k} = \\dfrac{c_{i,k+1} - c_{i,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{i,k}^2}}}\\) \\(c_{i,k}\\) Weighted Average \\(\\alpha_k^2 \\times \\mathbf{c_{i,k}}\\) \\(\\varepsilon_{i,k} = \\dfrac{c_{i,k+1} - c_{i,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{c_{i,k}}}}\\) \\(c_{i,k}^2\\) Least Square2 \\(\\alpha_k^2 \\times \\mathbf{1}\\) \\(\\varepsilon_{i,k} = \\dfrac{c_{i,k+1} - c_{i,k} \\: \\hat{f_k}}{\\sqrt{\\mathbf{1}}}\\) Note the least square here we are forcing the intercept through the origin Assumption for LS is that variance is the same for each exposure year (See Brosius) Use different method for LDF selection based on variance assumption Variance and the weight always multiply to \\(c_{j,k}^2\\) "],
["mean-squared-error.html", "4.2 Mean Squared Error", " 4.2 Mean Squared Error We wish to measure the average error between the ultimate losses \\(c_u\\) and the estimate \\(\\hat{c}_u\\) \\[\\begin{align} MSE(\\hat{c}_{i,I}) = \\mathrm{E}\\left[(c_{i,I}-\\hat{c}_{i,I})^2 \\mid D\\right] \\end{align}\\] Remark. Above represents the average error between estimate and actual utlimate given data to date \\(D\\) Standard error: \\(s.e. = \\sqrt{MSE}\\) Appling the properties of \\(\\mathrm{Var}(X) = \\mathrm{E}[X^2] - \\mathrm{E}[X]^2\\) we get \\[MSE(\\hat{c}_{i,I}) = \\mathrm{E}\\left[(c_{i,I}-\\hat{c}_{i,I})^2 \\mid D\\right] = \\mathrm{Var}\\left((c_{i,I}-\\hat{c}_{i,I}) \\mid D\\right) + \\left[\\mathrm{E}\\left[(c_{i,I}-\\hat{c}_{i,I}) \\mid D\\right]\\right]^2\\] We can pull out \\(\\hat{c}_{i,I}\\) from the variance and expectation term to get \\[MSE(\\hat{c}_{i,I}) = \\underbrace{\\mathrm{Var}\\left(c_{i,I} \\mid D\\right)}_{\\text{Process Variance}} + \\underbrace{\\left[\\mathrm{E}\\left[c_{i,I} \\mid D\\right] - \\hat{c}_{i,I} \\right]^2}_{\\text{Parameter Variance}}\\] \\(R_i\\) is the unpaid for year \\(i\\): \\(R_i = c_{i,I} - c_{i,k}\\) So the estimate of the unpaid claim is \\(\\hat{R}_i = \\hat{c}_{i,I} - c_{i,k}\\) Since the difference between \\(\\hat{R}_i\\) and \\(\\hat{c}_{i,I}\\) is just paid to date (constant) so the MSE are the same \\[MSE(\\hat{R}_i) = MSE(\\hat{c}_{i,I})\\] 4.2.1 Applying the MSE Formula Important formulas below \\[\\begin{equation} MSE(\\hat{c}_{i,I}) = \\left[ s.e.(\\hat{c}_{i,I}) \\right]^2 = \\hat{c}_{i,I}^2 \\Bigg \\{ \\sum_{k = I + 1 - i}^{I-1} \\frac{\\hat{\\alpha}_k^2}{\\hat{f}_k^2} \\Bigg ( \\frac{1}{\\hat{c}_{i,k}} + \\underbrace{\\frac{1}{\\sum_{j=1}^{I-k}c_{j,k}}}_{\\text{Column x latest}}\\Bigg ) \\Bigg \\} \\tag{4.3} \\end{equation}\\] Remark. The big \\(\\sum\\) sum over the remaining years till ultimate For bit outside the \\(\\sum\\) is same for the row 4.2.1.1 Estimating \\(\\alpha\\) Important formulas below \\(\\alpha\\) is the proportions for the variance \\[\\begin{equation} \\hat{\\alpha}_k^2 = \\frac{1}{I - k - 1} \\sum_{j=1}^{I-k} c_{j,k} \\Big ( \\underbrace{\\frac{c_{j,k+1}}{c_{j,k}}-\\hat{f_k}}_{\\text{AY LDFs - Selected}} \\Big )^2 \\tag{4.4} \\end{equation}\\] Remark. This one varies by age (columns), for each age: Calculate the difference2 of the LDFs for each AY and the selected Then multiply by each rowâ€™s respective cumulative loss for that age Sum the above and then divide by the number of rows minus 1 \\(\\alpha^2_{I-1}\\) (e.g. \\(\\alpha^2_9\\) for a 10 \\(\\times\\) 10 triangle) \\[\\begin{equation} \\hat{\\alpha}_{I-1}^2 = \\operatorname{min} \\left( \\hat{\\alpha}_{I-2}^2 \\times \\dfrac{\\hat{\\alpha}_{I-2}^2}{\\hat{\\alpha}_{I-3}^2},\\hat{\\alpha}_{I-3}^2 \\right) = \\begin{cases} \\hat{\\alpha}_{I-3}^2 &amp; \\text{if } \\hat{\\alpha}_{I-3}^2 &lt; \\hat{\\alpha}_{I-2}^2 \\\\ \\hat{\\alpha}_{I-2}^2 \\times \\dfrac{\\hat{\\alpha}_{I-2}^2}{\\hat{\\alpha}_{I-3}^2} &amp; \\text{else}\\\\ \\end{cases} \\tag{4.5} \\end{equation}\\] Remark. For the last \\(\\alpha\\): If the 3rd last \\(\\alpha\\) is lower than the 2nd last \\(\\alpha\\), use the 3rd last \\(\\alpha\\) (If the final \\(alpha\\)â€™s are not trending down, take the lower one) If the final \\(\\alpha\\)â€™s are decreasing, then just take the same % of decrease to get the last \\(\\alpha\\) Also note that if we believe the claims development have stopped at some age \\(j\\) then we can set \\(\\alpha^2_j = 0\\) Finally, you can also fit a logarithmic curve to the \\(\\alpha^2_k\\)â€™s to estimate \\(\\alpha^2_{I-1}\\) "],
["confidence-intervals.html", "4.3 Confidence Intervals", " 4.3 Confidence Intervals Can have different assumptions on the distribution of the unpaid Important formulas below Normal Estimation \\[\\begin{equation} \\hat{R}_i \\pm Z_\\alpha \\: s.e.(\\hat{R}_i) \\tag{4.6} \\end{equation}\\] Remark. Under CLT we can assume that \\(\\hat{R}_i\\) is normally distributed given that the outstanding claims are large We can get the 95% CI with \\(Z_{0.975} = 1.96\\) Log-Normal Estimation \\[\\begin{equation} e^{\\mu_i + Z_{\\alpha} \\sigma_i} = \\hat{R}_i \\times \\exp\\left \\{ -\\dfrac{\\sigma_i^2}{2} \\pm \\: Z_\\alpha \\sigma_i \\right \\} \\tag{4.7} \\end{equation}\\] \\[\\sigma_i^2 = \\operatorname{ln} \\left [ 1 + \\left ( \\dfrac{s.e.(\\hat{R}_i)}{\\hat{R}_i} \\right)^2 \\right]\\] \\[\\mu_i = \\operatorname{ln}(\\hat{R}_i) - \\dfrac{\\sigma_i^2}{2}\\] Remark. Use log-normal when the true distribution of \\(R_i\\) is skewed Especially true when \\(s.e.(\\hat{R}_i) &gt; \\dfrac{R_i}{2}\\) This would give us negative value for the bottom end of the CI if we use normal distribution 4.3.1 CI for All Years Reserves Mean is easy: \\(\\hat{R} = \\sum \\limits_{i} \\hat{R}_i\\) But since \\(\\hat{R}_{i}\\) rely on the same LDFs they are not independent and we need to include a correlation factor for the \\(MSE(\\hat{R})\\) \\[\\begin{equation} [s.e.(\\hat{R})]^2 = \\sum \\limits_{i=2}^I \\left\\{ [s.e.(\\hat{R}_i)]^2 + \\hat{c}_{i,I} \\left( \\sum \\limits_{j = i+1}^{I} \\hat{c}_{j,I} \\right) \\left( \\sum \\limits_{k = I + 1 - i}^{I - 1} \\dfrac{2 \\hat{\\alpha}^2_k \\big/ \\hat{f}^2_k}{\\sum_{n=1}^{I-k}c_{n,k}}\\right) \\right\\} \\tag{4.8} \\end{equation}\\] If we want to simplify things we can use the square root rule to sum up the different AYs if we assume independence "],
["chain-ladder-assumptions-test.html", "4.4 Chain Ladder Assumptions Test", " 4.4 Chain Ladder Assumptions Test Tests on the various Chain Ladder assumptions Intercept Residuals Calendar year test Correlation of adjacent LDFs 4.4.1 Intercept Test for assumption 1 (prop. 4.1) Plot the losses at adjacent ages to see if the line of best fit goes through the origin Do this for every age \\(k\\) vs age \\(k+1\\) 4.4.2 Residuals Test for assumption 3 (prop. 4.3) For each age \\(k\\), plot the \\(c_{i,k}\\) with the residuals \\(\\varepsilon_{i,k}\\) \\[\\begin{equation} \\varepsilon_{i,k} = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{\\operatorname{Var}(c_{j,k})}} \\tag{4.9} \\end{equation}\\] Remark. We can take out the \\(\\alpha^2_k\\) term since itâ€™s constant for the same \\(k\\) e.g. \\(\\varepsilon = \\dfrac{c_{j,k+1} - c_{j,k} \\: \\hat{f_k}}{\\sqrt{c_{j,k}}}\\) for weighted average assumption For residuals @ \\(k\\), you need LDFs from \\(t-1\\) to \\(k\\) They should vary randomly around zero If passed \\(\\Rightarrow\\) expected losses are linear w.r.t. cumulative losses paid to date Can also use to test assumption (3) on the other variance assumption by calculating the \\(\\varepsilon\\) differently 4.4.3 Calendar Year Test Test for assumption 1 (prop. 4.2) Step 1) Rank the LDFs in each column Step 2) Label them S and L and the median is discarded Step 3) For each diagonal with at least 2 elements, \\(z = \\operatorname{min}(\\text{# of S}, \\text{# of L})\\) Step 4) Calculate \\(\\operatorname{E}[z_n]\\) and \\(\\operatorname{Var}(z_n)\\) \\(\\operatorname{E}[z_n] = \\dfrac{n}{2} - c_n\\) Memorize Formulas \\(n =\\) # of elements in each diagonal excluding the throw away value \\(c_n = {n - 1 \\choose m}\\frac{n}{2^n}\\) \\(m = \\operatorname{floor}\\left[ \\dfrac{n-1}{2} \\right]\\) \\(\\operatorname{Var}(z_n) = \\dfrac{n(n-1)}{4} - c_n (n-1) + \\operatorname{E}[Z_n] - \\operatorname{E}[z_n]^2\\) Memorize Formulas \\(z \\sim\\) Normal Step 5) See if \\(Z\\) is in the CI based on the the above \\(Z = \\sum_{diagonal} z\\) Since \\(Z \\sim\\) Normal, can sum the mean and variance by assuming independence Test 95% CI: \\(\\operatorname{E}[Z] \\pm 2 \\times \\sigma\\) If the observed is outside the range \\(\\Rightarrow\\) There is calendar year effects, fail assumption (2) CY effect can be caused by changing or high inflation; change in claims handling; change in legal environment 4.4.4 Correlation of Adjacent LDFs Test assumption (1) (prop. 4.1) where expected only depends on most recent Use a relatively low threshold of 50% Step 1) Calculate Spearmanâ€™s correlation for each pair of adjacent LDFs Memorize Formulas \\(S = \\sum \\limits_{\\in rows} \\Big \\{ [Rank \\: Col \\: i \\: LDF] - [Rank \\: Col \\: j \\: LDF] \\Big \\}^2\\) Rank from low to high (i.e. lowest is 1) \\(T_k = 1 - \\dfrac{S}{n(n^2-1)/6}\\) \\(n =\\) # of rows For a 10 x 10 triangle, \\(k\\) is at most 7 because thereâ€™s only 9 LDFs so 8 pairs. And down to 7 because we donâ€™t use the pair with only 1 row Step 2) Calculate \\(T\\) for the whole triangle Memorize Formulas \\(T = \\dfrac{\\sum T_k (n_k - 1)}{\\sum (n_k-1)} = \\dfrac{\\sum_k (I - k -1)T_k}{\\sum_k I - k -1}\\) \\(I =\\) size of triangle \\(k\\) starts at 2 Formula gives more weight to \\(T_k\\) with more data Step 3) Compare \\(T\\) with CI based on distribution Memorize Formulas \\(\\operatorname{E}[T] = 0\\) \\(\\operatorname{Var}[T] = \\dfrac{1}{(I-2)(I-3)/2}\\) \\(\\operatorname{E}[T] \\pm Z \\sqrt{\\operatorname{Var}(T)}\\) Use \\(Z = 0.67\\) for range of [25%, 75%] Do not reject the \\(H_0\\) of uncorrelated LDFs if the \\(T\\) is in the CI Venter has a method too "],
["past-exam-questions.html", "4.5 Past Exam Questions", " 4.5 Past Exam Questions Havenâ€™t done TIA practice questions No historical questions on the MSE yet Concepts 2011 #3 b: CL method assumptions 2012 #5: CL assumptions and whether they are met 2014 #2: CL assumptions on intercept and residuals 2014 #4 b: CL LS assumptions Assumption test \\(\\star\\) 2011 #3 a (fig 4.1): CY Test 2013 #3: Residuals test 2013 #1: CY test and potential cause \\(\\star\\) 2014 #4 a (fig 4.2): Adjacent LDFs correlation test Has to use method from Venter? 2015 #3: Residuals test and plot, remember to label them 2015 #4: CY Test 4.5.1 Question Highlights Figure 4.1: 2011 Question 3 Figure 4.1: 2011 Question 3 Figure 4.2: 2014 Question 4 Figure 4.2: 2014 Question 4 -->"]
]
